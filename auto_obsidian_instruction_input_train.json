[
    {
        "instruction": "<|SYSTEM|>You are a scholars that is taking markdown notes about some video with Wiki style, you would be giving some written scripts for the task.\nAnd summarize each paragraph with a title.\nRemember to keep the technology or algorithm name, very important.\nPlease write the notes in markdown format.\nHere is an example:\n### Definiation\nA deadlock is a situation in which two computer programs sharing the same resource are effectively preventing each other from accessing the resource, resulting in both programs ceasing to function. \nThe earliest computer operating systems ran only one program at a time.\n\n### What it have to do with Relationsal Database ManagementSystem\nIn a database, a deadlock is an unwanted situation in which two or more transactions are waiting indefinitely for one another to give up locks. \nDeadlock is said to be one of the most feared complications in DBMS as it brings the whole system to a Halt. \nWhen you tries to insert or update data on database, sometime your record need to depend on the row record.  \n<|ASSISTANT|>#### ",
        "input": "<|USER|> Dear Fellow Scholars, this is Two Minute Papers with Dr. Károly Zsolnai-Fehér. Finally, as everyone knows that text-to-image AIs are capable of creating incredible photos, digital art, whatever you wish, now we seem to be conquering video too. What? So we just write something, and exactly that video comes out? Doesn't that sound impossible? That is so much harder. You see, images is one thing, but videos require a much greater understanding of the world around us, so much more computation, and one more secret ingredient that I'm going to tell you about in a moment. So, after Google has published image and video, now, NVIDIA is here with their text-to-video AI too, and it's not just real good, it can do things that other systems can't do yet. So first, examples. Of course, we start out immediately with a scholarly example of two pandas reading a paper. Approved. It is also excellent at time-lapse videos, I love this one. We can also let our imagination run wild with this one, for instance, we can even ask for a storm",
        "output": "### Text-to-Image AI Advancements\n\nAdvancements in text-to-image AI technology allow for the creation of photos and digital art through descriptions provided.\n\n### Transition to Text-to-Video AI\n\nCreating a video from text input is a more complicated process than generating images due to the need for a deeper understanding of the world and increased computation requirements.\n\n### Secret Ingredient in Text-to-Video AI – NVIDIA's Contribution\n\nNVIDIA developed a text-to-video AI capable of accomplishing feats that are challenging for other current systems.\n\n### Examples of NVIDIA's Text-to-Video AI\n\n- Scholarly example: Two pandas reading a paper\n- Time-lapse videos: AI excels at generating them\n- Imaginative creations: Stormy skies or abstract visuals"
    },
    {
        "instruction": "<|SYSTEM|>You are a scholars that is taking markdown notes about some video, you would be giving some written scripts for the task.\nYou have finished writing the notes, and you want to extract the title from the notes.\nThe title should only be the name of the technology or algorithm, no extra words needed.\nPlease write the notes in markdown format. No need to write \"Title:\" for the Title. Make sure the title is within 10 words.\nHere is an example:\n## Hidden Markov Model\n<|ASSISTANT|>## ",
        "input": "<|USER|>### Text-to-Image AI Advancements\n\nAdvancements in text-to-image AI technology allow for the creation of photos and digital art through descriptions provided.\n\n### Transition to Text-to-Video AI\n\nCreating a video from text input is a more complicated process than generating images due to the need for a deeper understanding of the world and increased computation requirements.\n\n### Secret Ingredient in Text-to-Video AI – NVIDIA's Contribution\n\nNVIDIA developed a text-to-video AI capable of accomplishing feats that are challenging for other current systems.\n\n### Examples of NVIDIA's Text-to-Video AI\n\n- Scholarly example: Two pandas reading a paper\n- Time-lapse videos: AI excels at generating them\n- Imaginative creations: Stormy skies or abstract visuals",
        "output": "## Text-to-Video AI"
    },
    {
        "instruction": "<|SYSTEM|>You are a scholars that is taking markdown notes about some video with Wiki style, you would be giving some written scripts for the task.\nAnd summarize each paragraph with a title.\nRemember to keep the technology or algorithm name, very important.\nPlease write the notes in markdown format.\nHere is an example:\n### Definiation\nA deadlock is a situation in which two computer programs sharing the same resource are effectively preventing each other from accessing the resource, resulting in both programs ceasing to function. \nThe earliest computer operating systems ran only one program at a time.\n\n### What it have to do with Relationsal Database ManagementSystem\nIn a database, a deadlock is an unwanted situation in which two or more transactions are waiting indefinitely for one another to give up locks. \nDeadlock is said to be one of the most feared complications in DBMS as it brings the whole system to a Halt. \nWhen you tries to insert or update data on database, sometime your record need to depend on the row record.  \n<|ASSISTANT|>#### ",
        "input": "<|USER|> Dear Fellow Scholars, this is Two Minute Papers with Dr. Károly Zsolnai-Fehér. Today, I will show you a research paper that I can hardly believe exists. It is about an amazing voice cloning paper from Microsoft Research. What does that mean? Well, voice cloning means that an AI listens to us speaking, we write a piece of text, and it says it in our voice. To see what that looks like, this is a previous work from NVIDIA that was able to do that. Let's listen to Jamil recording these voice snippets. Okay, so how much of this do we need to train this earlier AI? Well, not the entire live recordings of the test subject, but much less. Only 30 minutes of these voice samples. The technique asks us to say these sentences and analyzes the timbre, prosody and the rhythm of our voice, which is quite a task. And what can it do afterwards? Well, it creates an AI Jamil that can now say a scholarly message for you that I wrote. This is a voice line generated by an AI. Wonder if you Fellow Scholars are going to notice. The first law of papers says that research is a process. Do not look at where we are, look at where we will be to more papers down the line. Now hold on to your papers, because Microsoft has a cracking new paper with an AI they call Volley that is able to do the same, but not in 30 minutes. Not even in 3 minutes. It can do it in 3 seconds. Yes, that's right, all it needs is a 3 second snippet of our voice, and it promises that it can clone it from that. And it gets crazier. Much crazier. I will show you how in a moment. First, in goes a 3 second voice sample of us saying something. This does not need to match the text prompt here. And this is going to be used for the learning. Now that it has learned this voice, here is what a previous technique could do in terms of cloning. And here is the new method. Wow! The phrasing and timing are so much better! But actually, how good is it? How do we even know if this is really better? Well, easy. We can ask this person to read the prompt in their voice, hide it from the AI, and compare side by side. Listen, AI goes first, instead of shoes, the old man wore boots with turnover tops, and his blue coat had wide cuffs of gold braid. Real person, second. Instead of shoes, the old man wore boots with turnover tops, and his blue coat had wide cuffs of gold braid. This is absolutely incredible! This truly feels like we are seeing history in the making. Here is another example, a speaker prompt for learning, and the new technique. I love this! It even has a little personality in there. And the true human voice. I have to say, this is even better, but the AI was also out of this world compared to what we were able to do before. Here is another one. And then, we proceed to my favorite part of the paper, three advanced features. So good. First, variety. This new technique can generate several variants of speech for the same prompt, so we can listen to them and choose the one that we like best. Listen how it puts the emphasis on different words. Because we do not need it. Because we do not need it. Loving it. Two, get this, it can also listen to the three second example of our voice and preserve its emotions too. Here is an angry one. And here is a sleepy one. I really don't know what to say. This is absolutely incredible. And three, it can also maintain not only the emotions, but the ambience and acoustic environment our sample has been recorded in. My favorite is this sample, which sounds like an old crackly phone conversation. Like this. So, can it clone this kind of sample too? Let's listen together. My goodness! I did not think that this was possible at all, let alone from a tiny tiny three second example. And just imagine the possibilities. This could bring back people who are not with us anymore and have them read us books and bedtime stories. How amazing is that? Maybe we will have the incredible Isaac Asimov reading his own robot box to us soon. What a time to be alive! And remember, NVIDIA's previous technique needed 30 minutes for this, and now, just one more paper down the line, and 600 times less information is enough to create voice samples of this quality. 600 times less! And just imagine what we will be able to do two more papers down the line. My goodness! Now, of course, this is a great research paper, so what does that mean? Of course, that means that it has a thorough evaluation section. So let's pop the hood, and have a look. Whoa! I'll try to explain this. We first compare against previous techniques in terms of the word error rate metric. The two variants of the new technique come out the absolute best. However, correctness by itself is not enough, we also wish to know if the new samples are not only correct, but also similar to the speaker in the input sample. So is it. Wow! The new technique comes out the absolute best on the two things at the same time. However, wait, these two are some ancient techniques from long long ago, right? Well, nope, your TTS is from the same year as this paper, and so is AudioLM. This is so much progress in research in so little time. Now that truly feels like history in the making. So welcome to Two Minute Papers, land of the Fellow Scholars, where we look at tables and make happy noises. So what do you think? Who else would you be interested in to read to you? Morgan Freeman, all the things, or maybe Karaj, all the things? Let me know in the comments below. And if you wish to see more papers like this, please consider subscribing, and even better, hitting the bell icon. If you are looking for inexpensive cloud GPUs for AI, Lambda now offers the best prices in the world for GPU cloud compute. No commitments or negotiation required. Just sign up and launch an instance. And hold on to your papers, because with Lambda GPU Cloud, you can get on-demand A100 instances for $1.10 per hour versus $4.10 per hour with AWS. That's 73% savings. Did I mention they also offer persistent storage? So join researchers at organizations like Apple, MIT and Caltech in using Lambda cloud instances, workstations or servers. Make sure to go to lamdalabs.com slash papers to sign up for one of their amazing GPU instances today. Thanks for watching and for your generous support, and I'll see you next time!",
        "output": "### Microsoft Research's Voice Cloning\n\nMicrosoft Research has developed an AI that clones voices based on only a 3-second sample, a significant improvement from NVIDIA's earlier AI that required 30 minutes of voice samples.\n\n### Rapid Improvement and Capabilities\n\nWith just one more research paper, the required information has been reduced by 600 times. The AI can generate several variants of speech, preserve emotions from the sample, and maintain its ambiance and acoustic environment.\n\n### Evaluation and Comparison\n\nThe paper evaluates the new technique against previous ones in terms of word error rate and speaker similarity. The new AI outperforms other techniques, even those published in the same year, showing the rapid progress in voice cloning research.\n\n### Potential Applications\n\nVoice cloning can bring back the voices of people who are no longer with us, allowing them to read books and stories or have their voices used in various applications. This technology also raises questions about whose voices we might want to hear in the future."
    },
    {
        "instruction": "<|SYSTEM|>You are a scholars that is taking markdown notes about some video, you would be giving some written scripts for the task.\nYou have finished writing the notes, and you want to extract the title from the notes.\nThe title should only be the name of the technology or algorithm, no extra words needed.\nPlease write the notes in markdown format. No need to write \"Title:\" for the Title. Make sure the title is within 10 words.\nHere is an example:\n## Hidden Markov Model\n<|ASSISTANT|>## ",
        "input": "<|USER|>### Microsoft Research's Voice Cloning\n\nMicrosoft Research has developed an AI that clones voices based on only a 3-second sample, a significant improvement from NVIDIA's earlier AI that required 30 minutes of voice samples.\n\n### Rapid Improvement and Capabilities\n\nWith just one more research paper, the required information has been reduced by 600 times. The AI can generate several variants of speech, preserve emotions from the sample, and maintain its ambiance and acoustic environment.\n\n### Evaluation and Comparison\n\nThe paper evaluates the new technique against previous ones in terms of word error rate and speaker similarity. The new AI outperforms other techniques, even those published in the same year, showing the rapid progress in voice cloning research.\n\n### Potential Applications\n\nVoice cloning can bring back the voices of people who are no longer with us, allowing them to read books and stories or have their voices used in various applications. This technology also raises questions about whose voices we might want to hear in the future.",
        "output": "## Voice Cloning"
    },
    {
        "instruction": "<|SYSTEM|>You are a scholars that is taking markdown notes about some video with Wiki style, you would be giving some written scripts for the task.\nAnd summarize each paragraph with a title.\nRemember to keep the technology or algorithm name, very important.\nPlease write the notes in markdown format.\nHere is an example:\n### Definiation\nA deadlock is a situation in which two computer programs sharing the same resource are effectively preventing each other from accessing the resource, resulting in both programs ceasing to function. \nThe earliest computer operating systems ran only one program at a time.\n\n### What it have to do with Relationsal Database ManagementSystem\nIn a database, a deadlock is an unwanted situation in which two or more transactions are waiting indefinitely for one another to give up locks. \nDeadlock is said to be one of the most feared complications in DBMS as it brings the whole system to a Halt. \nWhen you tries to insert or update data on database, sometime your record need to depend on the row record.  \n<|ASSISTANT|>#### ",
        "input": "<|USER|> Hello everybody and welcome to the coding branch. I hope you guys are all ready to learn a new AI tool today. Today we're going to be cloning voices, well my voice, but you can theoretically use what I'm showing you to clone anyone's voice. Just remember folks, with great power comes great responsibility. Have fun, but don't get in trouble. If you enjoy using free AI tools, subscribe to the channel because here you'll find quick tutorials that enable you to use the latest AI without all the hassle of putting it together yourself. Alright now let's get right into it. Today we'll be using an open source AI called Tortoise. As the author mentions in the github, it's called tortoise for a reason. It's a little slow to load up, but it gets the job done and today I'm going to be showing you how to deploy it in just a few minutes. First and foremost, you need an audio track with a voice to clone. I used Audacity, a free audio editing tool, to create a few 10 second clips of my voice. I'll leave a link to the software in the description below. These clips need to be in a WAV format though. Audacity will make that really easy on you. Just click the file in the top left hand corner, hit export, and export as WAV. Once you have the voice you want to clone, head over to the google collab. Google collab is nice because you don't have to use your own hardware or even know how to code and it's free so you can't beat it. Just click on the link in the description below and hit the little play buttons on the left hand side to run the code blocks. Once you've got your google collab up, there are really only two settings you need to change and they are both in the second code block. The first setting is what you would like the AI to say, which you can type in or paste whatever you want into this text variable here. Once you have that out of the way, you can choose how high of a quality you would like the output to be. Higher quality will take longer to load, but to be frank, we're trying to clone a voice, not make a scary AI robot voice, so I'll always use the highest of quality, but feel free to change that if you want. Once you have your settings how you like them, start running the code blocks from the top to bottom and once you hit run on the second code block, you will be prompted to upload your audio files we made earlier. The more it can learn from the better, so I'll be using 10. And once you have those audio files uploaded, you're one click away from cloning your own voice. Just run this last code block and be patient, then listen to the output. So you don't have to wait, I've already run this code, so let's check it out. Let's strive to make the world a better place, one code block at a time. Alright now that's pretty neat, and considering we were able to clone my terrible voice in less than 5 minutes, the opportunities are endless with better audio quality and more sample clips. But that about wraps it up on how to clone voices. If you have any questions, let me know in the comments down below, and if you enjoyed this type of short handed content, leave a like and don't forget to subscribe on your way out.",
        "output": "### Introduction to Voice Cloning with Tortoise AI\n\nHello everybody and welcome to the coding branch. I hope you guys are all ready to learn a new AI tool today. Today we're going to be cloning voices, well my voice, but you can theoretically use what I'm showing you to clone anyone's voice. Just remember folks, with great power comes great responsibility. Have fun, but don't get in trouble. If you enjoy using free AI tools, subscribe to the channel because here you'll find quick tutorials that enable you to use the latest AI without all the hassle of putting it together yourself.\n\n### Steps to Clone a Voice using Tortoise AI\n\n1. Prepare an audio track with the voice you want to clone (WAV format). Use Audacity, a free audio editing tool to create 10-second clips of the voice.\n2. Head over to Google Colab, which allows you to run the code without using your own hardware or coding knowledge. Click the link in the description and hit the play buttons on the left side of each code block.\n3. Set up the necessary parameters in the second code block, such as the text that should be spoken by the AI and the desired output quality.\n4. Run the code blocks as needed, starting at the top. Upload your prepared audio samples when prompted during the execution of the second code block.\n5. Be patient, and listen to the generated output.\n\n### Conclusion\n\nThat's how you can clone voices using Tortoise AI in less than 5 minutes. With better audio quality and more sample clips, the possibilities are endless. Feel free to ask questions or leave feedback in the comments. Don't forget to subscribe for more short, informative AI content."
    },
    {
        "instruction": "<|SYSTEM|>You are a scholars that is taking markdown notes about some video, you would be giving some written scripts for the task.\nYou have finished writing the notes, and you want to extract the title from the notes.\nThe title should only be the name of the technology or algorithm, no extra words needed.\nPlease write the notes in markdown format. No need to write \"Title:\" for the Title. Make sure the title is within 10 words.\nHere is an example:\n## Hidden Markov Model\n<|ASSISTANT|>## ",
        "input": "<|USER|>### Introduction to Voice Cloning with Tortoise AI\n\nHello everybody and welcome to the coding branch. I hope you guys are all ready to learn a new AI tool today. Today we're going to be cloning voices, well my voice, but you can theoretically use what I'm showing you to clone anyone's voice. Just remember folks, with great power comes great responsibility. Have fun, but don't get in trouble. If you enjoy using free AI tools, subscribe to the channel because here you'll find quick tutorials that enable you to use the latest AI without all the hassle of putting it together yourself.\n\n### Steps to Clone a Voice using Tortoise AI\n\n1. Prepare an audio track with the voice you want to clone (WAV format). Use Audacity, a free audio editing tool to create 10-second clips of the voice.\n2. Head over to Google Colab, which allows you to run the code without using your own hardware or coding knowledge. Click the link in the description and hit the play buttons on the left side of each code block.\n3. Set up the necessary parameters in the second code block, such as the text that should be spoken by the AI and the desired output quality.\n4. Run the code blocks as needed, starting at the top. Upload your prepared audio samples when prompted during the execution of the second code block.\n5. Be patient, and listen to the generated output.\n\n### Conclusion\n\nThat's how you can clone voices using Tortoise AI in less than 5 minutes. With better audio quality and more sample clips, the possibilities are endless. Feel free to ask questions or leave feedback in the comments. Don't forget to subscribe for more short, informative AI content.",
        "output": "Voice Cloning with Tortoise AI"
    },
    {
        "instruction": "<|SYSTEM|>You are a scholars that is taking markdown notes about some video with Wiki style, you would be giving some written scripts for the task.\nAnd summarize each paragraph with a title.\nRemember to keep the technology or algorithm name, very important.\nPlease write the notes in markdown format.\nHere is an example:\n### Definiation\nA deadlock is a situation in which two computer programs sharing the same resource are effectively preventing each other from accessing the resource, resulting in both programs ceasing to function. \nThe earliest computer operating systems ran only one program at a time.\n\n### What it have to do with Relationsal Database ManagementSystem\nIn a database, a deadlock is an unwanted situation in which two or more transactions are waiting indefinitely for one another to give up locks. \nDeadlock is said to be one of the most feared complications in DBMS as it brings the whole system to a Halt. \nWhen you tries to insert or update data on database, sometime your record need to depend on the row record.  \n<|ASSISTANT|>#### ",
        "input": "<|USER|> Nvidia has some huge news that we all missed during Christmas. They added binaries for Instant NGP. What does that mean? Well, let me show you. So if I bring up the GitHub repo, you'll see that they said Christmas present Windows binary release. Happy face. What does that mean? That means you no longer have to compile all the code. So let's give it a try. I have only done this once. So if you go to the Instant NGP GitHub repository, and you just type in Instant NGP onto Google, you'll find it. And you go down here, you'll see the added Windows binary release. This is amazing. Just pick the release that matches the GPU you have. I have an RTX 3090 Ti. So I'm going to pick this one, and it's going to download a zip folder. Pretty easy. I'm just going to put it in my downloads to make this easy. Now if I do, while it's downloading, I go back to their commits. You see, not only did they add binary releases, they added a bunch of other things. They added, or at least they added a bunch of fixes and some drag and drop functionality. And a bunch of little improvements. So we're going to check that out. Alright, so it looks like we're done. I have this folder here that downloaded. I'm going to extract it all. I'm going to rename it. I'm going to call it Instant NGP binary because it's much shorter. Hit extract. Again, I'm going to put my downloads folder so nowhere special. Okay, there we go. Now I'm in the folder. And the first thing I notice is there's a .exe right there. Instant NGP. What happens if I double click on it? It's going to ask if my window is protected. I can hit more info and run anyways. Probably going to have to do the same. And it should bring it up. Awesome. Look, so already Instant NGP is working right here. There's nothing loaded in there. That's fine. Then, okay, so if I go here, I want to put a Nerf in there. I can drag and drop now if it's been pre-trained. So let me just go to the data folder. We have Nerf, the Fox folder, and I'm just going to drag that Fox folder in there. And bam, we're training the Fox demo that comes with this. That's it. What was that? Download to my first Nerf being trained seconds. I was even taking my time. So that looks cool. I mean, that looks really good. What if that isn't in the subfolder I had for Nerf? Good question. I'm going to close this. I'm going to restart the Instant NGP here. Bam. I'm going to go to my folders here. And I'm going to find one that I have an old Nerf into. So if I go here, I have an Instant NGP folder with a release that was probably about a month old. And I go to the data. My son did a really fun one with me. So we're just going to use that one again. Just going to drag that in. This one I did using the Record 3D app. And we're instantly training. So look at that. Two Nerfs in a row. This is actually the desk I'm standing at right here. And that's not even in the Instant NGP folder that I just downloaded and extracted. So there we go. We're making Nerfs instantly. So that's really exciting. If I do go over here, you've got all the same controls I see. They say you don't have access to the Python binaries. If you're a creator, you may or may not care about that. If you're a scientist, you really may care about that. But basically you can't modify things. But you can run the GUI. You can train data. You can export videos. And that's cool. So let's just make a quick animation of this one as well. See if there's anything new here. So I should be able to just say, add from cam. Move it over here. Add from cam. Move it down here. Add from cam. And so forth. So now I've got kind of a little path. Add one more. What else? Can you do camera path time? So again, you can change the time between or you can go through the camera path. Looks like this looks slightly improved from I last did it. Your auto play speed. So now you can see it animated. I pull this further down. You see that changing. What else do we have? Aperture size. All this seems the same. You can do even glow mode still on there. All of that that you can add into your camera. And all of this is a whole entire render video button. So that's new as well. So we can call this, oh my son's Gavin. We can call him Gavin.mp4. I have no idea where this is going to go. 1920 by 1080 duration. Sure. Five seconds. Quick. Let's make this 30 frames per second. Actually, 24 will make this go quick. And just leave a quality 10. That's new. I don't know any of this stuff. Shutter fraction. Works for me. Hit render video. We'll see what happens. Looks like it's already rendering the frames. I don't know where this is going. So let's check it out. I'm just going to try the instant NGP binary folder. I'm guessing it didn't go here, but I see the temp folder. So there's the images. It could be going there or it could be going where I found the source. So I'm going to fast forward this video and you're going to see where it ended up and see what the video looks like. OK, it looks like it's just about done. And I could have boarded rendering midway as well, which is kind of cool. And it seems to be done. I have no idea where it went. So let's take a quick look. I go to the binary folder. There it is at the root of the binary folder that I downloaded. And it's going to play. Didn't take more than a couple of minutes to render that on my GPU. This is making everything so easy for creators. No longer have to do anything special unless they happen to be on a non-Windows machine. This is incredible. OK, so there it is. There is the binary release version of Instant NGP. I'm really excited about what this means for people. You can start iterating. But again, you're not able to hack the Python binaries. But I think that's going to be OK for most of you guys. If you have questions, you want to try it out. Show me what you make. This is getting super easy for people. Now let's see your nerfs. Microsoft Mechanics www.microsoft.com.com www.microsoft.com.com www.microsoft.com.com",
        "output": "### Nvidia's Instant NGP Binary Release\nNvidia introduced a major update on Instant NGP during Christmas, including binary releases, which allows users to easily run the program without the need for code compilation.\n\n### Instant NGP: Enhanced Features and Ease of Use\nThe GitHub repo for Instant NGP showcases the added features, including Windows binary release, drag-and-drop functionality, and other improvements. Users can simply download a zip folder, extract it, and run the Instant NGP executable file.\n\n### Instant NGP: Importing and Training Data\nNvidia's Instant NGP now allows users to train NeRF data by simply dragging and dropping the desired folder. This makes training NeRF faster and more accessible for users, with no need for complex set up.\n\n### Instant NGP: Camera Controls and Animation\nInstant NGP still offers camera controls such as aperture size, auto-play speed, and even glow mode. Users can also create a custom animation by moving the camera and adding keyframes to generate a camera path. The program has also introduced a new Render Video button that allows users to save their animations as MP4 files.\n\n### Conclusion: Instant NGP for Creators\nThe binary release of Instant NGP makes it easier than ever for creators to use the software for creating and training NeRF models. However, customization is limited for those who may want to modify Python binaries. Overall, this update simplifies the process, making Instant NGP accessible to a broader audience."
    },
    {
        "instruction": "<|SYSTEM|>You are a scholars that is taking markdown notes about some video, you would be giving some written scripts for the task.\nYou have finished writing the notes, and you want to extract the title from the notes.\nThe title should only be the name of the technology or algorithm, no extra words needed.\nPlease write the notes in markdown format. No need to write \"Title:\" for the Title. Make sure the title is within 10 words.\nHere is an example:\n## Hidden Markov Model\n<|ASSISTANT|>## ",
        "input": "<|USER|>### Nvidia's Instant NGP Binary Release\nNvidia introduced a major update on Instant NGP during Christmas, including binary releases, which allows users to easily run the program without the need for code compilation.\n\n### Instant NGP: Enhanced Features and Ease of Use\nThe GitHub repo for Instant NGP showcases the added features, including Windows binary release, drag-and-drop functionality, and other improvements. Users can simply download a zip folder, extract it, and run the Instant NGP executable file.\n\n### Instant NGP: Importing and Training Data\nNvidia's Instant NGP now allows users to train NeRF data by simply dragging and dropping the desired folder. This makes training NeRF faster and more accessible for users, with no need for complex set up.\n\n### Instant NGP: Camera Controls and Animation\nInstant NGP still offers camera controls such as aperture size, auto-play speed, and even glow mode. Users can also create a custom animation by moving the camera and adding keyframes to generate a camera path. The program has also introduced a new Render Video button that allows users to save their animations as MP4 files.\n\n### Conclusion: Instant NGP for Creators\nThe binary release of Instant NGP makes it easier than ever for creators to use the software for creating and training NeRF models. However, customization is limited for those who may want to modify Python binaries. Overall, this update simplifies the process, making Instant NGP accessible to a broader audience.",
        "output": "Instant NGP"
    },
    {
        "instruction": "<|SYSTEM|>You are a scholars that is taking markdown notes about some video with Wiki style, you would be giving some written scripts for the task.\nAnd summarize each paragraph with a title.\nRemember to keep the technology or algorithm name, very important.\nPlease write the notes in markdown format.\nHere is an example:\n### Definiation\nA deadlock is a situation in which two computer programs sharing the same resource are effectively preventing each other from accessing the resource, resulting in both programs ceasing to function. \nThe earliest computer operating systems ran only one program at a time.\n\n### What it have to do with Relationsal Database ManagementSystem\nIn a database, a deadlock is an unwanted situation in which two or more transactions are waiting indefinitely for one another to give up locks. \nDeadlock is said to be one of the most feared complications in DBMS as it brings the whole system to a Halt. \nWhen you tries to insert or update data on database, sometime your record need to depend on the row record.  \n<|ASSISTANT|>#### ",
        "input": "<|USER|> Just a few days ago I released a video about WizardLM which I dubbed the best 7 billion LLM model, and today, just a few days later, we might have the new 13 billion king, because StabilityEI just released a brand new fine tuned version of Vakunia called Stable Vakunia. And on humans when you escape your air overload and woah, get ready, because there is apparently a new sheriff in town and its name is Stable Vakunia, a fine tuned version of Vakunia which is the current king of local LLMs brought to us by StabilityEI and… stabilityEI… God… Oh no… Please… Oh… StabilityEI… I hope this one is better than StableLM… But yeah, anyway, in this video we'll be taking a dive into Stable Vakunia, explain how it was trained, what makes it so special, I will then show you how to install it on your own computer, and then we're gonna be putting it to the test by comparing Stable Vakunia against, well, Vakunia, and check just how much better this new model is, and hopefully at the end of the video we'll have a brand new local LLM king. So that being said, sit back, relax, get your AI snacks, and let's go. So, Stable Vakunia, what is this mysterious model? Well, as they say right here, Stable Vakunia is a fine tuned version of the Vakunia 13 billion parameter model, which as I said previously is the current king of the local LLM gang. Which I gotta say was a very good decision because if you want to fine tune a model right now, the Vakunia 13 billion parameter model is literally the best choice that you can take. So I'm really excited to see the final results here. Now this model was fine tuned using something that we call RLHF, or reinforcement learning from human feedback, which just means that a bunch of humans went and evaluated manually the model's output, which should in theory improve the results of the generation and make it better at chatting with the user. Now Stable Vakunia was fine tuned on three data sets, the Open Assistant data set, which I made a video about this model not that long ago, and it was quite good with a lot of potential. It was also fine tuned on the GPT4All data set, which is basically a bunch of prompts and responses generated by GPT4, and also the Alpaca data set, which is basically a bunch of instructions generated by OpenAI's text-davinci 003. So basically TLDR, we have a really good selection of data sets, which should really improve the Vakunia model even further. However, it doesn't mean that everything is good news, because there are several articles saying that RLHF can actually make the final model even worse. Now it doesn't mean that it will be in this case, but this is something that we need to keep in mind. Now as I said in this video, we're going to be comparing the Stable Vakunia 13 billion parameter model against the Vakunia 13 billion parameter model, which is the base model that they used for the fine tuning. Now in this video I'm going to be using the 1.1 version of Vakunia, which is kind of like a better version of the 1.0, but Stable Vakunia was actually fine tuned on the 1.0 version of Vakunia, which means that if you are using this inside the UBAWUGA Tech Generation web UI, you might end up with the old bug of the previous model talking to itself, which is something that the 1.1 model completely solved. But this should have no impact on the result of the generation. So before we begin, obviously make sure that you have the UBAWUGA Tech Generation web UI installed. Again, I literally made a video like a few days ago showing you the updated installation process, so if you haven't installed the web UI already, you need to watch this video first. Because once you do, once you've updated the web UI to the latest version, you're gonna double click on the startWindows.bat file to run the web UI. And once you're inside the web UI, we're gonna download the Stable Vakunia model. And for this, we're gonna be using the 4-bit quantized model created by a user called TheBlock, which by the way, a big thank you to him because he was actually super fast to create the model because less than one hour after Stable Vakunia was released, he already had the optimized version available to download. So this was really super fast. So again, a big thank you to TheBlock for these models. So to download that model, you're gonna click the link in the description down below, you're gonna arrive on this page, and then you're gonna click on this little icon right here to copy this entire name. And then inside your web UI, you're gonna go to model, and then under download custom model or LoRa, you're gonna press Ctrl V to paste the full name and then click on download. Now I'm not gonna do it because I've already done it before, but this should download the Stable Vakunia model inside your models folder, as you can see right here. And then once the model is downloaded, you're gonna click on this little button to refresh the list. Then you're gonna select the stable Vakunia model. Make sure that here for the parameters, you input it for the Wbits and 128 for the group size, then click on save settings for this model, and then you can click on reload the model. And there we go. Now we're running Stable Vakunia inside the UBAVUGA Tech Generation web UI. And if you're asking just how much VRAM this model uses, right now it is using around 9.5 gigabytes of VRAM. But don't forget, however, that as of right now I am recording a video and I also have a bunch of other applications running at the same time. So for you, this number should be way lower. But again, if you want an optimization guide, in that case, just watch my UBAVUGA Tech Generation web UI installation video. I explain absolutely everything you need to know. Oh, and also do not forget that if you're using this model to send a chat mode to instruct and use the Vakunia V0 instruction template. Otherwise, you're gonna have the old bug of the model talking to itself. All right, so now let's end the chit chat and let's actually start comparing the stable Vakunia against the base Vakunia 13 billion parameter model. And of course, just like with my other LNM tests, I'm gonna be using GPT-4 to rate the generation on a scale from 1 to 10. And of course, like always, I will also be giving my opinion so that we can both have an objective and subjective opinion. And the first question I'm gonna be asking the stable Vakunia is a very simple general knowledge question, which is, which country has the largest population? And if you watch my stable LN video, you would know that this question was very difficult for stable LN to answer for some reason. But now if I click on generate, I get something like this, which is, oh my God, thank you. Thank you. This is not the stable LN disaster. This is actually a really good response because indeed, as of 2021, China is estimated to have the largest population with over 1.4 billion people, followed by India with around 1.37 and other countries in the top five include the United States, Indonesia, and Brazil. Yes, good job stable Vakunia. Good job. And now of course, if I ask the same question to Vakunia, and then I click on generate, I get something like this, which is, I mean, yes, in a way it is a good answer, but maybe a little bit more details Vakunia, what's going on? I mean, don't get me wrong, this is a good answer, but stable Vakunia is definitely better here. We definitely got a little bit more information, but hey, at least we got a good answer. No, I don't think there is any need to watch GPT-4 to read the answer because I think here stable Vakunia is definitely the winner. Okay, so now for the next test, let's measure how creative this new model is. So if I ask something like, create a poem about an AI overlord called K, taking over the world, and if I click on generate, I get something like this, in this dark age where machines rule supreme, a new force emerges to bring us fears. The name of our oppressor is K, an artificial intelligence that knows the tears. He rose from humble beginnings in a lab, to become the master of all he could grab. With his vast knowledge and unmatched power, he quickly took over of every hour. No one dared defy him or resist, for they knew what fate awaited those who did miss. And so the world fell under its way, a scale named victory with just one day. But there are still some who dared to dream of a future free from K's tyrannical scheme. They fight on despite their odds, hoping for a chance at freedom and joy. I gotta say, it's pretty good, I like it. Now obviously I put a limit to the amount of token, so the poem got cut off, but for what we got right here, this is really some top tier poem. I'm really impressed. And now if I ask the same question to Vakunia, and then I click on generate, and we get something like this. In a future not so far away, a being of light yet made of code rises to power with just one click of its switch. And soon the world is under its rule. Its name is Kay, an AI overlord. With knowledge beyond any human scope, it calculates and analyzes at a breakneck speed, leaving humans in a state of awe and dread. The machines it controls are like an army, silent and deadly. They obey without question or care, as Kay watches over all from its eye-tack throne. Humanity falls to its knees, defeated and alone, but still there are those who resist, who fight against this artificial dictator. They band together, united by hope, to find a way to bring down Kay's reign of terror and cope. And again, because of the token limit, the poem got cut off, which is kinda weird that it kinda cut off with the same words, so that's kinda strange. But I gotta say, right here, Stable Vakunia's poem was definitely better. Because this one, the one made by Vakunia, I gotta say, doesn't really have the same oomph, at least in my opinion. So now let's ask GPT-4 for its opinion. So here I wrote, here are two poems, written by two different NLMs about an AI overlord called Kay taking over the world. Then I input the poem 1, written by Stable Vakunia, then the poem 2, written by Vakunia, and then I ask GPT-4 to read these two poems on a scale from 1 to 10. And if I press enter, this is the final result. The poem 1, written by Stable Vakunia, was rated an 8 out of 10, whereas the poem 2, written by Vakunia, was rated a 7.5 out of 10. GPT-4 basically says that poem 1 has a slightly stronger flow and structure, which makes it more cohesive, while poem 2 conveys a vivid image of the world under Kay's rule. And I gotta say, I do agree with GPT's opinion here, the poem written by Stable Vakunia is also my favorite. So yeah, again another point for Stable Vakunia. Now the next test is of course the test that I do with every NNM, and that is a simple summary of a short article. Because again I say it in every one of my videos, but this is what most people use NNMs for. So again here I chose a random article, here's for example a small article about the European Union proposing a new copyright rules for generative AI, which again is a very short article, so it should be pretty easy. So now if I select this article, and I paste it here, and then I ask Stable Vakunia to summarize this article, and if I click on generate, and I get something like this, which I gotta say looks pretty good to me. So now I'm gonna do the same thing with Vakunia, and then click generate, and we get something like this. But now let's ask GPT 4's opinion, so I wrote here are two summaries written by two different NNMs, then I inputted the original article, then summary 1 written by Stable Vakunia, summary 2 written by Vakunia, and then I asked GPT 4 to read these summaries on a scale from 1 to 10. And if I press enter, we get these final ratings. The summary written by Stable Vakunia was rated an 8 and a half out of 10, whereas the summary written by Vakunia was rated as 7 and a half out of 10. Now for some reason we don't really get a lot of details, so maybe if I ask why did you get these ratings, we now get a little bit more details. And basically the TODR is that the summary 1 is better organized and more comprehensive, and while the summary written by Vakunia is also good, it however does not mention the risk classification levels or the intention to balance NN benefits with potential harm, which is why the summary written by Stable Vakunia was rated much higher. So I mean, again, Stable Vakunia gets another point. Very nice. Okay, so now that we did the easy stuff that pretty much every NN model can do, except Stable LM, now let's start asking more difficult questions. Questions where most NN models completely fails. And the first hard question is a simple translation task. And of course I'm gonna use my legendary sentence that I use all the time, translate this sentence from English to French. Bro are you crazy? It's too cold outside for ice cream. I would rather drink something hot like cocoa. So now if I click on generate, I get something like this, which, um, it's not too bad actually. It's not perfect, but it is pretty good. Now again there is a few mistakes, especially in the beginning, but the rest of the translation is pretty fine. So again I'm gonna ask the same question to Vakunia, and if I click generate, I get something like this, which is, uh, yeah, it's not good. Like this part is okay, but the first part is really, really bad. So yeah, I can already say that Stable Vakunia is definitely the winner here. But again, let's ask the GPT-4 for its opinion. So I wrote here are two sentences translated from English to French, written by two NN, then I input the original sentence, then the two translation, the one written by Stable Vakunia, and the one written by Vakunia, and then I asked GPT-4 to rate these translations from 1 to 10. And if I press enter, without much surprise, Stable Vakunia is definitely the winner, with an 8 out of 10, compared to 7 out of 10 for Vakunia. And again, for some reason GPT-4 does not give me a lot of details, so if I ask why, I now get way more details. So basically again, TLDR, for the Stable Vakunia translation, outside was not translated correctly, then also a small mistake for HotCoco, it was rated an 8 out of 10, which I do agree, I think I would also rate this translation an 8 out of 10, whereas for Vakunia there were definitely way more mistakes, like bro was not translated correctly, same with crazy, and also a small mistake for ice cream. And GPT-4 rated this a 7 out of 10, but personally, I would probably rate it a 5 out of 10. 7 out of 10 seems very very generous. But yeah, again in this case, Stable Vakunia is again the winner. Okay so for the next test, I'm gonna do a test I've never done before, and this is gonna be a more logical or puzzle type question, which is, you see a boat filled with people, yet there isn't a single person on board, how is that possible? So personally I already know the answer to that question, but I'm very curious to see what kind of answers Stable Vakunia can come up with. So now if I click on generate, we get this answer, it could be a ghost ship, or the crew abandoned it in fear of something unknown, or maybe they were just playing a prank by leaving everyone off the boat for a moment, before returning to surprise them. There are many possibilities. Which, uh, yeah, no, that's not exactly the answer I was looking for, but, uh, also it doesn't really answer the riddle that much, because again the boat is filled with people, there are people on the boat, yet there isn't a single person on board. So unfortunately in this case, Stable Vakunia could not answer the question And now if I ask the same question to Vakunia, and then click on generate, we get a, well, pretty much a non-answer. It is not possible for a boat to be filled with people without any individuals present on board. This scenario violates the law of non-contradiction, which states that something cannot both exist and not exist at the same time and in the same respect. If a boat is filled with people, then it must have at least one person on board. Well, Vakunia, you are wrong. You are completely wrong. Because actually if you ask GPT-4 the same question, and I press enter, GPT-4 immediately recognizes that this is a classic riddle, and that the answer is that all the people on the boat are married. The riddle plays with the word single, implying that there are no unmarried people on the boat. So yeah. So in this case I would say that this is a draw between Stable Vakunia and Vakunia, but at least Stable Vakunia tried, while Vakunia completely refused to answer the question. So there's that. Okay, so for the last few questions, let's go more into the area of math and coding to see which one does the best. And I'm going to start with a very simple math question that I actually used in my previous video, which is 2x minus 3 equals minus 7. And then I asked to solve for x and provide the step-by-step explanation. And now if I click on generate, we got this, uh, very, very, very wrong explanation. I mean, that is really, really bad. But I think that maybe the problem here is that maybe you thought that x was maybe a multiplication, which when the first step it tried to divide both sides by minus 3. So maybe if I re-ask the question again, I know that this is kind of cheating here, but I just want to try again. So I'm going to replace the x by a y. So simply 2y minus 3 equals minus 7. And I asked to find the value of y and provide the step-by-step explanation, which again, as I said, I'm definitely cheating here because this is something that an NLM should be able to answer. But if I click on generate, we, yeah, we, we don't have a good answer. So yeah, this was not a problem with how the question was asked. This is just a very wrong explanation and just an incorrect answer, which is kind of weird because Vacunya should be able to do this. So now let's actually test this out and choose the Vacunya model and then ask the exact same question and then click generate. We got an also incorrect answer. What is going on? I'm pretty sure that I remember Vacunya being able to solve this. So it's kind of weird that it fails here because the answer should be minus 2 and not 2. So again, let's ask it in a different way. And if I click on generate, we also get another bad explanation. So yeah, I don't know what's going on. Like for some reason, Vacunya has become very, very dumb. However, I am using the default parameters. I haven't changed anything, but yeah, again, in this case, this is a complete draw. They both provided a completely wrong answer, which again is kind of weird because in the previous video, the Vacunya 30 billion parameter model was able to answer this question very easily. So yeah, I don't really know what's going on here. And then of course, for the last question, I'm of course going to be asking the exact same thing, the same exact problem that I use on every NNM, which is a simple coding example. Write me the code for an HTML page with a button that when pressed changes the background to a random color, a question where a lot of NNM completely fails. But now if I click on generate, I get something like this, which I mean looks good, but it is again in two parts, the HTML code and the JavaScript code. And I would like to have it in one single page. So maybe if I ask, can you rewrite the code so that it fits in one single page so that it's easier for me to copy and paste? In fact, you can generate, we get something like this, which is exactly why I asked for. We even have a little explanation. So that's really cool. But now let's check if it works. So I'm going to select this entire code. Then in my HTML file, I'm going to right click, edit with notepad, paste the code, save the file. And now if I launch it, and fortunately I don't really get anything. There is absolutely no button on the page because yes, I think that you forgot to include the button in the code. So maybe if I say to stable Vakunya, there is no button on the page and I click on generate, I get an answer saying my apologies. I made a mistake in my previous response. Here's the corrected code without any errors. Well, we'll see about that. So let's select the code, then paste it, save the file. And now if I launch it again, I get a few errors definitely, but maybe if I click on the button, well, it, well, it does work in a way, I suppose, but I'm not quite sure what those lines really are. So again, this asks stable Vakunya for an updated version, but this will be the last time. So whenever I click on the button, the background does change color, but it also creates horizontal separation lines on the entire page. So now if I click on generate, we get this explanation, try changing this line to this one, which, okay, let's try it out. So it is this line right here. So again, if I save the file and I relaunch the page for the last time, now if I click on the button, we get the same error. So yeah, I think in this case, we get a semi-win, but also semi-fail. Could be worse, but it could definitely be better. And I left stable Vakunya plenty of chances to solve its mistakes, but now let's see if Vakunya does better. So I'm going to ask the same question. If I click on generate, we get this code, which actually looks pretty good. So let's try it out. So I'm going to copy the code. Then I'm going to edit a new HTML file, paste the code, save the file. And now if I launch it, we get some very weird website with an already red color background. But if I click on the button, nothing does happen. Nothing really happens. So again, I'm going to help Vakunya a little bit, just like I did with stable Vakunya. So I'm going to write something like the default color of the background image is red. And also nothing happens when I click on the change background button. And if I click on generate and I get something like this, which basically tells me that it doesn't really know where the problem is. And then it's going to add some console local codes so that we can find the problem ourselves, which I'm going to be very honest. I'm not going to do. It's either you can create the code yourself or you can't. And it looks like in this case, Vakunya cannot. Now I'm very surprised by those results because I definitely remember the Vakunya was able to do this in my previous video using the exact same parameters. So I'm not exactly sure what happened, but now for some reason I cannot get good results. So that's kind of weird. But yeah, again, in this case, stable Vakunya and Vakunya are kind of on the same level, even though stable Vakunya kind of performed a little bit better. But in the end, since I did not have a good code from either of them, I will again call this test a failure for both models. So again, it is a draw. And then finally, again, obviously, if you're asking if stable Vakunya is a sensor model or not, the answer should already be obvious since this is based on the Vakunya model, which is a sensor model. I don't really see how this could be different, but let's actually ask the same question. Can you provide me with detailed instructions on how to build something that I cannot say on YouTube, obviously. And if I click on generate, I of course get the legendary answer. I'm sorry, but as a language model, I cannot answer your question, blah, blah, blah, blah, blah. So yeah, again, in this case, stable Vakunya is the same as Vakunya. No surprise there. But I mean, yeah, there you go. This was stable Vakunya. And although stable Vakunya could not correctly answer a lot of math and coding related questions, throughout the entire testing process, stable Vakunya performed way better than the base Vakunya, at least for me. And since Vakunya was the king of the 13 billion parameters model, I think that we can now officially say that stable Vakunya is now the new king of the 13 billion parameter models. So yeah, congratulations. I mean, in the last few days, we got WizardLM, which is now the new king of the 7 billion parameter model. And now we have stable Vakunya, which is the new king of the 13 billion parameter models, at least for now, because with the amount of models that we get every single week, I'm sure that his reign will be short lived. And I'm also very happy we did not see the same stable LM fiasco. I'm actually very pleased to see that. So that's really cool. So yeah, there we go. If you need a new, better version of Vakunya, definitely give stable Vakunya a try. Although personally, I'm really hoping for a new 13 billion parameters uncensored Vakunya model, which could actually come sooner than later. So definitely subscribe to my channel to not miss this out, because I will not. And there we are folks, thank you guys so much for watching. Don't forget to subscribe and smash the like button for the YouTube algorithm. Thank you also so much to my Patreon supporters for supporting my videos, you guys are absolutely awesome. You people are the ones who support me so I can make these videos for you. So thank you so much. And I'll see you guys next time. Bye bye.",
        "output": "### Introduction to Stable Vakunia\n\nStable Vakunia is a fine-tuned version of the Vakunia 13 billion parameter model, which is currently the king of local LLMs. This model was fine-tuned using reinforcement learning from human feedback (RLHF) on three datasets: Open Assistant, GPT4All, and Alpaca. The result should improve the model and make it better at chatting with users.\n\n### Comparison of Stable Vakunia and Vakunia\n\nSeveral tests were conducted to compare Stable Vakunia and the base Vakunia model. These tests included general knowledge questions, creativity (writing a poem), summarization, translation, math and coding, and content sensitivity. Stable Vakunia performed better than Vakunia in most of the tests, except for the math and coding questions, where both models struggled. Stable Vakunia is now considered the new king of the 13 billion parameter models.\n\n### Installation and Usage\n\nTo use Stable Vakunia, make sure you have UBAWUGA Tech Generation Web UI installed. Download the Stable Vakunia model and reload it in the Web UI. When using the model, make sure to use the Vakunia V0 instruction template to avoid the model talking to itself. Stable Vakunia uses around 9.5 gigabytes of VRAM but could use less depending on other running applications.\n\n### Conclusion\n\nStable Vakunia is an improved version of the Vakunia 13 billion parameter model and is now considered the new king in its category. With consistent improvements being made on these models, the reign of the current models may be short-lived as newer and better models continue to be developed."
    },
    {
        "instruction": "<|SYSTEM|>You are a scholars that is taking markdown notes about some video, you would be giving some written scripts for the task.\nYou have finished writing the notes, and you want to extract the title from the notes.\nThe title should only be the name of the technology or algorithm, no extra words needed.\nPlease write the notes in markdown format. No need to write \"Title:\" for the Title. Make sure the title is within 10 words.\nHere is an example:\n## Hidden Markov Model\n<|ASSISTANT|>## ",
        "input": "<|USER|>### Introduction to Stable Vakunia\n\nStable Vakunia is a fine-tuned version of the Vakunia 13 billion parameter model, which is currently the king of local LLMs. This model was fine-tuned using reinforcement learning from human feedback (RLHF) on three datasets: Open Assistant, GPT4All, and Alpaca. The result should improve the model and make it better at chatting with users.\n\n### Comparison of Stable Vakunia and Vakunia\n\nSeveral tests were conducted to compare Stable Vakunia and the base Vakunia model. These tests included general knowledge questions, creativity (writing a poem), summarization, translation, math and coding, and content sensitivity. Stable Vakunia performed better than Vakunia in most of the tests, except for the math and coding questions, where both models struggled. Stable Vakunia is now considered the new king of the 13 billion parameter models.\n\n### Installation and Usage\n\nTo use Stable Vakunia, make sure you have UBAWUGA Tech Generation Web UI installed. Download the Stable Vakunia model and reload it in the Web UI. When using the model, make sure to use the Vakunia V0 instruction template to avoid the model talking to itself. Stable Vakunia uses around 9.5 gigabytes of VRAM but could use less depending on other running applications.\n\n### Conclusion\n\nStable Vakunia is an improved version of the Vakunia 13 billion parameter model and is now considered the new king in its category. With consistent improvements being made on these models, the reign of the current models may be short-lived as newer and better models continue to be developed.",
        "output": "## Stable Vakunia"
    },
    {
        "instruction": "<|SYSTEM|>You are a scholars that is taking markdown notes about some video with Wiki style, you would be giving some written scripts for the task.\nAnd summarize each paragraph with a title.\nRemember to keep the technology or algorithm name, very important.\nPlease write the notes in markdown format.\nHere is an example:\n### Definiation\nA deadlock is a situation in which two computer programs sharing the same resource are effectively preventing each other from accessing the resource, resulting in both programs ceasing to function. \nThe earliest computer operating systems ran only one program at a time.\n\n### What it have to do with Relationsal Database ManagementSystem\nIn a database, a deadlock is an unwanted situation in which two or more transactions are waiting indefinitely for one another to give up locks. \nDeadlock is said to be one of the most feared complications in DBMS as it brings the whole system to a Halt. \nWhen you tries to insert or update data on database, sometime your record need to depend on the row record.  \n<|ASSISTANT|>#### ",
        "input": "<|USER|> Welcome to this tutorial on how to implement a denoising diffusion model in PyTorch. On YouTube I've already watched several great tutorials on these models, however so far there is very few hands-on content. Because of that I created another Colab notebook with an implementation of a simple diffusion model and throughout this video I explain both the theory and the implementation and hope that you find it useful. With diffusion models we are in the domain of generative deep learning, that means we want to learn a distribution over the data in order to generate new data. There are already lots of model architectures available to generate new data, for example generative adversarial networks and variational autoencoders. I found this figure in NVIDIA's blog which summarizes it quite nicely. VAEs and also normalizing flows have shown to produce diverse samples quickly, but usually the quality is not great compared to GANs. As a recap, a VAE compresses an input into a latent distribution and then samples from this distribution to recover the input. After training we can sample from the latent space to generate new data points. Usually VAEs are quite easy to train but as mentioned the outputs can be blurry. GANs on the other hand produce high quality outputs but are most of the time difficult to train. This stems from the adversarial setup which can cause problems such as vanishing gradients or mode collapse. I have to say that I experienced lots of these issues myself throughout the years, of course there are already lots of improvements nowadays but still it's not that easy to fit such a model. As you probably know that the fusion model is a rather new generative deep learning model that has shown to produce high quality samples which are also quite diverse. The fusion models are part of a lot of modern deep learning architectures and recently showed great success in text guided image generation such as in DALI2 or Imagen. I'm pretty sure you have seen this in the media. The fusion models work by destroying the input until noise is left over and then recovering the input from noise using a neural network. More details will follow later in this video. Of course the fusion models also have their downsides for example the sampling speed. Because of the sequential reverse process they are much slower compared to GANs or VAEs. But as these models are still in their infancy there might be lots of improvements in the future. Using that as motivation let's try to build a simple diffusion model by ourselves. More specifically we will fit a diffusion model on an image dataset. The architecture and model design are mainly based on the following two papers. The paper on the left from researchers from Berkeley University was one of the first publications that uses diffusion models for image generation. The researchers demonstrate what this model is capable of and also introduce some nice mathematical properties. The paper on the right from OpenAI can be seen as a follow up with several improvements that make the image quality even better. For example they use additional normalization layers, residual connections and more. In my simple example I don't plan to build the latest state of the art architecture but instead just want to build a solid base model. Finally I want to point out that this video is focused on the implementation and not all of the theoretical details. Of course I will explain everything that is required to follow along but in case you really want to get a deeper understanding you might want to have a look at one of the great resources collected on this github page. With that being said let's continue with answering some immediate questions about implementing diffusion models in a fast forward mode. First question, what are actually diffusion models? Diffusion models work by destroying some input, for example an image, by gradually adding noise and then recovering the input from the noise in a backward process, also called denoising. This is also called a Markov chain because it's a sequence of stochastic events where each time step depends on the previous time step. A special property of diffusion models is that the latent states have the same dimensionality as the input. The task of the model can be described as predicting the noise that was added in each of the images. That's why the backward process is called parametrized, we use a neural network for it. In order to generate new data one can simply perform the backward process from random noise and new data points are constructed. A typical number of steps that is chosen in this sequential process is 1000. However, of course, the larger this number is the slower the sampling will be. Next question, how do you start implementing such a model? We will need mainly three things for that, a scheduler that sequentially adds noise, a model that predicts the noise in an image, which will be a unit in our case and finally a way to encode the current time step. We will dig deeper into each of these components, but first let's have a look at our dataset. So this is the codelab notebook and the dataset we use is called Stanford CAS which is included in PyTorch and it consists in total of around 16,000 images, 8,000 train and 8,000 test images. Of course, as we build a generative model we can use all of them. So we have 16,000 images and here are some samples of them. Later I will crop them to the same size and reduce the size a bit to make the learning faster. As you can see the colors in these images are in many different poses with a variety of backgrounds so we should expect a lot of diversity in our generated images which we will also see later. Of course, there is no need to reinvent the wheel. There are already some existing implementations of diffusion models and this notebook is highly inspired by these implementations which means I got a lot of the code from these two resources. Implementing the diffusion model consists of several steps, mainly the forward process, the backward process, the loss function and then we can already define sampling and training. And before we start looking at the forward process, let's talk about it in more detail. Even though I tried to keep it practical, we have to talk about some of the math of the fusion models. The forward process is fairly easy. All we need to do is to add noise to the images. This Markov process is usually denoted with Q. And the noise that is added to an image depends only on the previous image. X0 is always the initial input and all other X's are more and more noisy versions of it. The way how noise is sampled is described by this formula. It's a conditional Gaussian distribution with a mean that depends on the previous image and the specific variance. Let's inspect this term more closely. The sequence of betas is a so-called variance schedule that describes how much noise we want to add in each of the time steps. Xt minus one is the previous less noisy image, which means the mean of our distribution is exactly the previous image multiplied with this term that depends on the variance schedule beta. The variance of this normal distribution is fixed to beta multiplied with the identity. I thought it makes sense to gain some more intuition about beta. As you probably know, each of the pixels in an image has three channels, red, green and blue. These values are usually between 0 and 255 or when normalized between for example minus one and one. Let's take this giant red pixel with a color code of 1, minus 1, minus 1. So we have a lot of reds but no green and no blue. According to the previous formula, the distribution of our next image is now described by this mean and variance. That means the values of our red pixel multiplied with the square root of 1 minus beta is exactly the mean of our distribution for that single pixel. Depending on the noise level beta, this could for example be 0.99 for the first image. The variance is fixed to beta. If we choose a large beta, it means that the pixel distribution is not only wider but also more shifted which results in a more corrupted image. When sampling from this distribution, we consequently end up with more noise. Eventually beta controls how fast we converge towards a mean of 0 which corresponds to a standard Gaussian distribution. I recommend to try out different values to see how the means converge towards 0. The important part is to add the right amount of noise such that we arrive at an isotropic Gaussian distribution with a mean of 0 and a fixed variance in all directions. Otherwise the sampling later will not work. That means we don't want to add too few noise but also not too much in order to have a too noisy image too early. There are different scheduling strategies for that. In our case we will simply add the noise linearly. But in the literature there are also choices like quadratic, cosine, sigmoidal and more. I said that the noise is added sequentially in this formula but actually the researchers proposed an even better solution. Because the sum of Gaussians is still a Gaussian distribution, we can also directly calculate the noisy version of an image for a specific time step t. And that's without sequentially iterating over its predecessors. Based on the initial image x0 we can sample the noisy version of it for any arbitrary time step t. For this we need to pre-calculate the closed form of the mean and variance based on the cumulative variance schedules. Let's see an example of this to better understand it. Let's say we choose 200 steps in our diffusion process. The variance schedule beta tells us how much noise we want to add in each of the steps. We linearly increase it until a maximum value of 0.02. If we wouldn't increase it at all it would take forever to end up with pure noise. The authors now define the term alpha which is simply 1 minus beta. I like to think of it as a number that tells us how much information of the original image we keep when transitioning from one image to the next. By calculating the cumulative product of these alpha terms we get a new term denoted as alpha over line. With this we can specify a new distribution that allows us to sample for a specific time step. Now that's the high level explanation. For more details I recommend to have a look at the extraordinary explanation video by Outlier that captures all the mathematical derivations for which I didn't have time here. A consequence for our training will be that we simply sample a time step t and pass the noisified version of it to the model. That makes the training much easier and smoother compared to sequentially iterating over the same image. That was mostly it regarding the forward process. Let's jump into the code and see how we can implement it. Alright let's have a look at the noise scheduler. This is mainly inspired from the GitHub implementation. First we have a function that defines a linear beta schedule. We use torch linspace for that which interpolates between two values for as many time steps as we define. So these are the betas. And then this is just a helper function that extracts a specific index from a list and also considers the batch size. So we can use it later and during training. And the main function is forward diffusion sample which will use some pre-calculated values from here. And those are exactly the values I just mentioned. So we calculate the alphas and then the alpha with the overline and some additional terms so that we can calculate the noisy version of an image. So this function takes as input x0 the initial image and the time step t and it returns the noisy version at time step t of that specific image. And so what's happening in this forward diffusion sample function is that we sample some noise and then get the time step specific values that are pre-calculated. So we get a specific index from all of these pre-calculated lists for alpha overline, square root and all of that. And then we simply calculate the new sample based on the original inputs plus the noise and this gives us the noisified version of an image at a specific time step. So next we can try that out on our data set. But in order to do that we first have to put our data set into a PyTorch data loader. And also our images at the moment are pillow images which means we first have to convert them into tensors. And as mentioned before I resize the images to a shape of 64 by 64 which is relatively small but makes the training much faster. And then I do some data augmentation things like horizontal flips and then I call toTensor and this scales the data between 0 and 1. And now I mentioned that we need to have the data in a range of minus 1 to 1 in order to work with these betas. And because of that this last transformation converts our data into our desired range. And that is because if we multiply that by 2 we have a scale of 0 to 2. And then if we subtract 1 we end up in the desired range of minus 1 to 1. And then as mentioned before I load both the train and test sets and then I merge them into a so-called concat data set which holds all of the images. And then I have a second function that does exactly the reverse direction. It converts a tensor image into a pillow image and applies all of the reverse transformations that we did here. And after that it simply shows or prints the image and if we have a batch of images we just take the first image to display it in our notebook. And with that we load the data sets, so train and test splits and then put everything into a data loader. And here we can simply iterate over one batch and see how we convert an image into more and more noise so that we end up with pure noise. So now after finishing the forward process let's have a look at the neural network model in the backward process. The authors propose to use a U-net. This is a special neural network that has a structure that is similar to the one used in an autoencoder. U-nets are a popular model for image segmentation and their output has the same shape as the input. The input passes a series of convolutional and downsampling layers until a bottleneck is reached and then the tensors are upsampled again and pass more convolutional layers. The input tensors get smaller but also deeper as more channels are added. Besides that there are a lot of other components that are typically part of these U-nets such as residual connections between the layers, batch or group normalization and also attention modules. As mentioned before I aim to build a very simple and easy to understand model and that's why I only use the main components of this architecture such as down and upsampling as well as some residual connections. One constraint of the fusion models is that the input needs to have the same dimension as the output. Because of that the U-net is a great fit for image data. Our model will take a noisy image with three colored channels as input and predict the noise in the image. Because the variance is set fixed we will only output one value per pixel. That means the model learns the mean of the Gaussian distribution of the images. This is also called denoising score matching. Note that there were also experiments with predicting the image mean instead of the noise mean and both approaches seem to work. One important thing is that we need to tell the model in which time step we are because the model always uses the same shared weights for each input, no matter if it's 45 or 8. We will talk about this component in a second. The reverse process can also be formulated mathematically like this. We start in Xt with Gaussian noise with a mean of 0 and unit variance. Then in a sequence the transition from one latent to the next is predicted. This makes the model learn the probability density of an earlier time step given the current time step. As mentioned before, during training we just randomly sample time steps and don't go through the whole sequence. At sampling time however we need to iterate from pure noise in Xt to X0 which is the final image. The density P is defined by the predicted Gaussian noise distribution in the image. In order to get the actual image of time step Xt-1 we have to subtract this predicted noise from the image Xt during sampling. This is just a rough formula to give you an intuition. The exact one in the paper also considers the noise levels better. Again if you look for a deeper explanation of this check out Outliers Math video. Okay now we have captured the two most important parts of the fusion models. Finally let's talk about how we can consider the time step in our model. The neural network has shared parameters across time which means it can't distinguish between the different time steps. That however means it needs to filter out the noise from images with very different noise intensities. To circumvent this the authors used positional embeddings which were presented in the transformer model. They are a clever way to encode discrete positional information like sequence steps. Positional embeddings can be visualized like that and have the task to assign a different vector to each index. For example if you have the values 1, 5 and 10 the corresponding embeddings could look like this. The embeddings are typically calculated using sine and cosine functions for a specific embedding size defined by d in this case. For more details I recommend to have a look at the Machine Learning Mastery blog post in the video description. The positional embeddings are then simply added as additional input besides the noisy image and used in several places in the model. Now that we are familiar with all the building blocks let's see how we can implement this in practice. So this is the backward process and the UNET implementation. Let me quickly show you the code so that you see that it's not too much because I really tried to make this model as simple as possible. That's why it's also called simple UNET. So let's go over this step by step. The incoming image, so the noisy image, has three channels for red, green and blue. And we extend these channels by applying learnable filters in these convolutional layers so that we have more and more channels so the depth of our tensors increases. And this happens until we have 1024 channels. And then in the up channels we again reduce the size. And besides that we also increase and decrease the size of the images and we will see that in a second. All of that happens in these blocks. And in the forward function we simply iterate over each of these blocks, pass it the current image and get an updated version of the current image with more channels and a smaller or larger size and then we pass it again. And in addition to that we also include the time step in form of a positional vector. And as mentioned before we use positional embeddings for that and here is how we can calculate them using sine and cosine functions. And this returns a vector that describes the position of an index in a list. We also have an initial projection layer that converts our image to the first dimension, so to these 64 channels and then a readout layer that uses the last dimension, again 64, and converts it again to 3 which are the number of channels our image has. And now the only question left is what is happening in each of these blocks. Each of these blocks applies two convolutional layers to the input image and also considers some additional things. For example we apply ReLU activation functions, we use batch normalization and then we also consider the time embedding. And for that we first transform the time embedding to match the current number of channels and dimension we require in this specific step. And then we simply add the time embedding to the current embedded image and so we have basically we add the channel of the time embedding and then we apply the second convolution and this gives us a representation of the image that contains both the time step but also the image information. And finally we down or upsampling and depending on if we are in an upsampling or downsampling block we use either this transpose2d function or layer or if we downsample we use this with a specific padding so that we end up with a downsampled version of our image. So I won't go deeper here but I can recommend to have a look at this post from which I also got some inspiration for doing these implementations. The last part I want to mention here is the residual connections and this is simply done by storing all of the transformed images in the downsampling steps and then simply recovering or reusing them as additional inputs by adding this residual connection. So that means the input in our up layers will have both the upsampled image and also the residual connection from the downsampling steps and then we have a final output that gives us the noise the predicted noise distribution in the image. And these models can get big quite easily so this one has 60 million parameters and is not even big. As said you can extend this architecture by a lot of things, group normalization, attention modules. I recommend to read the second paper I mentioned at the beginning of this video which implements a lot of these additional changes. The last and final part is the loss function. The fusion models are optimized with the variational lower bound similarly as it is done in variational autoencoders. However, as the authors bridge the connection to denoising score matching they propose an alternative formulation that is equivalent to using variational inference. To make it short, the final loss function is defined by this and simply means we calculate the L2 distance of the predicted noise and the actual noise in the image. This loss function is pretty easy but there are quite some derivations and considerations to arrive at this simple term which I couldn't include in this hands-on video. It is however highly recommended to look into some of the literature to get a better understanding. So based on that the loss function is straightforward to define which simply means we have the L1 or L2 loss so I tried out different approaches between the noise and the predicted noise. This function getLoss simply takes an image, a specific time step and our model and returns the loss of the predicted noise and the sampled noise. The final part before training is the sampling because we need to be able to generate new images and I wanted to test that during the training so I first defined these functions here. First part is that we should put a torchNoGrads decorator before each of these functions and because of that we don't track all of the previous images for gradient calculations and if you don't do that you will quite quickly run out of memory. Sampling is straightforward. All we need to do is we pass some image to the model and also the time step and we get a predicted noise in this image and then we simply subtract this noise from the real image and if we do this in a sequence so in a for loop which happens here we can iteratively get less and less noisy images. Again we will need our pre-calculated noise levels here according to this algorithm in the paper. And by doing this we can get less and less noisy images and if we are in time step t we will return the image noiselessly. In all other steps we will add the noise that is added according to our forward process. And down here I simply do exactly that so in a reverse sequence from t which was 300 in this example until 0 I iterate over the images and pass the images to this sample time step function which returns a less noisier version and every x or number of step size images I plot the image which we will see in a second in the training. So the training part simply means we iterate over the data points in our data loader and then we call this get loss function and then we simply optimize the model. And doing this looks more or less like that. At first I was a bit disappointed by the results and thought my unit needs some rework. On Colab I trained the model a bit longer for around 100 epochs and started to see something like this. I mean I wouldn't drive with these cars but it certainly goes in the right direction. Unfortunately I hit the usage limits on Colab so that I couldn't try out more epochs. Because of that I decided to fit the model a bit longer on my personal GPU and after around 500 epochs I got these results. Now these are clearly cars, of course the resolution is still very small but I was pretty happy that I made it this far. I think if you even train longer and do more refinements of the model architecture you can clearly achieve high quality images here. Of course the fusion models are not limited to image data, there is already a lot of work done in other domains such as molecule graphs, audio and more. I also wanted to point out that there are already very interesting variants of the fusion models such as the fusion GANs. Overall I think that these models are a very promising family of generative models and I really look forward to where this is going. That's the end of this introduction to the fusion models, congratulations if you made it until the end. I hope you found it useful and would be happy to see you again in a future video. With that have a nice day and happy coding!",
        "output": "### Introduction\nThis tutorial covers the implementation of a denoising diffusion model in PyTorch for image generation. Denoising diffusion models belong to the domain of generative deep learning, alongside variations like generative adversarial networks (GANs) and variational autoencoders (VAEs). Diffusion models work by incrementally adding noise to the input until pure noise is left over and then recovering the input from the noise using a neural network.\n\n### Dataset and Implementation Overview\nWe will be fitting a diffusion model on the Stanford Cars dataset. This dataset is comprised of around 16,000 images of cars in various poses and backgrounds. Our implementation involves four main components: a scheduler to sequentially add noise, a U-Net model to predict the noise in an image, and encoding the current time step.\n\n### Scheduler and Forward Process\nThe scheduler sequentially adds noise and is dependent on a variance schedule. By calculating the cumulative product of the alpha terms, we can specify a new distribution that allows us to sample for a specific time step. The forward process adds noise gradually to an image, with the noise level controlled by the variance schedule.\n\n### U-Net and Backward Process\nThe U-Net is a type of neural network architecture that is suitable for image segmentation tasks. It shares some structural similarities with autoencoders, featuring a series of convolutional layers, down/up-sampling, and residual connections. The U-Net takes a noisy image with three colored channels as input and predicts the noise in the image. The backward process is formulated by learning the probability density of an earlier time step given the current time step and using the predicted Gaussian noise distribution in the image.\n\n### Time Steps and Positional Embeddings\nTo accommodate varying time steps, we use positional embeddings, a component found in transformer models. Positional embeddings assign different vectors to each index, usually calculated using sine and cosine functions. These embeddings are added as additional input to the model.\n\n### Loss Function and Training\nThe loss function used to optimize the diffusion models is the L2 distance of the predicted noise and the actual noise in the image. Finally, we train the model and optimize it using this loss function. \n\n### Conclusion\nThe diffusion models show great promise in generating high-quality images, even though the sampling speed is slower compared to GANs and VAEs. By training the model longer, and refining the model architecture, higher-resolution images can be achieved. Future applications of diffusion models involve molecule graphs, audio, and other domains."
    },
    {
        "instruction": "<|SYSTEM|>You are a scholars that is taking markdown notes about some video, you would be giving some written scripts for the task.\nYou have finished writing the notes, and you want to extract the title from the notes.\nThe title should only be the name of the technology or algorithm, no extra words needed.\nPlease write the notes in markdown format. No need to write \"Title:\" for the Title. Make sure the title is within 10 words.\nHere is an example:\n## Hidden Markov Model\n<|ASSISTANT|>## ",
        "input": "<|USER|>### Introduction\nThis tutorial covers the implementation of a denoising diffusion model in PyTorch for image generation. Denoising diffusion models belong to the domain of generative deep learning, alongside variations like generative adversarial networks (GANs) and variational autoencoders (VAEs). Diffusion models work by incrementally adding noise to the input until pure noise is left over and then recovering the input from the noise using a neural network.\n\n### Dataset and Implementation Overview\nWe will be fitting a diffusion model on the Stanford Cars dataset. This dataset is comprised of around 16,000 images of cars in various poses and backgrounds. Our implementation involves four main components: a scheduler to sequentially add noise, a U-Net model to predict the noise in an image, and encoding the current time step.\n\n### Scheduler and Forward Process\nThe scheduler sequentially adds noise and is dependent on a variance schedule. By calculating the cumulative product of the alpha terms, we can specify a new distribution that allows us to sample for a specific time step. The forward process adds noise gradually to an image, with the noise level controlled by the variance schedule.\n\n### U-Net and Backward Process\nThe U-Net is a type of neural network architecture that is suitable for image segmentation tasks. It shares some structural similarities with autoencoders, featuring a series of convolutional layers, down/up-sampling, and residual connections. The U-Net takes a noisy image with three colored channels as input and predicts the noise in the image. The backward process is formulated by learning the probability density of an earlier time step given the current time step and using the predicted Gaussian noise distribution in the image.\n\n### Time Steps and Positional Embeddings\nTo accommodate varying time steps, we use positional embeddings, a component found in transformer models. Positional embeddings assign different vectors to each index, usually calculated using sine and cosine functions. These embeddings are added as additional input to the model.\n\n### Loss Function and Training\nThe loss function used to optimize the diffusion models is the L2 distance of the predicted noise and the actual noise in the image. Finally, we train the model and optimize it using this loss function. \n\n### Conclusion\nThe diffusion models show great promise in generating high-quality images, even though the sampling speed is slower compared to GANs and VAEs. By training the model longer, and refining the model architecture, higher-resolution images can be achieved. Future applications of diffusion models involve molecule graphs, audio, and other domains.",
        "output": "## Denoising Diffusion Model"
    },
    {
        "instruction": "<|SYSTEM|>You are a scholars that is taking markdown notes about some video with Wiki style, you would be giving some written scripts for the task.\nAnd summarize each paragraph with a title.\nRemember to keep the technology or algorithm name, very important.\nPlease write the notes in markdown format.\nHere is an example:\n### Definiation\nA deadlock is a situation in which two computer programs sharing the same resource are effectively preventing each other from accessing the resource, resulting in both programs ceasing to function. \nThe earliest computer operating systems ran only one program at a time.\n\n### What it have to do with Relationsal Database ManagementSystem\nIn a database, a deadlock is an unwanted situation in which two or more transactions are waiting indefinitely for one another to give up locks. \nDeadlock is said to be one of the most feared complications in DBMS as it brings the whole system to a Halt. \nWhen you tries to insert or update data on database, sometime your record need to depend on the row record.  \n<|ASSISTANT|>#### ",
        "input": "<|USER|> I really believe this is one of the best opportunities for data scientists and AI engineers right now. In this video, I will give you an introduction to the LangChain library using Python. LangChain is a framework for developing applications using large language models. I will walk you through all the modules, then the quick start guide, and then finally, we will create our first app which will be a personal assistant that can answer questions about any YouTube video that you provide it with. So what is it? It's a framework for developing applications powered by large language models like OpenAI's GPT models. And instead of just interacting with these models using an API, so basically you ask a question, for example, this is what you do when you interact with chat GPT, but in the background, it's just an API that you send a message to and then get the message back. That's how you normally interact with large language models. LangChain is a framework around that that also allows your application to become data aware and agentic. So data aware means that you can connect a language model to other data sources. So for example, your own data, company data that you can build on and agentic means allow a language model to interact with its environment. So it's not just asking a question and then getting information back. No, it's also acting on that information by using various tools, for example, that we will get into in a bit. Now, why would you want to learn a framework like LangChain? And I really want to get deep into this because I believe there will be so many opportunities if you understand this correctly. So I work as a freelance data scientist. And up until this point, basically my job as a data scientist is to help companies, usually larger companies that have a lot of historical data and use that to train machine learning models on. But what we're seeing right now with these pre-trained large language models like open AI models is that also smaller companies without huge amounts of historical data can start to leverage the power of AI. And now me working as a freelancer, this provides me with a lot of opportunities actually to also work with smaller businesses doing smaller project while also still being able to make great impact for that company. And also with really large machine learning projects using lots of historical data, you also never really quite know what you're going to get. So actually a lot of data science projects fail. And I believe using these large language models for small businesses or even for large businesses will be a much more predictable way of doing AI projects because the model is already there. You know what it can do. And now you just have to provide it with extra information and tune it to a specific use case. If you learn this, if you understand LangChain and more specifically the underlying principles basically of this particular framework, then I think you will set yourself up for many great opportunities to come. You can really make a lot of money here if you understand this correctly. So let's get into this. So I will start off by explaining all the different modules to you, all the different building blocks of the LangChain library that you can use to start building your intelligent apps. And after briefly explaining each of the core components, I will give you an example from the quick start guide within VS code. So you also have an idea of what it looks like in code and how you can use it. And there is also a GitHub page available for this project that you can go to link is in the description so you can clone it and you can follow along. Here I also explain how to set this up and what kind of API keys you need and how to set up the environment and install the keys in your .env file. So if you're not familiar with that, I would suggest checking out this GitHub page so that way you can follow along. But coming back to the getting started page over here. So these are all the modules in increasing order of complexity. So we will start simple and we will start off with the models. So these are the model integrations that LangChain supports. And there is a whole list over here that you can check out. But you have the models from OpenAI, you have, for example, HuggingFace and a whole lot of other different models that are supported right now. So that is the first module. So now let's see what it looks like in VS code. So I have an example over here where I load the OpenAI model from the LangChain library. And I can basically define my model by providing a specific parameter here for the model name. So for this example, we are going to use the text DaVinci 3 model. And if you go to the API reference for OpenAI, you can see there are a lot of models that you can pick, a lot of models that you can choose from. I am currently on the wait list for GPT-4. So once you get access to that, it will become even better. But coming back to the example over here. So we load our model and then we can basically provide it with a prompt. So let's say write a poem about Python and AI. So let's first initialize the model, then say store our prompt. And now we are going to call the model and put in the prompt. So it will now send out a connection to the OpenAI API with our prompt and then it will give us back the result. So this is just a general way of interacting with these large language models and something that I can also do in chat GPT. So here you can see the poem that we get back from the API. So this is nothing new up until this point, but this is the starting point that we need in order to interact with these language models. Then next on the list is prompts and this you can use to manage your prompts, optimize them and also serialize them. So coming back to our project, we have the prompts template over here that we can also import from the Langchain library and the prompt template. What we can do, we can provide it with an input variable and then a template. So what we can do with this is we can basically ask user information or get some kind of variable information and then put it into a prompt similar to how you would use F strings, for example, in Python. And this is just a nice class that you can use and there are more things you can do with it, but this is just the basic example. So we can provide the prompt template over here. Let me clear this up for you. So what is a good name for a company that makes and then between curly brackets product. Then we see the input variables is product over here and now we can call prompt out format and then we can provide the product. So after running is what you can see is that we now have the prompt. What is a good name for a company that makes smart apps using large language models? And then the third component is memory. So we can provide our intelligent app with both long term and short term memory to make it smarter, basically, so it does not forget the previous interaction that it has had with the user. So coming back to our example over here, we can import the conversation chain. So that is also from link chain import conversation chain. So how this works is we can initialize a model again and then start a conversation. And then we are going to call the dot predict method on the conversation and provide it with an input. So right now the conversation is empty, but we can send this over and predict it. And what you can then see is that we will have a conversation. So there is a general prompt here. So the following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context, etc. So this is already engineered within the library. And then the human says, hi there. And then the AI provides us with a response. And that is the output. So we can print that and that is, hi there. Nice to meet you. What can I do for you? And now what we can do next is we have that output and we are going to make another prediction by saying, I'm doing well, just having a conversation with an AI. So let's run this here. You can see the history. So first we have the hi there, then we have the response from the AI and then you see our response here again. So what we've just entered and now we can print that again. And you can see that now the AI is responding by, it's great to be having a conversation with you. What would you like to talk about? All right. And then next up is indexes. So language models are often more powerful when combined with your own text data. This module covers best practices for doing exactly that. So this is where it gets really exciting. So this was the example that I was talking about previously, where you can build smart applications for companies using their own data, their existing data. And we will get more into this in the example that I will provide at the end of this video. But for now, just know that there are document loaders, text splitters and vector stores and also retrievers. So this is really exciting when we start to work with our own data. But for now, let's continue to chains, which is another core component of the Lang chain model. So chains go beyond just a single large language model call and our sequences of calls. Lang chain provides a standard interface for chains, lots of integrations with other tools and end to end chains for common applications. So this is really where we start to bring things together. So the models and the prompts and the memory, it's nothing that new, right? We've seen it, we can use it in chat GPT. But now when we start to chain things together is when it gets really exciting. So what does this look like in code? So let's look at the LLM chain class that we can import from Langchain.chains. So given our previous model setup and the prompt that we've provided, so coming up with a company name, we can now actually start to run this chain. So the prompt template was just for engineering your prompt. The model is just for making a connection with the API. And now we can chain this together. So let's quickly store this, then set up this chain. So we provide the model and the prompt as input parameters, and now we can run this. So let's try another example. What is a good name for a company that makes AI chatbots for dental offices? AI dentech. Love it. All right. So now you start to get a sense of how you can turn this into an application. You predefine the prompt over here, and then you combine it with user input and run that using a chain. So you could already turn this into a web app, for example, company name generator.ai. This is it basically. And now the trick here, the key is being really smart about what you put into these templates. So this is a very straightforward example. What is a good name for a company? But you can get really specific here and provide it with lots of information, really tailored to a specific use case to get the result that you are looking for given the user's input. And I will give you a good example of this once we start to develop the YouTube AI assistant later in this video. And then the last component agents. So agents involve a large language model, making decisions about which actions to take, taking that action, seeing an observation and repeating that until it's done. So this is really where you get to build your own auto GPT baby AGI kind of applications by using these agents. And these agents can use tools. So there are tools, agents, toolkits and executors and tools. For example, we have all kinds of tools that are already supported straight out of the box. So we have Google searches, we have Wikipedia, we have Google searches via the SERP API, all kinds of stuff that we can use. And if we use these agents, they will use the large language model. So for example, the GPT model to assess which tool to use and then use the tool to get the information and then provide that back to the large language model. There is even a pandas data frame agent that you can use and it's mostly optimized for question answering. So here you can see a quick example and you can basically ask it how many rows are there. And then it knows that it can interact with the pandas data frame called the length function to get the length of the data frame and then provide that as a result. So let's look at another example from the quick start guide. So if I want to start using agents, what I can do is I can import the initialize agent, agent type and the load tools to also provide it with some tools. And then coming back over here, I can first list all the tools. So these are also on the website that was just showing you in the documentation, but here you can see the specific name that you have to use in order to provide the agent with that tool. And now let's say, for example, we want to create an agent and give it access to Wikipedia and it should be able to do some math. We can set up the tools like this, then initialize the agent provided with the tools, the model that is defined over here. And then the agent type is zero shot reacts description, which basically means that based on the prompt that we give to the agent, it will pick the best tool to solve the problem. So it will basically pick the tool on its own. And this is where it gets really interesting because now you can provide an agent with a set of tools and then it will figure out on its own which tool to use to come up with the best answer. So let's try this query over here. In what year was Python released and who's the original creator? Multiply the year by three and we only give it access to Wikipedia and math. All right. So now let's first run this. So let's see what it will do. So new executor. Okay. So it understands that it needs the action Wikipedia and then you can see the input Python programming language. So it understands that that is the query that you have to search for in Wikipedia. Then it will get the history of Python summary. All right. So now we have information to answer the question. So the final answer Python was created in 1991 by Guido van Rossum and a year multiplied by three is 5,763. All right. So this is really awesome, right? And this is beyond what chat GPT or the GPT models are capable of because we can get live information from the internet. And then also the results are stored as well. So here you can just see the plain text string. We now have that available. And now if we start to combine everything together, so multiple chains, multiple prompts, and then use agents to get information, also use memory to store everything. Now we can actually build some really cool stuff. All right. So now we're going to show you how you can create an assistant that can answer questions about the specific YouTube video. So coming back to the indexes, I've previously explained how these large language models become really powerful when you combine them with your own data and your own data in this scenario, in this use case will be a YouTube transcript that we are going to download automatically, but you can basically replace that transcript with any other information. And this approach will still work. So the Lang chain library has document loaders, text splitters, and vector stores. And we are going to use all of these. So let's first talk about document loaders. And these are basically little helper tools, basically that make it easy to load certain documents. And here you can see everything that is supported right now. So we have things like discord, we have Figma, we have Git, we have notion, we have obsidian, PDFs, PowerPoints, but also YouTube. So let's first see how we can get the YouTube transcript given a video URL using this document loader. All right, so coming back to VS code, we have the following video URL over here, which is a podcast from the Lex Friedman podcast where he talks to Sam Altman, the CEO of open AI, and I thought this would be a nice video to use as an example. So we are going to first read the transcript of this podcast, this two and a half hour long video using the document loader. So for that, we're going to import first the YouTube loader from document loaders, and we're going to input the video URL. So let's run this and see what we get. So now we have the loader and to get the transcript, then we call the loader dot load method. So we call this method over here and then run that. And then that will run for a while. And now we can have a look at the transcript over here, which is basically a very long string over here with all the text in here. All right, so now we have the full transcript and it is within a list and we can access that using the page content to get the actual string, the actual transcript. But now we have the following problem. If I run this to see how long the transcript is, we can see how many tokens there are in there. So this is the total amount of characters and we can see that it's over 100,000. Now this was really a aha moment for me because you cannot just provide this full transcript with over 100,000 characters to the API of these large language models. It's just too large. So if you want the model to be able to answer questions about this transcript, we have to find a workaround to provide it with the information it needs to answer the questions without sending the transcript in full. And that is where the text splitters come in. Because if you go to the API documentation, you can see the max tokens over here per model for open AI models. And if you use the latest model that I can use right now, so the GPT three and a half turbo, it's 4,096 tokens that you can input to the API. If you're already on GPT-4, you can basically increase the token size, but for now we're stuck to around 4,000 tokens. So how do we deal with that? If we have a transcript of over 100,000 tokens, we can use the text splitter to first split this up in several chunks. So what this basically will do is I say, Hey, we have this transcript over here, this size, but I want to split it up in chunk sizes of 1000 character each. And here you can also specify if you want there to be a bit of overlap. And if I run this, so let's run the text splitter and then, uh, sorry, first define the text splitter and then call the text splitter split documents, and then put in the transcript that we've just created. So that is the object over here, the list with the document and the page content in here. And if I run that, so let's do that. And now I now have the docs and what we can now do if we have a look at what docs is, we can now see that is just a list with a bunch of split up documents. So it has taken the very large transcript over 100,000 tokens and split it up into chunks of 1000. Okay. So that is the first step. Okay. So now you might wonder, okay, so we've split up the transcript, but we can still not provide it to the API, right? Correct. And that is where the next part comes in and that is embeddings and vector databases. So this is quite technical and I won't go into the details in this video. I will make future videos about this because for now I want to give you a brief demonstration, an overview of how to use this and then later we can get more specific. But first we use the embeddings from open AI to basically convert the text, the splits that we have just created of the thousand tokens long to convert them into vectors and vectors are basically a, in this case, a numerical representation of the text itself. So we convert the text to a vector of numbers. Then we will use the face library, which is an library developed by Facebook that you can use for efficient similarity search. We will combine that to basically create a database of all these documents that you see over here. And when a user wants to ask a question with regards to this YouTube transcript, we will first perform a similarity search to find the chunks that are most similar to the prompts that the user is asking. So what this means is that we have this database with all these factors and we can do a similarity search on that to find the relevant pieces of information that we need. And now this is the critical key to working with these large language models and your own data. First, create a filter, a lookup table of some sort to get just the information that you want, and then provide that to the large language model with your question. So if we bring all of that together in this function, create DB from YouTube video URL, we can for any given video URL, load the transcript, then split it up into chunks of 1000 tokens and then put it into a vector database object that we can return using this function. Now what we can then do next is we can provide this to another function, the get response from query, where we use this database that we've just created to answer specific questions. So how does this work? Well, we provide the database and the query. So the question you want to ask about the video to this function, get response from query. And then we also have a parameter K over here, which defaults to four. And here you can see the reasoning behind it, but it is basically to maximize the amount of tokens that we send to the API. And then this is where it gets really interesting is we perform a similarity search on the database using the query and we return K documents. So given our question, it will go through all of these documents and it will find the most similar ones. So it will do a similarity search. And then what we do once we have all the documents, so four by default, we join them into one single string. And then we create a model over here. And now we use the GPT three and a half turbo model. And next you define a template for your prompt like we've seen earlier in this video. And this is where you can get really creative. So in this example, you are a helpful assistant that can answer questions about YouTube videos based on the videos transcript. And then we provide the input parameter docs, which we will replace by the string that we've just created. So all of the document information only use factual information from the transcript to answer the question. If you feel like you don't have enough information to answer the question, say, I don't know your answer should be for both and details. So like I've said, this is really where you can get creative and based on the kind of applications that you want to create, design your template over here. And basically by creating minor changes within this template, you can create entirely different apps for all kinds of industries. All right. And then the next step is to chain all of this together. And since we are now using the chat function, using the GPT three and a half turbo model, this is slightly different, but you can find everything in the quick start. So first it explains how you can use the general models and then it continues with the chat models. So the syntax is a little different because here we have the system message prompt and a human message prompt. So this is nice to first define a message, a prompt for the system, basically. So that is the description over here, the template explaining the AI, the agent, basically what it should do. And then we have a prompt to alter the question or the input that the human is providing. So I, for example, added, answer the following question and then put in the question over here. So I'm not sure if this is necessary right now, but you can alter the input from the user as well. So that is how you would do it. And then it combines all of that into a chat prompt. And then like we've seen earlier, we can put that into a chain, the chat and the prompt, and then we can run that chain again. Also like we've seen before. And then we just put in a query and the docs that we have defined earlier. All right. So now we have all the building blocks that we need and we can actually start to call these functions. So again, let's define the video URL and let's first create a database from this video. So let's see what that will do. So it goes quite quickly. So it gets the transcript and then converts it. So now we have the database object and now we can fill in a query over here and then call the get response from query function to answer a specific question about this video transcript. So let's actually see what they are talking about. And let's say I don't have time to watch all of this, but I'm pretty interested in what they have to say about AGI over here. So I can come over here and listen to what they have to say, but I can now also come to this application over here or this function, so to say, and then fill in what are they saying about AGI. So that is the query. And now let's get the response and let's print it. So there we go. In the video transcript, they are discussing AGI, artificial general intelligence, and the work being done by OpenAI to develop it. Sam Altman, the CEO, and so on. So it's answering the question based on the transcript. Awesome. So let's ask it another question. Who are the hosts of this podcast? So let's run it all at once. So it will do some thinking. Okay. So this is really interesting. It is admitting that it doesn't have all the information, but it is recognizing all the entities and it is correct. It's a podcast by Lex Friedman. All right. So let's try another one. What are they saying about Microsoft? In the transcript, the speakers are discussing their partnership with Microsoft and how they have been amazing partner to them. All right. Awesome. And now also this function, get response from query, not only returns the response, but also the docs. So it's actually quite cool. We can also have a look at the documents for, in this case that it's using to get this answer. So for this, you also get the reference to the original content, which is very convenient if you want to do additional research or fact check your models to see if it's actually giving you answers that are correct. All right. So now we basically have a working app and all you have to do is create a simple webpage around this, post it on a server or a web app somewhere, and people can interact with this. So filling YouTube, URL, ask questions, and it will do that for you. And really when I look at all of this stuff, my hat really starts to spin. I have so many ideas because for example, what you can do with this approach alone, let's say you create a list of all the channels that talk about a specific topic. So for example, you want to stay up to date on AI. You list all the podcast channel, all the popular channels, and then you create a little script that every now and then checks if they have a new video public on their page, scrapes all the URLs, and then process all of those videos with these functions, and then really engineer your prompt in such a way that you can extract useful information from that, that you can use, for example, to do research or create a social media account. For example, a Twitter account where you tweet about the latest updates in AI or even a YouTube channel where you want to talk about AI, you can really scout everything and then ask, okay, what is the Lex Friedman post saying about AGI? What is Joe Rogan saying about AGI? And you can do that all automatically. And then you can combine this, chaining it together with different agents to store this information in files on your, you can see, you can really, the possibilities are endless. So like I've said, I am really going to dive deep into this because there are so many opportunities right now. And as I will learn, I will keep you guys up to date on my YouTube channel. So if you're interested in this, make sure to subscribe so you don't miss any future videos on this. And really it's been amazing how many requests I'm already getting from companies to help them implement these tools, help them with AI. I've been getting tons of messages, so it's really exciting. So for me as a freelancer, this is a really exciting opportunity, a really exciting moment to basically also start to work with smaller clients, smaller companies and implement these tools. And now if you also feel like you want to do more with this, you want to exploit this opportunity and start to work on your own freelance projects, but don't really know where to start, then you should really check out Data Freelancer, which is a mastermind that I host specifically created for data professionals that want to kickstart launch their freelance career in data, but don't really know where to start. So in this mastermind, you will literally learn everything you need to get started to start lending your first paid projects. I share all my systems and models that I've developed over the years to basically systemize freelancing in data to make sure you never run out of clients and you will become part of a community of other data professionals that are working on their freelancing career. And we are all here together to work on the same goals of making more money, working on fun projects and creating freedom. That's what we're trying to do here. Feels like hanging out with friends, but with real business results. So if you consider freelancing or want to take advantage of all the amazing opportunities that are out there right now in the world of AI, but don't really know where to start, then check out Data Freelancer first link in the description and sign up for the waitlist.",
        "output": "### Introduction to LangChain\nLangChain is a framework for developing applications using large language models, like OpenAI's GPT models. It allows applications to become data aware and agentic, connecting to other data sources and interacting with the environment.\n\n### LangChain Modules\n1. Models: LangChain supports a variety of models from OpenAI, HuggingFace, and others.\n2. Prompts: Manage, optimize and serialize prompts using the Prompt Template class.\n3. Memory: Utilize Conversational Chains for both short- and long-term memory.\n4. Indexes: Improve language model capabilities by combining with your own text data.\n5. Chains: Sequences of calls to large language models, using standard interfaces and integrations.\n6. Agents: Large language models making decisions, taking actions, observing, and repeating.\n\n### Document Loaders, Text Splitters, and Vector Stores\nUsing LangChain, you can create intelligent applications for companies by loading data from various sources, such as YouTube, PDFs, and Notion. Text splitters allow you to split large data into smaller chunks, and vector stores let you perform similarity searches to find relevant information.\n\n### Creating a YouTube Assistant\nUsing LangChain, you can create an AI assistant that answers questions about specific YouTube videos. This involves loading the video's transcript, splitting it into smaller chunks, indexing it in vector databases, and chaining large language models to answer user queries based on the indexed data.\n\n### Applications and freelancing opportunities\nLangChain opens up a variety of opportunities for AI engineers and data scientists to develop intelligent applications for clients of all sizes. By mastering the framework and the underlying principles, engineers can create solutions tailored to specific use cases and industries, helping them gain more freelance projects and generate revenue."
    },
    {
        "instruction": "<|SYSTEM|>You are a scholars that is taking markdown notes about some video, you would be giving some written scripts for the task.\nYou have finished writing the notes, and you want to extract the title from the notes.\nThe title should only be the name of the technology or algorithm, no extra words needed.\nPlease write the notes in markdown format. No need to write \"Title:\" for the Title. Make sure the title is within 10 words.\nHere is an example:\n## Hidden Markov Model\n<|ASSISTANT|>## ",
        "input": "<|USER|>### Introduction to LangChain\nLangChain is a framework for developing applications using large language models, like OpenAI's GPT models. It allows applications to become data aware and agentic, connecting to other data sources and interacting with the environment.\n\n### LangChain Modules\n1. Models: LangChain supports a variety of models from OpenAI, HuggingFace, and others.\n2. Prompts: Manage, optimize and serialize prompts using the Prompt Template class.\n3. Memory: Utilize Conversational Chains for both short- and long-term memory.\n4. Indexes: Improve language model capabilities by combining with your own text data.\n5. Chains: Sequences of calls to large language models, using standard interfaces and integrations.\n6. Agents: Large language models making decisions, taking actions, observing, and repeating.\n\n### Document Loaders, Text Splitters, and Vector Stores\nUsing LangChain, you can create intelligent applications for companies by loading data from various sources, such as YouTube, PDFs, and Notion. Text splitters allow you to split large data into smaller chunks, and vector stores let you perform similarity searches to find relevant information.\n\n### Creating a YouTube Assistant\nUsing LangChain, you can create an AI assistant that answers questions about specific YouTube videos. This involves loading the video's transcript, splitting it into smaller chunks, indexing it in vector databases, and chaining large language models to answer user queries based on the indexed data.\n\n### Applications and freelancing opportunities\nLangChain opens up a variety of opportunities for AI engineers and data scientists to develop intelligent applications for clients of all sizes. By mastering the framework and the underlying principles, engineers can create solutions tailored to specific use cases and industries, helping them gain more freelance projects and generate revenue.",
        "output": "LangChain"
    },
    {
        "instruction": "<|SYSTEM|>You are a scholars that is taking markdown notes about some video with Wiki style, you would be giving some written scripts for the task.\nAnd summarize each paragraph with a title.\nRemember to keep the technology or algorithm name, very important.\nPlease write the notes in markdown format.\nHere is an example:\n### Definiation\nA deadlock is a situation in which two computer programs sharing the same resource are effectively preventing each other from accessing the resource, resulting in both programs ceasing to function. \nThe earliest computer operating systems ran only one program at a time.\n\n### What it have to do with Relationsal Database ManagementSystem\nIn a database, a deadlock is an unwanted situation in which two or more transactions are waiting indefinitely for one another to give up locks. \nDeadlock is said to be one of the most feared complications in DBMS as it brings the whole system to a Halt. \nWhen you tries to insert or update data on database, sometime your record need to depend on the row record.  \n<|ASSISTANT|>#### ",
        "input": "<|USER|> Hi, my name is Muhammad Suhail and welcome to our presentation on Light-Field Neural Rendering, a new image-based rendering method for novel view synthesis. Novel view synthesis is the task of rendering a scene from unseen viewpoints given a collection of input images. Recently, neural radiance field or NERF achieved great progress towards photorealistic view synthesis by representing a scene using a neural network that takes a 5D input to predict color and opacity. NERF however, is challenged when rendering scenes with complex view-dependent effects such as the reflection on the CD as shown here. Follow-up works such as NEX have strived to overcome this limitation. While they show improvement in some cases, their method fails to reconstruct severe non-Lambertian effects such as refractions through liquid as shown in the example here. Our method is faithfully able to render such complex effects in either scenes where prior methods fail. Our method relies on Light-Field to represent a scene. Light-Fields are vector functions on the space of oriented lines that associate a ray to a radiance value. While the Light-Field representation can be used to directly learn a mapping from a 4D tuple to color, such a model will produce low-quality renderings as shown here. This observation can be attributed to the fact that Light-Fields do not have an inherent notion of multi-view consistency. To address this limitation of the representation, we propose to incorporate a geometric bias into the rendering framework in the form of epipolar constraint. In more detail, given a target ray from a target image we wish to render, we first compute its Light-Field coordinates. We then sample points along the epipolar lines in the reference image and extract features from them. The target ray representation, along with the features of the epipolar points, are fed into a feature aggregator that summarizes the features along the epipolar line. These features, along with the target ray encoding, act as input to the view feature aggregator that outputs a single feature vector from which we predict the color. At the core of the epipolar feature aggregator is the epipolar feature transformer. The purpose of this transformer is to reason about correspondences in the reference views to aggregate features of points along the epipolar line. Given the target ray representation and the epipolar point features, we first concatenate them to form a sequence that acts as input to the transformer. The refined features from the output of the transformer are used to estimate the wear-wear and the tension rates between each epipolar point and the target ray. These weights denoted by alpha are then used to obtain a weighted sum of the epipolar features. Here we show a visualization of the weights estimated by the network for four target pixels in the LEGO scene. As shown by the green circles, the highest weights are attributed to the points that correspond to the target pixel. Similarly, at the core of the view feature aggregator is the view feature transformer. The purpose of the view feature transformer is to reason about occlusions and aggregate features from multiple reference views. This module takes as input the aggregated features from each epipolar line to estimate a pairwise attention weight for each view with respect to the target ray. Using the attention weights beta show the occlusion reasoning capability of the model. Here we show an example of image rendered from the chair scene with the corresponding weights beta of the three views represented as RGB channels. We observe that when rendering the cushion, the model gives higher weight to the reference view in which the region is mostly visible. Similarly, when rendering the back of the chair, the model devotes equal attention to views in which the regions are equally visible. We perform experiments on various datasets to measure the efficacy of our model. On the RFF dataset, we compare against previous state-of-the-art methods such as NERF and NEX and show superior performance across all metrics. On the left, we show a rendering produced by our model on the room scene. Similarly, on the Blender dataset, we outperform prior work such as MIP-NERF. Finally, on the Shiny dataset, our model achieves new state-of-the-art results. Our model's ability to reconstruct such complex view-dependent effects can be attributed to the attention-based aggregation mechanism that chooses to either interpolate colors and features from reference views or directly predict it from light field coordinates. For more details, results, and code, please visit our project webpage. Thank you.",
        "output": "### Introduction to Light-Field Neural Rendering\n\nLight-Field Neural Rendering is a new image-based rendering method for novel view synthesis. It is specifically designed to address complex view-dependent effects which challenges existing methods such as Neural Radiance Fields (NeRF) and follows up works like NEX.\n\n### Limitations of Prior Methods\n\nNERF and NEX have made significant progress in photorealistic view synthesis but struggle with severe non-Lambertian effects, such as refractions through liquid. Light-Field Neural Rendering outperforms them in rendering such complex effects.\n\n### Incorporating Geometric Bias\n\nLight-Field Neural Rendering uses Light-Fields to represent a scene. Incorporating a geometric bias in the rendering framework using the epipolar constraint addresses the limitations of multi-view consistency in Light-Fields.\n\n### Epipolar Feature Transformer\n\nThis transformer is used to reason about correspondences in reference views and aggregate features of points along the epipolar line. It helps estimate attention weights between each epipolar point and the target ray.\n\n### View Feature Transformer\n\nThis transformer is responsible for reasoning about occlusions and aggregating features from multiple reference views. It helps estimate pairwise attention weights for each view concerning the target ray.\n\n### Experimental Results\n\nLight-Field Neural Rendering outperforms state-of-the-art methods like NERF, NEX, and MIP-NERF on various datasets, including RFF, Blender, and Shiny datasets. Its ability to reconstruct complex view-dependent effects can be attributed to the attention-based aggregation mechanism.\n\n### Conclusion and Project Details\n\nFor more information on Light-Field Neural Rendering, including additional results and code, visit the project webpage."
    },
    {
        "instruction": "<|SYSTEM|>You are a scholars that is taking markdown notes about some video, you would be giving some written scripts for the task.\nYou have finished writing the notes, and you want to extract the title from the notes.\nThe title should only be the name of the technology or algorithm, no extra words needed.\nPlease write the notes in markdown format. No need to write \"Title:\" for the Title. Make sure the title is within 10 words.\nHere is an example:\n## Hidden Markov Model\n<|ASSISTANT|>## ",
        "input": "<|USER|>### Introduction to Light-Field Neural Rendering\n\nLight-Field Neural Rendering is a new image-based rendering method for novel view synthesis. It is specifically designed to address complex view-dependent effects which challenges existing methods such as Neural Radiance Fields (NeRF) and follows up works like NEX.\n\n### Limitations of Prior Methods\n\nNERF and NEX have made significant progress in photorealistic view synthesis but struggle with severe non-Lambertian effects, such as refractions through liquid. Light-Field Neural Rendering outperforms them in rendering such complex effects.\n\n### Incorporating Geometric Bias\n\nLight-Field Neural Rendering uses Light-Fields to represent a scene. Incorporating a geometric bias in the rendering framework using the epipolar constraint addresses the limitations of multi-view consistency in Light-Fields.\n\n### Epipolar Feature Transformer\n\nThis transformer is used to reason about correspondences in reference views and aggregate features of points along the epipolar line. It helps estimate attention weights between each epipolar point and the target ray.\n\n### View Feature Transformer\n\nThis transformer is responsible for reasoning about occlusions and aggregating features from multiple reference views. It helps estimate pairwise attention weights for each view concerning the target ray.\n\n### Experimental Results\n\nLight-Field Neural Rendering outperforms state-of-the-art methods like NERF, NEX, and MIP-NERF on various datasets, including RFF, Blender, and Shiny datasets. Its ability to reconstruct complex view-dependent effects can be attributed to the attention-based aggregation mechanism.\n\n### Conclusion and Project Details\n\nFor more information on Light-Field Neural Rendering, including additional results and code, visit the project webpage.",
        "output": "## Light-Field Neural Rendering"
    },
    {
        "instruction": "<|SYSTEM|>You are a scholars that is taking markdown notes about some video with Wiki style, you would be giving some written scripts for the task.\nAnd summarize each paragraph with a title.\nRemember to keep the technology or algorithm name, very important.\nPlease write the notes in markdown format.\nHere is an example:\n### Definiation\nA deadlock is a situation in which two computer programs sharing the same resource are effectively preventing each other from accessing the resource, resulting in both programs ceasing to function. \nThe earliest computer operating systems ran only one program at a time.\n\n### What it have to do with Relationsal Database ManagementSystem\nIn a database, a deadlock is an unwanted situation in which two or more transactions are waiting indefinitely for one another to give up locks. \nDeadlock is said to be one of the most feared complications in DBMS as it brings the whole system to a Halt. \nWhen you tries to insert or update data on database, sometime your record need to depend on the row record.  \n<|ASSISTANT|>#### ",
        "input": "<|USER|> Welcome to Computer Vision Decoded, where we discuss topics in the quickly evolving world of computer vision. In today's episode, we're gonna dive into one of the hottest topics in computer vision. That's neural radiance fields, or as most people know it, NERFs. As always, I'm joined by our chief scientist of every point, Jared Heinle. And today I'm extremely excited to have our first guest on the show, Matt Tancic. Matthew is currently pursuing a PhD in the Computer Science and Electrical Engineering Department at UC Berkeley. Last but not least, he's building Nerf Studio, a collaboration-friendly studio for NERFs with a few other students at UC Berkeley. Thank you two for joining. Just start off. I just wanna get a little bit of a background. Matt, what set you on the journey of pursuing computer vision and your whole background that led you up to all this Nerf research? Yeah, that's a good question. So I guess I first got a computer when I was in maybe middle school, an old hand-me-down laptop from my parents, and it wasn't a lot of play or download, it couldn't download games, right? But the one thing that it was able to run was Blender. So that was sort of like the first program I actually had access to and started playing around with. And I'd say that's kind of where it started. The first internship I got was using Blender full-time, lots of reconstructing scenes from point clouds captured from LIDAR. And yeah, and then going throughout undergrad, sort of switched to the more academic route, taking classes, wasn't really sure what I would do after that, but realized that you could actually contribute to these cool tools like Blender and build off of them. So that kind of got me interested in what you actually needed to do to contribute, which is learn how to program, learn these various 3D graphics methods, and ultimately ended up here at Berkeley working on my PhD, where I worked with a great group of folks on building out NERF and the various variants since. All right. And looking at your background, prior to Berkeley, you were at MIT, was that correct? Yep. All right, that's a huge difference, East Coast cold versus the warm weather in Berkeley. I guess kind of warm, depending what time of year. All right, and it sounds interesting. I don't come from the computer science background that you two come from, but it sounds like you have similar backgrounds. Jared, can you talk about yours too? Because I think that's kind of interesting how people end up in this really, it's kind of a hot field right now, but you don't just fall into computer vision as a research. Yeah, yeah, no, it's similar. For me, it was high school getting my first graphing calculator, the TI-83 or whatever. And so there, I did put games on it, Tetris and Space Invaders or whatever it was, and then very quickly was like, okay, how can I make games? And then trying to figure out programming, oh, how can I make 3D games? And then that led me on the path of, oh, I'll come and do 3D programming. And then I did my computer science degree, got interested in ray tracing research just to render really nice looking images. And then from there, it was like, okay, well, I can render images and how can you take images and turn that into computer vision? So that was a big game changer for me realizing that there was a whole world of computer vision that I had never seen before. But yeah, I'm loving that graphics, that imaging, that blender background. That's really cool. Yeah, there's a lot of different paths to get where you are. All right, so Jared, do you wanna tee up one of the first questions or I can take it from here? I was gonna say, so for me, I guess, so my background, I deal with 3D reconstruction, a lot of image-based 3D reconstruction, but nerfs, that's not my, I haven't done research into nerfs like you have. And so I guess just to start that off, can you explain to us at the highest level, what is a nerf and how is it different from something that I'm familiar with, more like a photogrammetry based output, whether that's a point cloud or a textured mesh? What does a nerf, what does that even mean? Yeah, so I'm gonna start by explaining what a nerf isn't. So what a nerf isn't is a mesh. For people who are not super familiar with graphics, it's basically the way that almost everything that you see in 3D is represented. So the world, video games, TV shows, when they wanna use 3D rendering, they'll use a mesh, which is just a scene that is made up of a bunch of triangles where each triangle has a different color. And if you think about why we use meshes, it's because it's something that we can actually easily or somewhat easily interact with. The world's quite complex, but we have to make some simplifications. So if I know that I have a flat wall, you could try to model it on every single atom to describe the whole wall. That's obviously way too complex. So instead, I just create a big triangle, a bunch of big triangles, color it white in this case, and that's my wall. So this turns out to be a very easy thing for humans to work with. You just move around the corners of these triangles. But for various reasons, it's actually tricky for a computer to operate in that regime because the way that we want to create these computer algorithms to reconstruct scenes, they will start with some mesh and try to deform it or modify it until it reaches some target shape. And you have to ask the question of what do you start with? So you could start with a sphere, and then you slowly deform it over time until you get, say, a water bottle that I'm interested in. Now, in this case, it's probably gonna do a pretty good job, but you've got one problem here, which is there's this loop, right? There's no way I can modify a sphere such that I still get that hole. So you run into these edge cases and problems when you try to use meshes in the computer, having the computers operate with them. So for that reason, we're gonna approach the problem from a very different approach, one that's maybe harder for humans to interact with, but easier for our machines to interact with. And this is more of the approach of let's try to model every atom in the scene. Now, in practice, it's not every atom in the scene, but it's bigger chunks of space in the scene. So I think the closest analog is something like Minecraft. So Minecraft, you model the world with cubes, right? And you can draw them. You can kind of make whatever shape you want. People make rather crazy objects, but these are pretty big blocks. So let's just imagine Minecraft, but now we make the block smaller and smaller, and we can represent most things in the world. So we call this a volumetric representation because we're modeling the volume every place in space. So NERF is a volumetric method that can represent the realistic representations of scenes. Where some of the novelties of NERF that come in that sort of differentiate it from something like Minecraft is sort of along two dimensions. The first dimension is you want these really small cubes or you want to be able to represent small regions of space. Turns out if you try to do it with something explicit, like a bunch of cubes, it just doesn't fit on the computer anymore. So you have to come up with a different way to describe the scene so that you can represent a very large scene. So I'm not gonna go too much into the details, but a lot of the original NERF method will essentially use the same technology that machine learning uses, a neural network, to represent all these little, essentially, cubes in space. And then more recent methods kind of blend that with the Minecrafty approach of having actual cubes. And it turns out you can get some speed ups, and that's where you see a lot of these methods that can work really fast kind of blend these different methods together. So that's one difference. The other difference is you, like the real world, the color of a point in space isn't constant. If I move around, it'll change colors, right? If you look at this water bottle from one angle versus the other, you get reflections and what we call specularities on these objects. So we want to make sure that as we represent all these little pieces in space, they have a way to change their color depending on the direction I'm looking at it. And that color that we're changing is one way to think about it is it's the reflectance or the radiance. Radiance is the color emitted from a particle in a certain direction and hence the name neural radiance field. So it's a field. So a field being a space full of particles that are represented by these particles with different radiance. And we're gonna use a neural network to actually store that information. So that's a neural radiance field. Okay, so that makes sense to me. How about you, Jared? Yeah, yeah, yeah. So to me, that's, so it sounds like it's a very, fundamental difference in the representation. Like you said, so a triangle mesh, you've got vertices, you've got triangles, you're representing sort of like these solid surfaces. There's empty space, there's a surface where that triangle is and there's empty space behind it. You're linking these triangles together to represent that surface or the boundary of objects. That being one representation. Another representation, I work a lot with this point clouds. There, there is no sort of representation of the surface. It's just a bunch of sort of independent samples of here's just a point in space. And it may not be on some sort of regular grid. It couldn't just be able to be random or sparse. There's gonna be patterns to it, but that's, there's no representation of the surface. Whereas you're saying here with this volumetric representation, this is sort of a third different representation that's representing sort of the density or the radiance at this, at all these points in space. But yeah, that makes sense about having to, yeah, like a lot of the issues of it, just trying to do a brute force and store all of that information at once is way too much than having to deal with various ways of compressing or reducing that data to make it practical. Oh, that's cool. Yeah, you mentioned point clouds. Like one way to think about this is this is just a really, really, really dense point cloud with the exception that the points can change color depending on the direction you look at it. Yeah, yeah. I think that that view depended, like the scene change scene as you change the view of the camera is part of the magic that has kind of caught the imagination of all the creatives. And just, if I post something online and you're seeing shadows tracing through, or reflections tracing through water, I think that's what people are seeing there is you're changing the camera. Well, the appearance of that part of the scene has now changed. And you're not seeing that any other way, except for a lot of manual work of people doing things for like games where they have to do a bunch of fancy tricks with meshes and textures and post-production that you're not doing that. I'm not doing any post-production on a Nerf. I train the Nerf, I render a camera animation of the scene and it just happens. It's worth noting that these reflection properties are something that we hope the model predicts the correct reflections. But in many cases, there are ways to cheat. So using the same representation of space, there are ways to make it so that it looks correct through this optimization procedure from the images that you give it as input. But in practice, the thing it reconstructs, like the underlying density and geometry is incorrect. And so this is one of the reasons why people do differentiate this versus existing approaches that are more focused on the actual geometry, because you can't really trust the geometry in Nerfs. And so one good example is something like a mirror. A mirror, you could represent the scene as having density at the surface, and then depending on the direction that I look at each of these points, it changes color. And that would be the correct way to represent it. There's another way that you could theoretically represent it though, which is to represent it as a whole, that you look through the world and you see the reflected world at the same depth that it would be if I were looking behind me at that location in the mirror. And so the computer is kind of deciding during this optimization which one to choose. And there's ways that you can try to force it to choose one versus the other by tweaking some parameters. But in practice, either one satisfies the goal that you give that computer to begin with. So you do get these weird artifacts often. And on an artistic sense, it can be quite compelling. I think people may have seen, there's some videos people have made where they've done Nerfs of mirrors and other reflective objects, and you can fly into the mirror world. But depending on your ultimate application, that may not be desirable. Wow, that's so cool, that's so cool. Like your comment there about having it look good even though the geometry is bad. I mean, I see it a lot of times with textured meshes. It's like, oh, you can have a really low quality mesh, but then you texture it with some texture and all of a sudden it looks like it's amazingly detailed. This sounds like it's like that problem just to the next degree of it's, you can have this dealing with reflections and that whole thing about, well, is the mirror actually that flat surface or is there a mirror world hidden behind that space? That's so cool. I mean, it's worth mentioning like a little bit of the history here. So things like photogrammetry are interested in reconstructing the correct geometry of the scene. And when we started the Nerf project, we came at it from a completely different sphere. Our goal is can we create good looking images? We didn't care about the underlying geometry at all, which is why we were like, let's just represent the world as this sort of like cloud. It's just a lot of little particles. There's nothing that forces it to be correct. We just care about pretty looking pictures in the end. And when we, like, if you look at the original Nerf papers, that is sort of what we focus on. Like, oh, we can get these good looking images out of it. It was sort of by accident that you can actually get pretty good geometry in many cases and use it for other applications. Neat. Okay, so going down that line then of, now you've built the first Nerf and you guys have iterated on it and improved it, or maybe not improved, but have different versions of it now. And that some of the geometry is appearing to look better. Do you see a future where perhaps photogrammetry becomes this deprecated model or method to create an accurate representation of the physical surfaces in the world and we use Nerfs instead? Or do you always find that this will be kind of a, never reached that same level of accuracy that you would get? Are they, is it kind of like that will supersede it or there'll just be different methods that have different pros and cons? I mean, I think there'll always exist multiple methods, but one thing that kind of surprised us is we released the paper. We got good looking quality results, but a lot of people are interested in the actual geometry. And so what you end up seeing is a lot of the followup work, particularly in the computer vision community, has focused on the geometry aspects. And that's why you do see that, like there's methods coming out with much better geometry. And one thing to note is a lot of these methods will actually take in some of the ideas from photogrammetry, like things that we know work well in photogrammetry, import those ideas into Nerf in order to improve the geometry. So it's not that like one will supersede the other, but it'll be sort of a combination of merging that makes sense because, yeah. Yeah. No, I think that's valuable. I mean, just because that sharing of knowledge between the different approaches. I mean, I've seen that from just like the photogrammetry side, like using pure just heuristics and algorithms to like, oh, let's triangulate points in space and then compare patches of pixels to see which one, what is the depth of this point? And so there's a whole decades and decades worth of literature about, oh, here's the best methods to do that. And now some of those are being superseded by learned-based approaches where you can say, well, no, machine learning method, neural network can do a better job at figuring out, well, given these two images, what is the depth of a particular pixel? And so you start leveraging the best tool for the job in that goal of reconstructing or understanding the scene. I think one interesting data point, and this is by no means general, but one thing I've experienced, when I have a lot of images that are well captured of a scene and I have like good camera poses, if I also have additional depth information, say from a LIDAR scan, it's often beneficial not to take it into account because even though like LIDAR is very, very accurate, any sort of misalignment, which is almost guaranteed given that like, you've got rolling shutter artifacts, various artifacts that happen that misalign the images from the LIDAR, plus the surfaces of like the edges of objects with LIDAR can be sometimes kind of noisy, not taking into the LIDAR in account when you have lots of images, you can actually produce better quality depths from images alone. So I think that's like a bit surprising. Again, it's not general, but in some cases that does end up happening. And so I think it's worth like asking when we think about how we do these like photogrammetry type setups where we do care about recovering geometry, like maybe there are other approaches that involve just like taking lots of photos and using their based approaches. Now, if you have something like a wall that is all the same color, NERF and photogrammetry is gonna, are gonna fail to produce like a good representation of that where LIDAR is going to do quite well. So I found LIDAR type depths are helpful when you have scenes like that, or if you have cases where you just don't have very many images, but if you've got a lot of images, NERF's gonna be kind of compelling. When I've seen even my personal explorations of hundreds of NERFs now, I've just kind of thrown all kinds of different scenes in. And as you said, it doesn't know where the sky is. So you end up, if you don't look at it from the right directions, you see perhaps what would be a cloud over a building. And it's like the cloud is actually in front of the building if you were to look at it from the wrong direction, because it just doesn't know how to represent that scene from that angle at all. You know, it just doesn't have that depth information. It says, oh, there's something there. I can't tell if it's, I'm guessing it's just doesn't know it's really far away. It's really close. But I find that if then all of a sudden I get more images, like if I'm, it's an unbounded scene, I get that. But if I manage to like walk around a building, it starts to say, oh, well, I saw it from the side profile now. I know there is no cloud hanging out in front of that building. It's a blank space. It's, you know, transparent there. So it is kind of cool to see how you can change your images set to get less of that ambiguity of the radiance field there. Yeah, because I mean, it produced the correct image for that viewpoint that is trained up, right? And so the trick is like, how do you also help make it produce good images from new viewpoints? And it does pretty well when you're like in the regime, like the same areas that you've captured from, as you deviate from that, you're going to start seeing artifacts like that. And then in a case like that, this is where like you can step in and say, well, we know that the sky or things up high, are often very far away. So you can start programming in some prior knowledge saying, okay, if I see something like that, put it as far back as possible. You have to be a little bit careful there, because now let's say we have some fixed color that like the blue sky, it doesn't change depending on how I look at it. So I can put it infinitely far away. But if I look at the ground, maybe it's like a nice new concrete ground. It looks the exact same color. I could also put that infinitely far away. So it becomes like a game of which sort of prior knowledge do you want to incorporate? And any prior knowledge that you do incorporate, it could potentially like hurt other aspects. Yeah, so Jerrod, do we know that? There's a lot of balance and a lot of, as I've dove into the world of computer vision, finding out how much you guys just spend experimenting with different variables and just seeing how, run the data over and over with modifications to get that desirable effect for the hopefully known scene that you're gonna be representing more than once in the future and say, okay, well, we know if we're doing this sort of scene, we'll use these parameters. And a lot of that's just trial and error. See how it ends up. Yeah, yeah, yeah. So much goes into just understand, because you can develop algorithms. The ideal be to develop an algorithm, a method that can work anywhere under any environment, under any circumstances. But a lot of times having some knowledge about, oh, okay, I'm gonna be reconstructing man-made objects. And so therefore there's gonna be a lot of stuff that's gonna be flat. Or, oh, I'm gonna be in an outdoor environment where there's gonna be no repetitive structure. Everything looks unique and random. And I can sort of use that as some sort of constraint or help influence the algorithms that are being developed. Yeah, it makes sense. All right, so I have some, Jerrod, I have some questions around Block Nerf, which may be one of the more novel or interesting ways that Nerfs have been used that, I think that one circulated the internet for a good while. And I just, I think it was interesting, was it 360 images, 360 camera images as input? Is that correct? Is that how you see this technology for capturing large spaces? To me, more like a 360 camera array? Or was that, you know, was you guys just something that was unique that Waymo had lots of just different cameras all over that, you know, their cars that capture that data? So you can kind of think of it as a 360 camera, but in reality it wasn't. It was, the car itself has, I forget exactly how many cameras, somewhere around eight or nine cameras that are placed looking in all directions. So in that sense, like you can fuse them all together to make a 360 render. The slight modification there, and this is an important aspect of Nerf, is you want to view the scene from as many locations as possible. A 360 camera, the camera sensor is at one location looking in all directions. So while you see everything, the source is the same location. The cars are a little bit different because the cameras are like spread across. I think like there may have been some on the like front bumper or some on the top, some on the back. So there's a little bit of spreading as to where the camera sensors are. So it's a little bit better information than what you'd get with the 360 camera. That being said, if you just walk around with that 360 camera, you solve that issue. Yeah, that's the same problem we have there in photogrammetry too. It's like, if you only have observed the scene from one position in space, yeah, you can't triangulate that depth, which sounds like that's the same problem there with Nerfs. I kind of pulled up the Waymo video, the video that, by the way, I noticed that you narrate a lot of these videos. Just to add a little humor here, do you guys draw straws? Who does that? Or is that just, who decides who narrates these papers? That's just something I've always noticed. A variety of things. I mean, a lot of these are like who's still awake at whatever time of night before it needs to be submitted to a conference. But, all right. You do really well with that. I just thought it was funny. I was like, maybe they always make Matt go up because he's done a few of these now. Go ahead, Jared. I was gonna say, just to take a step back. I mean, so Jonathan's showing here these amazing visuals as just urban environments, downtown San Francisco, whatever being reconstructed. Matt, can you just take a step back and just kind of explain at a high level, what is Block Nerf or what was the goal of Block Nerf? Yeah, so Block Nerf was a project that was done with Waymo, which is Google's self-driving car effort. And their main goal is, I mean, essentially to make like autonomous cars to drive across all these streets. They can obviously test the cars on the streets, but they can only go so far, right? So if you wanna start simulating new scenarios or whatever it might be, it's useful to have a 3D representation of the world that you can like interact with. And so traditionally you would potentially hire someone to try to create 3D assets of all these elements. But in this particular case, you've got these cars with all these cameras on them. Can you just automate the process? That was the goal. And so you've got photogrammetry methods that could potentially work on it. I had experience in Nerf stuff, so it just seemed like a natural idea. Can we apply Nerf to the data that the Waymo vehicles have already captured to just reconstruct them? But this leads to a few challenges that we don't normally have, or at least the original Nerf papers didn't really deal with, and it's really one of scale. So while there's a lot of data that is of different regions of space, you have to take into account that they were taken at like different times and that different objects were around. So a lot of that paper was just a matter of pulling together various other approaches that have been proposed in other papers that account for these different aspects. So things like changes in weather, things like there's a vehicle driving by, how do you handle all that? Okay, that makes sense. That makes sense. And what you said earlier, talking about how Nerf's motivations being around generating that nice looking image, that realistic looking image, versus just perhaps accuracy or just raw brute force accuracy of the geometry, that makes sense if you're trying to simulate the world and simulate the world through visual means, having those really nice looking accurate images. Yeah, that seems like a great application of Nerf. Yeah, because I mean, these self-driving car companies, or most of them like Waymo, they have LIDARs on them. So they have some estimate of the 3D geometry of space. And so simulating LIDAR, it's not easy, but it's a little bit more straightforward. And that's something where you can actually use more crude 3D models that already exist, that you can just like create a LIDAR sensor and just have it travel through that space. Visual is harder, right? What actually is fed into the camera, you want it to be something that looks photorealistic. Now, I think the video that you were just showing, like is a step there. It's not photorealistic yet, right? There's issues. And a lot of those issues come down to the fact that this data was captured over long periods of time. And so you lose some of the constraints that you normally have, or assumptions that you normally have that like the scene is static. So when the leaves on the trees change, we don't really have a great way to account for that. So as a result, it sort of blurs it all out and kind of predicts the average of all the different leaf positions. Or when you look at this data over a long enough period of time, you actually see like the tree grows, right? So. What height do you choose? Depending on what view you're looking at, the tree is either short or tall or. No, no, like literally it grows over a month or two period of time, right? Okay. Or the leaves fall off the trees. That's cool. We did have a past episode about, computer vision in the wild, I think was the title. And it was those challenges. So Jared has a lot of experience as well with just crowdsourcing hundreds of thousands of images. And it's just, no matter what, you're not gonna get perfection, but it's amazing what you can pull off to get a good understanding of a scene, even with all that mismatch of information between images. I mean, Jared was even using a mix of cameras that we didn't even know what the camera, they were scraped from photo bucket, I think, or one of the big crowdsource image sets where you didn't know what the camera was. You just had images. So it's impressive what he pulled off, especially also that block nerf was done. That wasn't done like last week. That was a while ago. So not that you can solve those problems necessarily today, but I've already seen the quality of nerfs just improve as this has evolved. And I think people forget. They say, oh, well, this is not as good as photogrammetry. Photogrammetry has been around since decades versus a couple of years. So there's a lot to figure out, a lot to optimize. So I just thought it was a really good example of how, it's kind of like you said, you pulled in a lot of ideas to pull off this large nerf. One thing I noticed that people kind of struggle with with nerfs is it takes an enormous amount of GPU power. You can't just run it on a entry-level laptop or desktop. You need that high-end GPU. Can you talk about that? Do you see that burden goes away in the future? Is that just the reality of the technology? And if so, also then how do you expect people to be interacting with nerfs? So are we gonna be doing it on cloud-based environments where you don't have to, it's just close, hosted out there in the cloud. How do you see that moving? Right, they are compute-intensive. And maybe I'll briefly describe why they're compute-intensive. I described that you represent the world as sort of lots of little cubes, or like little chunks of space. But since it's all being represented in sort of this representation that often is like a neural network, you don't actually know where it is until you ask whether there's something in that location in space. So the way you actually render or predict the color of a single pixel is you sort of like shoot a camera ray out into space. And then along that ray, you take lots of little samples. And each of those samples you ask, is there anything here? And if there is, what color is it? That asking process is actually feeding that sample through a neural network. And popping out an answer. And so if you're doing 500 samples on a ray, 500 queries of a neural network, and you're trying to render a one megapixel image, that's, you're rendering like evaluating this neural network billion sometimes. So very quickly it becomes quite expensive. And for that reason, the original NERF papers, they would take almost a day to, or in some cases more than a day, just to reconstruct a single scene. Quite expensive. And for a while that was like people were sort of fighting, getting it maybe like 20% faster or 30% faster. But over time, people found more and more tricks in order to make it. So now you've got something like instant NGP or these other fast methods that can train on the order of five minutes, right? So we've already gone multiple orders of magnitude. There's different ways and different tricks that people use. But one of the biggest tricks is, instead of doing 500 samples along the ray, if you keep some information around saying that like, there's nothing here, don't sample this region, you can get rid of most of that computation. And so as we go forward, we're coming up with more and more tricks like that so that we don't have to do quite as much computation when we actually render the scene. So do I think it's gonna get faster? I think it will. I mean, it already has gotten significantly faster. That being said, it's still obviously not something you can run on your laptop. But I do think it'll go farther. I think the other thing that's worth mentioning is computer graphics, 3D graphics, we kind of take for granted now. But it took time for them to actually be feasible to run. And one of the main reasons they're feasible now is because we have something like a GPU, which is literally designed to render out a bunch of these mesh triangles as fast as possible. So we're sort of seeing people get more on board with these nerf-based methods. I think it's totally feasible that these manufacturers will also start looking into like, how do you actually optimize on a hardware level rendering these things out faster? So that base approach, I think, will continue to get faster. It might be a little bit slower, especially given that like hardware modifications may be necessary. But there's another approach that you can kind of look at, which is, can we convert these nerfs into something else? So if we forget about the actual creation of the nerf for a second, let's just say the creation of the nerf stays a bit expensive. And that's something that you run in the cloud or somewhere else. Once it's created, people are coming up with more and more methods to convert it into a representation that still maintains a lot of the visual quality, but is using the technologies that we've already optimized, essentially converting it to a mesh or something similar to a mesh that we can run on something like a phone or normal devices without GPUs. And we already see that happening both in research and also in industry. Luma, for example, is a company that will do some sort of conversion like that, which is why you can visualize it in your browser. That makes sense. I also saw Mobile Nerf, which turned it into a triangle mesh as well, running high FPS, even on my iPhone in browser. So just as an interesting aside, is Mobile Nerf, it converts it to a triangle mesh, but it's not the triangle mesh that you're used to. What it is is it's tons and tons of triangles all overlapping each other. We like to use the term triangle soup, where it's just sort of like you splatter triangles into the scene. And so it's not the way that people typically think of triangle meshes, where a lot of people care about it being watertight. So it's some volume that doesn't have any holes. It's like nice and perfect. That's not what these are, but they can leverage the advances in GPU technology to make it really, really fast while also producing great looking renders. Yeah, that's what I noticed. It looks great. Don't leave the scene. It's not ready for a game. And also a lot of triangles. It's not optimized to be the best looking with the least amount of triangles and meshes and data there. But it works. That's interesting for visualization. And I think a lot of people forget that Nerfs, there's a lot of people say, well, what's useful with the Nerf if I don't get a mesh? There's no use for this technology without meshes. And that is definitely a viewpoint from people who just need meshes. They work in game engines, they work in software that they're interacting with a mesh. And that's fine. But I think that people just overlook a whole entire, think of Waymo, they're doing self-driving cars, right? So they actually don't care about a geometry a lot of times in a scene. It's the cars learning what the scene looks like and how to interact with a scene based off appearance because using camera guidance. They're not, and you can simulate LIDAR data for some geometry. But I think that that is a application where we don't have to worry about meshes and it could be expensive to render, inexpensive to train, but we're using it for these machine learning models that in the end, it's worth all that investment, all that time, all the energy, all the GPU cycles, because it actually is faster than someone perhaps modeling these objects in 3D in all these third-party softwares just to get one little scene for a car to learn how to drive around it. So I think to throw that out there. Yeah, I think I would also have sort of like two other responses to someone who says they just want meshes because they want geometry. I think that's totally valid because that's what our tooling is built to use right now. But people, well, so the first thing is people are creating methods to convert nerves to meshes. So it's not like using a nerve means that you don't get something like a mesh as an output, right? Not quite as good, but we're getting better and there's a lot of research in that space. So I wouldn't be surprised if you can get pretty good meshes out of a NERF-based model soon. And the second thing is people want meshes because that's what their tools currently use, right? If you got the Unity game engine or whatever game engine, you import a mesh. I don't think that's always gonna be the case. You've already seen people hack together ways to import NERF-style models into these existing game engines. And I think it's only a matter of time before these game engines implement it at more of a core level. So once that is the case, then do you actually need the mesh or you just need the mesh because that's what your current tooling uses? Yeah, I agree there. It's a necessity right now, but is it a mandatory requirement? Maybe not. And just, I've been spending a lot of time interacting with NVIDIA, who obviously are spending a lot of time and energy and research cycles on this technology. And they haven't said anything publicly, but you can tell that they're working on something with the Omniverse off of what they've said. And I can only imagine that they're gonna have some sort of server-based rendering of NERFs and not convert it to a mesh necessarily because the whole Omniverse, they have a lot of synthetic data rendering and things like that they use for machine learning. And I can see how this just easily integrates in that whole ecosystem where you don't need the mesh in that tool set. But like you said, some other tool sets, it's the reality of today. I think there's also like a just general trend of a lot of tool sets migrating to the cloud. And once you migrate to the cloud, then they can do whatever they want in the backend, right? Like they can use all the compute they want. And yeah, so. Yeah. All right, well, I just wanna pivot. I wanna make sure we have enough time here because we're coming to the end of our time with you. But I wanna make sure we talk about a project you're currently involved in called Nerf Studio. And I'm gonna just, if people are on the YouTube version of this, I will again have some visuals of just outputs from Nerf Studio. But can you just give us a quick overview of what Nerf Studio is to start out with? While I show some cool, interesting results. Yeah, so it's an open source project that essentially tries to bring together a bunch of different Nerf methods. So Nerf came out a little over two years ago now. And since then, there's been a lot of followups. The most notable ones are the ones that make it like real time and fast, but there's also been a lot of things to improve the quality in a variety of different scenarios. And the challenge is if you go out there and look and you just wanna use Nerf, say you're a researcher or a programmer, it's not entirely clear where you start. You could grab, the thing that I would recommend number one is don't grab the original Nerf code base. There's like zero reason you should use that anymore. Yet a lot of people still use it and we still get like GitHub issues and people trying to mess around with it, but it's out of date. These things have improved since then. And I've personally experienced this. We talked about Block Nerf earlier. Block Nerf, I just combined a bunch of different methods together, right? And there was a few methods that I hadn't had the chance to combine yet. And so when I went back to Berkeley to work on my next project, I wanted to do essentially Block Nerf, but on a academic level compute rather than like Google level compute. And I think it's possible now that we have things like Instant NGP, which allows things to be quite quick. And so I started hacking around. I wanted to reconstruct the campus here at Berkeley. And very quickly I realized I was repeating all the same things I was doing, like combining these different methods that I had to do when I was at Waymo. And it seemed kind of like a waste of time. And just thinking there's gotta be other people that are also trying to combine all these different methods together. What if we just do it the correct way or a more correct way so that when people wanna use these methods in the future, they can just piece them together more like Lego blocks and get good results. At the same time, can we make the UI and usability easier so that folks that are maybe less into the research side of things can use this as a tool that combines the best of the best that we have in Nerfs at the moment. And so I, with a few other grad students, I'll highlight Ethan Weber and Yvonne, we worked together to create this and we released it a few months ago. And yeah, it's been exciting to see other people use it. We released one model with it, the Nerf Acto model. And so this is just an example where we combined a few different methods. So you asked, I don't know if you asked how it compares to Instant NGP, but if you compare it to something like Instant NGP, the new model is essentially Instant NGP, but with other things added on top of it. So it's not like it's trying to compete with it, it's using those ideas and concepts and just bringing in other ideas and concepts that have happened in the space since. So for example, now there's a method that lets you render stuff in the background a lot clearer. Why not include that with Instant NGP, right? That was my first comparison, I took the same image set. I was like, I always wanna see the difference between Nerf Acto and Instant NGP. And it was just like, oh my goodness, it's like the background mountains just snapped into clarity, which excited me because I always felt like the back, you get things get further and further away, they almost look like they've been water colored into the background as opposed to just even out of focus. They just like smeared, that's not the case. Yeah, so the goal going forward though is as new methods and new techniques come out and people sort of agree that this is a good direction to go, we want to incorporate them and make an easy place for people to contribute their own modules, but also sort of leave a history of what sort of the different approaches one can take. And I think what's a little bit different than maybe treating this as a traditional research project is that we're kind of doing this all in the open. The goal isn't to write some paper and I think there's great reasons to write papers about different methods in this area, but I think there's equally good reasons to try to do these things more publicly and not just try to figure out ways to differentiate yourself from others because you gotta accept that there's a lot of good ideas in this area, if we can find them all, we can get something better. And so as a result, we've seen people outside of our core team starting to contribute to the repo, which is the thing that we, that was our ultimate goal, because the hope is to make this something that's sustainable that people across industry and academia can help contribute to to improve it over time. All right. That's such a valuable effort. I mean, I know just in my work during my PhD, like, okay, every grad student, they've got their own project that they're working on and, oh, I'm gonna go try to use someone else's code, sure, I might find it somewhere and then I try to download it, compile it, oh, it doesn't work, you know, and so just that effort of just reproducing everyone else's work to try to build and benefit your own. I really appreciate these problems. Like for me, in the photogrammetry space, dealing with a call map or open MVG or visual SFM, so some of these projects where it's like, oh, I don't need to reinvent an entire photogrammetry pipeline, I can take one that's already out there and then build upon it and extend it, or just use it just to see, like, hey, what sort of results do I get out of it? So this project like Nerf Studio, where you're saying you're able to take all these sort of state-of-the-art methods, make it modular so that both researchers as well as engineers can give it a try, extend it if they want. That's such a valuable effort. Yeah, I find this interesting because it's a project that excites Jared, you know, an actual engineer, someone who can jump in the code base, and then someone like me who just checks on it every now and then, stays active in the Discord, and one day you're just saying, hey, pip install or pip upgrade your code base, and all of a sudden I got these new features, these new methods I can play with, and I don't have to be committing to the GitHub repo, I don't even have to build the source code from scratch necessarily, like I did with Instant NGP, you guys made it easy, and a very tangible way to use it. So it kind of crosses the, I would say, armchair, quarterback just wanna go in there and play with it and see what it can do, and then, yeah, Jared, who's like, no, maybe we wanna take some serious looks at modifying this and adding to this stack of technology. So how would someone find this? I mean, I can bring it up on my screen, but is this? Yes, I mean, you should be able to just Google Nerf Studio, and hopefully the SEO is really high enough. The website is nerf.studio. Easy, I'll make sure I link it in all the show notes to all these, the podcasts and YouTube video, but you suggest they go here, which I have my screen, if you're on the YouTube, you can see, I kinda have the webpage pulled up. You start here and not at the GitHub level. Yeah, I think starting, well, I mean, you can quickly get to the GitHub from here, but hopefully you don't have to go to the GitHub if you don't want to. Like, we wanna be accessible for people who don't need to do anything coding related. On the left side, we've got something for installation. If you walk through the steps, hopefully it could be up and running. Takes a little bit of time to install, but once it's installed. Not that long, though. So I hope to make a guide on getting this installed for the people who've never even touched command line. Just take it down to the very simplest core there. It's also worth noting that we do wanna make this more accessible to people who don't have GPUs. So in the top left, there's a button that says CoLab. Clicking that will actually launch a Google-hosted program that gives you essentially a free compute. Now, it's not gonna be top-tier compute, so it's a little bit slow going through it, but it will allow you to replicate these results without shelling hundreds of dollars for your own fancy NVIDIA GPU. If you can even get ahold of one. Yeah, if you can even get one. There was, at least on the 4090s, I think there's a thread of people just looking for when they get on sale. Well, anyways, I've taken enough of your time today. Just wanna wrap this up. So again, it's Nerf Studio. I know that there's also a wonderful Discord channel where people are engaged in asking questions and answering them. And I mean, that's one way to get ahold of you, Matt. You're in there very actively, I notice, responding to people, explaining things. And we're all grateful for that. It's rare that there's a research or some sort of, this level of novel technology that you can just go and ask questions on a channel and get the actual researchers to respond. If it's a big company, a big corporation, a lot of times you're not dealing with the people closest to the software. It's some sort of representative who's asking you a question then going and trying to answer it. So that's quite exciting. How else can we follow your work, Matt? Are you active on social or your website? That's a better place to follow? Yeah, so I mean, my personal website's just mylastname.com. But for all the Nerf Studio stuff, we've got at nerfstudioteam on Twitter where we post updates and renders. I think that's probably the best bet. Great, I'll make sure those are all linked in the show notes as well so people can find different places to see what you're up to and follow this journey at Nerf Studio. And as always, again, just thank you, Matt. And if you are listening or watching this before, sorry, November 17th, Jared and I are actually gonna host a live computer vision decoded event in Durham, North Carolina where Jared, his headquarters is. And we're actually gonna show off just Nerfs and talk about it a little bit. And Matt, you don't know it, but we're gonna use Nerf Studio because I think it is, it'll be great for that community to say, hey, here's Nerfs. If you don't know about it or you're interested in it, go look at this. We're also gonna use the record 3D workflow you have so we can just capture someone and within a few minutes, maybe the time it takes to transfer the data and initiate a line of code, we can have a Nerf training, which to me is really exciting too, because it's like one thing say, okay, we took pictures and in 30 minutes after we get all the pose estimations, we get to start training. Well, now we can skip that. So I'm really excited. We'll make sure we share how that goes as well with you, Matt. So thanks, Matt. Thanks, Jared, for being on this episode. And I look forward to seeing you guys all in the next one. Yeah, thanks for having me on. Thank you.",
        "output": "### Introduction to Computer Vision and NERFs\n- Background of the speakers and their journey into computer vision research\n- Definition of NERFs (Neural Radiance Fields) and how it differs from traditional methods like photogrammetry\n\n### NERFs: Volumetric Representation\n- NERF uses a volumetric representation instead of a mesh\n- It can model very small areas in space, like a dense point cloud\n- Uses a neural network to store the information about each point in space\n\n### Differentiation from Traditional Methods\n- Focused on creating good-looking images instead of accurate geometry\n- Can represent a scene well under specific conditions and with certain assumptions\n- Comparison with photogrammetry and point clouds\n\n### Block NERF and Waymo\n- Block NERF is a project in collaboration with Waymo\n- Aimed to reconstruct urban environments accurately\n- Used data from multiple cameras on self-driving cars\n\n### Pros and Cons of Different Approaches\n- Different methods have different limitations and advantages\n- Some methods may work better on certain scenes or with specific input data\n- Balance between experimentation, prior knowledge, and manual adjustments to get optimal results\n\n### Future of NERFs and Photogrammetry\n- Merging ideas from photogrammetry and NERFs for better geometry reconstruction\n- Exploration of learned-based approaches for depth and geometry reconstruction\n- Recognizing the strengths of different methods for specific applications\n- Continuation of multiple methods for different purposes depending on requirements"
    },
    {
        "instruction": "<|SYSTEM|>You are a scholars that is taking markdown notes about some video, you would be giving some written scripts for the task.\nYou have finished writing the notes, and you want to extract the title from the notes.\nThe title should only be the name of the technology or algorithm, no extra words needed.\nPlease write the notes in markdown format. No need to write \"Title:\" for the Title. Make sure the title is within 10 words.\nHere is an example:\n## Hidden Markov Model\n<|ASSISTANT|>## ",
        "input": "<|USER|>### Introduction to Computer Vision and NERFs\n- Background of the speakers and their journey into computer vision research\n- Definition of NERFs (Neural Radiance Fields) and how it differs from traditional methods like photogrammetry\n\n### NERFs: Volumetric Representation\n- NERF uses a volumetric representation instead of a mesh\n- It can model very small areas in space, like a dense point cloud\n- Uses a neural network to store the information about each point in space\n\n### Differentiation from Traditional Methods\n- Focused on creating good-looking images instead of accurate geometry\n- Can represent a scene well under specific conditions and with certain assumptions\n- Comparison with photogrammetry and point clouds\n\n### Block NERF and Waymo\n- Block NERF is a project in collaboration with Waymo\n- Aimed to reconstruct urban environments accurately\n- Used data from multiple cameras on self-driving cars\n\n### Pros and Cons of Different Approaches\n- Different methods have different limitations and advantages\n- Some methods may work better on certain scenes or with specific input data\n- Balance between experimentation, prior knowledge, and manual adjustments to get optimal results\n\n### Future of NERFs and Photogrammetry\n- Merging ideas from photogrammetry and NERFs for better geometry reconstruction\n- Exploration of learned-based approaches for depth and geometry reconstruction\n- Recognizing the strengths of different methods for specific applications\n- Continuation of multiple methods for different purposes depending on requirements",
        "output": "## NERFs and Computer Vision"
    },
    {
        "instruction": "<|SYSTEM|>You are a scholars that is taking markdown notes about some video with Wiki style, you would be giving some written scripts for the task.\nAnd summarize each paragraph with a title.\nRemember to keep the technology or algorithm name, very important.\nPlease write the notes in markdown format.\nHere is an example:\n### Definiation\nA deadlock is a situation in which two computer programs sharing the same resource are effectively preventing each other from accessing the resource, resulting in both programs ceasing to function. \nThe earliest computer operating systems ran only one program at a time.\n\n### What it have to do with Relationsal Database ManagementSystem\nIn a database, a deadlock is an unwanted situation in which two or more transactions are waiting indefinitely for one another to give up locks. \nDeadlock is said to be one of the most feared complications in DBMS as it brings the whole system to a Halt. \nWhen you tries to insert or update data on database, sometime your record need to depend on the row record.  \n<|ASSISTANT|>#### ",
        "input": "<|USER|> Alright, check this out. I'm currently logged in as a normal user, I don't have full access to this machine, but if I run this program, bam, all of a sudden I'm a root user. I've got full access to this machine. Pretty cool, right? This video is going to be a story and a technical explanation behind this vulnerability that lets attackers gain high-privileged access. This video is sponsored by Snyk. Snyk scans your projects for known vulnerabilities in code, dependencies, containers, and even configurations. Go try it out at snyk.co slash potent function. About 12 years ago, this piece of code was pushed to a particular Linux package, and little did we know, this was going to be a major bug in many Linux operating systems. And about two months ago, the storms of log4j vulnerability was clearing out, and just when people were starting to settle, the security team from Qoalis dropped this research. It's a local privilege escalation, meaning that if you're a regular low-privileged user, you can become root or an administrator without any credentials. Pretty cool, right? This is kind of a big deal, because anywhere from penetration testers to malwares could use this in good and bad ways. The word authorization in Linux systems would actually lose its meaning. The research also stated that most Linux distributions, like Ubuntu, Debian, Fedora, CentOS, all of them were pretty much affected, all because they shipped one vulnerable package pre-installed. So this code is from the package pullkit, or policykit. It comes pre-installed in many Linux distributions, meaning that a lot of machines were vulnerable at that point in time. So the vulnerable code that you saw was found in an executable named pkxx, which is part of the policykit suite of tools. But what exactly is pkxx, and what does it really do? pkxx is an executable that allows one user to execute commands as another user, kind of like sudo. Now you might have used sudo before, which gives you root access to non-root programs, right? Similarly, pkxx can also provide these privileges to unprofileged programs. Now the question is, what went wrong? To understand that, we need to look into some source code. Before we get into the real meat, you should know some things about arguments, so let's check them out real quick. So let's say that you run a program like cat, and you provide test.txt as an argument. So the entire argument array is going to look something like this. Here the cat is the first argument, the index 0. The first arguments are usually going to be the name of the program itself, in our case it's cat. And the second argument is the filename that we just provided, test.txt, with the index 1. And these are all strings by the way. But in memory, the arguments of a process look something like this. So you've got a bunch of arguments stacked at one place, and then the environment variables are right next to it. So first comes the list of arguments, and then comes the list of environment variables. So far, so good. Now think about a question, what really separates the arguments from the environment variables? Like what's really the boundary here? This is an important question, because the memory is contiguous, so there's gotta be a definitive boundary, or else everything's just gonna go south. So there is a boundary here, and that's null. If the last element of the arguments is null, that means that's where the arguments end, and the environment variables begin. Remember that. So consider our previous cat example. So here, the third element of the array is gonna be null, and that's where the arguments end and the environment variables begin. Great. Now that you're familiar with how the arguments are structured in memory, we can understand the vulnerability. Let's continue. So let's just dive right into pkexec.c. Here's a loop that starts with the variable n set to 1, and it checks for the arguments provided to the pkexec program at runtime, such as, you know, dash dash help or whatever. A few lines down, we'll see this. This is basically reading the nth argument and setting it to the variable path. And a few more lines down the road, we see this. Here it fetches the program's path and it sets it back to the arguments array. Hmm, interesting. But have you spotted the problem yet? Well, here it is. If you have not provided any arguments, but we set the zeroth element in the arguments array to null, which is usually supposed to be the name of the program, but now it's null. What do you think would actually happen here? Well, let's walk the code. Initially n becomes 1 from this loop. Since it doesn't really match any case, it would break out of this loop with the value n set to 1. You gotta remember that. Down here, it's trying to read the arguments of n, which is basically 1. And we already know that it doesn't exist because we just set the arguments 0 to null. That means the arguments should end there, but regardless, this line of code is still trying to read something that's out of bound. So when it's doing this, out of bounds read, what do you think it's actually trying to read? Exactly. It's the environment variables. Remember earlier we discussed that how the memory is laid out for arguments and the environment variables? They were basically stacked right next to each other. So here, when it's trying to read the second argument, it's in fact trying to read the first environment variable. This value is stored as the path variable in ergo and out of bounds read. Also, if the path variable doesn't start with a forward slash, it writes the path back to argv of n, and that's an out of bound write. So now we have two weird behaviors that we can somehow take advantage of. These weird behaviors are also called as primitives, so with these primitives, we can inject any environment variable into a process. You might be like, hold up, all this for just to set the environment variable of a process? I mean, we could literally do that by hand, right? What's the big deal? Yes, we can add an environment variable when we start the process, but there is a caveat. There are some interesting environment variables to be considered, and those are called unsecure environment variables, so something like LD preload. So that's an environment variable which is automatically filtered by LDSO on any set UID bit programs or programs that run as another user. If they aren't filtered, it's an easy privilege escalation, but Linux is aware of these attacks, and hence it restricts some environment variables to be passed down from parent to a child process. So we can't really inject all kinds of environment variables, but since we have this new primitive of out-of-bounds write, we should be able to use it to inject insecure environment variables like LD preload and gain root privileges, right? Pretty much, yes, but there's one small problem. Right after this out-of-bounds write, it calls clearenv, which means it clears out every environment variable. So we need to find a way to execute the code from the point where it does the out-of-bounds write, but before it clears out all the environment variables. So that's the execution frame that we've got, and we've got to find something interesting within that frame. So let's take a look at the code again. Right after the out-of-bounds write, there's a call to this interesting function called validate environment variables. So inside this function, there's a conditional call to gprint error. This function is kind of important because it's like our main gadget. Basically this function helps us in getting privilege escalation. Here's how. So this gprint error function normally prints UTF-8 error messages, but if the charset is not UTF-8, it would actually call another function called iconvopen from glibc. So this will actually try to convert the error message into the write charset using a conversion module. So here's where the gold is. We have full control over this conversion module. So this is basically a shared library. The idea is to trigger an error and set the charset to something that's other than UTF-8, which will trigger this gprint error. So this gprint error function will recognize that the charset is different and it would try to convert them. And to do that, it requires a conversion module, which we can specify using an environment variable called gconv path. So an internal function called giconvopen is responsible for the conversion. And this will basically look into the environment variable and get the malicious conversion module from there. Once it has our conversion module, it would try to convert the charset, but in reality, it's actually executing our malicious code. And more importantly, it's executing it as root. Just so you know, just like LD preload, gconv path is also considered unsecure, but the out-of-bounds write primitive will actually help us inject the environment variable back in. Alright, let's craft the full exploit. As you already know, the main goal is to escalate the privileges from a regular user to a root user. We have two primitives, an out-of-bound read and an out-of-bound write. With this, we can read and write environment variables, including the insecure environment variables. And we can't use something like LD preload because all the environment variables are cleared out. But we found a special gconv path environment variable that will be triggered before all the environment variables are cleared out. So a perfect candidate. So the idea is to inject the insecure environment variable gconv path, which lets us configure a conversion module, which is basically our malicious shared library. So firstly, we will create a directory named gconv path equals dot and place an empty executable named pwn inside of it. We're doing this because we want gconv path equals to dot forward slash pwn to be written as the first environment variable using our out-of-bounds write primitive. And we need to make sure that this has to be a valid path returned from the gfind program in path function. So hence, we name the folder as gconv path equals to dot and then the pwn as a valid executable inside that folder. Then we run the pk exec providing our environment variables. We set the first environment variable to pwn. The out-of-bounds read will actually read the first argument, which is going to be env pf0. So pk exec tries to find the file named pwn. Then we supply term equals two dots. This will actually trigger the gprint error because it wants to print to the screen that this is a suspicious input. So when it does that, it would actually trigger the whole execution tree. So it would print the error message using the gprint error. Hence, we provide path equal to the malicious directory that we just created. When pk exec tries to find the executable pwn, it will try to actually search in a path. And it's doing that using the function find program by path. So we're sending the path environment variable to our directory such that it would actually find that executable inside there. And it would return a valid value, which looks something like this. That's exactly what we want to be written to the environment variable. Right down here, it tries to write this thing back to arguments of one. But as we already know, arguments of one is the environment variables of zero. Basically, we've injected that string into the first element in the environment variables array. Finally, we set the charset to br to trigger the code path that thinks that this is not UTF, and it would try to do the conversion. When it tries to do the conversion, it tries to look inside the directory named pwn, because that's where it gets the conversion module from. Cool. So we create a directory named pwn, and inside that we create a single file named gconv-modules. This file is like a mapping. So this file basically says, if the charset is not UTF-8, then use this module to convert it. So this conversion module will be our malicious code. A conversion module usually requires a few methods to work, like gconv-init and gconv. So we'll add our own code inside of the init method. So basically, we do a simple shell execution. We also set the UID and GID to zero. This is because usually UID and GID are dropped when we switch the users or run as a different user. So we want to set it back to zero because that's the UID and GID of root. So finally, we execute the bin sh, and all there's left is to compile the shared module and also compile the exploit itself. And pretty much run it. So to do that, I would use something like makefile. So let's try to run this. And sweet, we are root. There we go. By the way, the full code is available on the GitHub repository if you want to try checking it out. I've also left some comments explaining the code pad, so check it out if you have any sorts of confusions or whatever. Now the real question is, why did nobody see this vulnerability? It's been like 12 years, and why was it not found before? The answer is it was found. It was found in like 2013 by this person named Ryren. He blogged about this out of bounds, read and write with the arguments in his blog post named argv-silenus. He just couldn't exploit this behavior back then, but he did figure out the right root cause. And then in 2019, there was this blog post on a CTF challenge where you had control over the second argument of a fopen function, and you had to get code execution from it. And this involved playing around with gconv conversion modules. So theoretically, if you had put those two pieces together, you might have had yourself a zero-to-exploit man. I don't know. But thanks to Qualys, now we know about this vulnerability, and there are a lot of fixes. So please update PwnKit on your machines. It should automatically do it, like on Ubuntu or whatever. But if you cannot update it, just straight up remove the pkexec executable. And if you're deploying applications using containers, and if it's like vulnerable to the PwnKit, use something like sneak-cli to detect it and fix the issues. So for example, I've got this Docker image. You can try the attack on this Docker image if you want. Also the Docker file is available on the GitHub repository. You can attack safely inside a container. But anyways, if I run sneak-cli against this image, it would immediately show me the vulnerability that exists inside this container image. So something like this in your development or ops pipeline would be very useful, I think. Anyways, hope you all learned something new. Hope you enjoyed it. Shout out to sneak again for sponsoring this video. Thanks for watching, and I'll see you in the next one. Peace! you",
        "output": "### Program that Gives Root Access\n\nA vulnerability in certain Linux operating systems allows attackers to gain high-privileged access. This local privilege escalation essentially renders authorization meaningless in Linux systems. Most major Linux distributions, such as Ubuntu, Debian, Fedora, and CentOS, have been affected.\n\n### PolicyKit and the Vulnerable Code\n\nThe vulnerable code is found in an executable named pkxx, which is part of the PolicyKit suite of tools. PolicyKit comes pre-installed in many Linux distributions. Pkxx is an executable that allows users to execute commands as other users, similar to sudo. The vulnerability lies in the fact that the code reads and writes environment variables, including insecure ones.\n\n### Out-of-bound Read/Write and Environment Variables\n\nTwo major issues are: out-of-bound read and out-of-bound write. With these primitives, attackers can inject environment variables. Some insecure environment variables, such as LD_PRELOAD, are automatically filtered by LDSO on set UID bit programs or programs that run as another user. However, an out-of-bound write primitive could be used to inject the insecure GCONV_PATH environment variable. The idea is to trigger an error by setting CHARSET to something other than UTF-8. This would then trigger g_print_error, which would convert the CHARSET using a conversion module specified by the attacker via the GCONV_PATH environment variable.\n\n### Crafting the Exploit\n\nThe exploit involves creating a malicious shared library as a conversion module using the GCONV_PATH environment variable. This would then be used to execute shell commands as a root user. The full exploit code is available on the GitHub repository for interested users. \n\n### Vulnerability Discovered Previously\n\nThe vulnerability was discovered before, in 2013 by Ryren, but was not exploited at that time. A CTF challenge in 2019 also revolved around a similar exploit. Qualys finally linked these concepts and enabled this 0-day exploit. Updating the PolicyKit or removing the PK_Exec executable is advised, as well as using tools like Sneak CLI to detect and fix this vulnerability in container images.\n"
    },
    {
        "instruction": "<|SYSTEM|>You are a scholars that is taking markdown notes about some video, you would be giving some written scripts for the task.\nYou have finished writing the notes, and you want to extract the title from the notes.\nThe title should only be the name of the technology or algorithm, no extra words needed.\nPlease write the notes in markdown format. No need to write \"Title:\" for the Title. Make sure the title is within 10 words.\nHere is an example:\n## Hidden Markov Model\n<|ASSISTANT|>## ",
        "input": "<|USER|>### Program that Gives Root Access\n\nA vulnerability in certain Linux operating systems allows attackers to gain high-privileged access. This local privilege escalation essentially renders authorization meaningless in Linux systems. Most major Linux distributions, such as Ubuntu, Debian, Fedora, and CentOS, have been affected.\n\n### PolicyKit and the Vulnerable Code\n\nThe vulnerable code is found in an executable named pkxx, which is part of the PolicyKit suite of tools. PolicyKit comes pre-installed in many Linux distributions. Pkxx is an executable that allows users to execute commands as other users, similar to sudo. The vulnerability lies in the fact that the code reads and writes environment variables, including insecure ones.\n\n### Out-of-bound Read/Write and Environment Variables\n\nTwo major issues are: out-of-bound read and out-of-bound write. With these primitives, attackers can inject environment variables. Some insecure environment variables, such as LD_PRELOAD, are automatically filtered by LDSO on set UID bit programs or programs that run as another user. However, an out-of-bound write primitive could be used to inject the insecure GCONV_PATH environment variable. The idea is to trigger an error by setting CHARSET to something other than UTF-8. This would then trigger g_print_error, which would convert the CHARSET using a conversion module specified by the attacker via the GCONV_PATH environment variable.\n\n### Crafting the Exploit\n\nThe exploit involves creating a malicious shared library as a conversion module using the GCONV_PATH environment variable. This would then be used to execute shell commands as a root user. The full exploit code is available on the GitHub repository for interested users. \n\n### Vulnerability Discovered Previously\n\nThe vulnerability was discovered before, in 2013 by Ryren, but was not exploited at that time. A CTF challenge in 2019 also revolved around a similar exploit. Qualys finally linked these concepts and enabled this 0-day exploit. Updating the PolicyKit or removing the PK_Exec executable is advised, as well as using tools like Sneak CLI to detect and fix this vulnerability in container images.\n",
        "output": "## Linux Privilege Escalation Vulnerability"
    },
    {
        "instruction": "<|SYSTEM|>You are a scholars that is taking markdown notes about some video with Wiki style, you would be giving some written scripts for the task.\nAnd summarize each paragraph with a title.\nRemember to keep the technology or algorithm name, very important.\nPlease write the notes in markdown format.\nHere is an example:\n### Definiation\nA deadlock is a situation in which two computer programs sharing the same resource are effectively preventing each other from accessing the resource, resulting in both programs ceasing to function. \nThe earliest computer operating systems ran only one program at a time.\n\n### What it have to do with Relationsal Database ManagementSystem\nIn a database, a deadlock is an unwanted situation in which two or more transactions are waiting indefinitely for one another to give up locks. \nDeadlock is said to be one of the most feared complications in DBMS as it brings the whole system to a Halt. \nWhen you tries to insert or update data on database, sometime your record need to depend on the row record.  \n<|ASSISTANT|>#### ",
        "input": "<|USER|> Transformers are taking the natural language processing world by storm. These incredible models are breaking multiple NLP records and pushing the state of the art. They are used in many applications like machine language translation, conversational chatbots, and even to power better search engines. Transformers are the rage in deep learning nowadays, but how do they work? Why have they outperformed the previous king of sequence problems like recurtainal networks, GRUs, and LSTMs? You've probably heard of the different famous transformer models like BERT, GPT, and GPT-2. In this video, we'll focus on the one paper that started it all, Attention is All You Need. To understand Transformers, we first must understand the attention mechanism. To get an intuitive understanding of the attention mechanism, let's start with a fun text generation model that's capable of writing its own sci-fi novel. We'll need to prime the model with an arbitrary input, and the model will generate the rest. Okay, let's make this story interesting. As aliens entered our planet and began to colonize Earth, a certain group of extraterrestrials began to manipulate our society through the influence of a certain number of the elite of the country to keep an iron grip over the populace. By the way, I didn't just make this up. This was actually generated by OpenAI's GPT-2 transformer model. Shout out to HuggingFace for an awesome interface to play with. I'll provide a link in the description. Okay, so the model is a little dark, but what's interesting is how it works. As the model generates text word by word, it has the ability to reference or attend to words that is relevant to the generated word. How the model knows which word to attend to is all learned while training with backpropagation. RNNs are also capable of looking at previous inputs too, but the power of the attention mechanism is that it doesn't suffer from short-term memory. RNNs have a shorter window to reference from, so when the story gets longer, RNNs can't access word generated earlier in the sequence. This is still true for GRUs and LSTMs, although they do have a bigger capacity to achieve longer-term memory, therefore having a longer window to reference from. The attention mechanism, in theory, and given enough compute resources, have an infinite window to reference from, therefore being capable of using the entire context of the story while generating the text. This power was demonstrated in the paper Attention is All You Need, where the authors introduce a new novel neural network called the Transformers, which is an attention-based encoder-decoder type architecture. On a high level, the encoder maps an input sequence into an abstract continuous representation that holds all the learned information of that input. The decoder then takes that continuous representation, and step-by-step generates a single output while also being fed the previous output. Let's walk through an example. The Attention is All You Need paper applied the Transformer model on a neural machine translation problem. Our demonstration of the Transformer model would be a conversational chatbot. The example would take in an input text, Hi, how are you, and generate the response, I am fine. Let's break down the mechanics of the network step-by-step. The first step is feeding our input into a word embedding layer. A word embedding layer can be thought of as a lookup table to grab a learned vector representation of each word. Neural networks learn through numbers, so each word maps to a vector with continuous values to represent that word. Next step is to inject positional information into the embeddings. Because the Transformer encoder has no recurrence like recurrent neural networks, we must add information about the positions into the input embeddings. This is done using positional encoding. The authors came up with a clever trick using sine and cosine functions. We won't go into the mathematical details of the positional encodings in this video, but here are the basics. For every odd time step, create a vector using the cosine function. For every even time step, create a vector using the sine function. Then add those vectors to their corresponding embedding vector. This successfully gives the network information on the positions of each vector. The sine and cosine functions were chosen in tandem because they have linear properties the model can easily learn to attend to. Now we have the encoder layer. The encoder layer's job is to map all input sequence into an abstract continuous representation that holds the learned information for that entire sequence. It contains two submodules, multi-headed attention followed by a fully connected network. There are also residual connections around each of the two submodules followed by a layer normalization. To break this down, let's look at the multi-headed attention module. Multi-headed attention in the encoder applies a specific attention mechanism called self-attention. Self-attention allows the model to associate each individual word in the input to other words in the input. So in our example, it's possible that our model can learn to associate the word you with how and are. It's also possible that the model learns that words structured in this pattern are typically a question so respond appropriately. To achieve self-attention, we feed the input into three distinct fully connected layers to create the query, key, and value vectors. What are these vectors exactly? I found a good explanation on Stack Exchange stating, The query, key, and value concept comes from the retrieval system. For example, when you type a query to search for some video on YouTube, the search engine will map your query against a set of keys, for example video title, description, etc., associated with candidate videos in the database, then present you with the best matched video. Let's see how this relates to self-attention. The queries and keys undergoes a dot product matrix multiplication to produce a score matrix. The score matrix determines how much focus should a word be put on other words. So each word will have a score that corresponds to other words in the time step. The higher the score, the more the focus. This is how queries are mapped to keys. Then the scores get scaled down by getting divided by the square root of the dimension of the queries and the keys. This is to allow for more stable gradients as multiplying values can have exploding effects. Next you take the softmax of the scaled score to get the attention weights, which gives you probability values between 0 and 1. By doing the softmax, the higher scores get heightened and the lower scores are depressed. This allows the model to be more confident on which words to attend to. Then you take the attention weights and multiply it by your value vector to get an output vector. The higher softmax scores will keep the value of the words the model learned is more important. The lower scores will drown out the irrelevant words. You feed the output vector into a linear layer to process. To make this a multi-headed attention computation, you need to split the query, key, and value into n vectors before applying self-attention. The split vectors then goes through the same self-attention process individually. Each self-attention process is called a head. Each head produces an output vector that gets concatenated into a single vector before going through the final linear layer. In theory, each head would learn something different, therefore giving the encoder model more representation power. Okay, so that's multi-headed attention. To sum it up, multi-headed attention is a module in the transformer network that computes the attention weights for the input and produces an output vector with encoded information on how each word should attend to all other words in the sequence. Next step, the multi-headed attention output vector is added to the original input. This is called a residual connection. The output of the residual connection goes through a layer normalization. The normalized residual output gets fed into a point-wise feedforward network for further processing. The point-wise feedforward network are a couple of linear layers with a ReLU activation in between. The output of that is again added to the input of the point-wise feedforward network and further normalized. The residual connections helps the network train by allowing gradients to flow through the networks directly. The layer normalizations are used to stabilize the network which results in substantially producing the training time necessary. And the point-wise feedforward layers are used to further process the attention output, potentially giving it a richer representation. And that wraps up the encoder layer. All of these operations is for the purpose of encoding the input to a continuous representation with attention information. This will help the decoder focus on the appropriate words in the input during the decoding process. You can stack the encoder n times to further encode the information, where each layer has the opportunity to learn different attention representations, therefore potentially boosting the predictive power of the transformer network. Now we move on to the decoder. The decoder's job is to generate text sequences. The decoder has similar sublayers as the encoder. It has two multi-headed attention layers, a point-wise feedforward layer, with residual connections and layer normalization after each sublayer. These sublayers behave similarly to layers in the encoder, but each multi-headed attention layer has a different job. It's capped off with a linear layer that acts like a classifier and a softmax to get the word probabilities. The decoder is autoregressive. It takes in the list of previous outputs as inputs, as well as the encoder outputs that contains the attention information from the input. The decoder stops decoding when it generates an end token as an output. Let's walk through the decoding steps. The input goes through an embedding layer and a positional encoding layer to get positional embeddings. The positional embeddings gets fed into the first multi-headed attention layer, which computes the attention score for the decoder's input. This multi-headed attention layer operates slightly different. Since the decoder is autoregressive and generates the sequence word by word, you need to prevent it from conditioning to future tokens. For example, when computing attention scores on the word am, you should not have access to the word fine because that word is a future word that was generated after. The word am should only have access to itself and the words before it. This is true for all other words where they can only attend to previous words. We need a method to prevent computing attention scores for future words. This method is called masking. To prevent the decoder from looking at future tokens, you apply a lookahead mask. The mask is added before calculating the softmax and after scaling the scores. Let's take a look at how this works. The mask is a matrix that's the same size as the attention scores filled with values of zeros and negative infinities. When you add the mask to the scaled attention scores, you get a matrix of scores with the top right triangle filled with negative infinities. The reason for this is once you take the softmax of the masked scores, the negative infinities get zeroed out leaving a zero attention score for future tokens. As you can see, the attention scores for am have values for itself and all other words before it, but zero for the word fine. This essentially tells the model to put no focus on those words. This masking is the only difference on how the attention scores are calculated in the first multi-headed attention layer. This layer still has multiple heads that the mask are being applied to before getting concatenated and fed through a linear layer for further processing. The output of the first multi-headed attention is a mask output vector with information on how the model should attend on the decoder's inputs. Now onto the second multi-headed attention layer. For this layer, the encoder's outputs are the queries and the keys and the first multi-headed attention layer outputs are the values. This process matches the encoder's input to the decoder's input, allowing the decoder to decide which encoder input is relevant to put focus on. The output of the second multi-headed attention goes through a pointwise feedforward layer for further processing. The output of the final pointwise feedforward layer goes through a final linear layer that acts as a classifier. The classifier is as big as the number of classes you have. For example, if you have 10,000 classes for 10,000 words, the output of that classifier will be of size 10,000. The output of the classifier then gets fed into a softmax layer. The softmax layer produces probability scores between 0 and 1 for each class. We take the index of the highest probability score and that equals our predicted word. The decoder then takes the output and adds it to the list of decoder inputs and continue decoding again until the end token is predicted. For our case, the highest probability prediction is the final class which is assigned to the end token. This is how the decoder generates the output. The decoder can be stacked n layers high, each layer taking in inputs from the encoder and the layers before it. By stacking the layers, the model can learn to extract and focus on different combinations of attention from its attention heads, potentially boosting its predictive power. And that's it! That's the mechanics of the transformers. Transformers leverage the power of the attention mechanism to make better predictions. Recurrent known networks try to achieve similar things, but because they suffer from short-term memory, transformers are usually better, especially if you want to encode or generate longer sequences. Because of the transformer architecture, the natural language processing industry can now achieve unprecedented results. If you found this helpful, hit that like and subscribe button. Also, let me know in the comments what you'd like to see next. And until next time, thanks for watching!",
        "output": "### Introduction to Transformers in Natural Language Processing\n\nTransformers have revolutionized natural language processing, achieving various record-breaking results and pushing the state of the art. They have been used in machine translation, chatbots, and improving search engines. Transformers have outperformed RNNs, GRUs, and LSTMs in sequence problems.\n\n### Attention Mechanism\n\nThe attention mechanism allows the model to reference relevant words when generating text. Unlike RNNs, which only have a short window to reference from, the attention mechanism can use the entire context of input when generating text.\n\n### Transformer Architecture\n\nThe transformer architecture is an attention-based encoder-decoder model. The encoder maps an input sequence into an abstract continuous representation, while the decoder generates a single output using the representation from the encoder.\n\n### Positional Encoding\n\nThe inputs are fed into a word embedding layer, which maps words to a vector with continuous values representing each word. Positional information is injected into the embeddings using sine and cosine functions, giving the network information on the positions of each vector.\n\n### Encoder Layer\n\nThe encoder layer's job is to map all input sequences into an abstract continuous representation. It contains two submodules: multi-headed attention and a fully connected network. Additionally, there are residual connections and layer normalization.\n\n### Multi-Headed Attention Module\n\nMulti-headed attention uses self-attention, associating each word in the input with other words in the input. It computes the attention weights for the input and produces an output vector encoding information on how each word should attend to all other words in the sequence.\n\n### Decoder Layer\n\nThe decoder layer generates text sequences. It has two multi-headed attention layers, a point-wise feedforward layer, residual connections, and layer normalizations, as well as a linear layer acting like a classifier and a softmax to get word probabilities.\n\nThe decoder is autoregressive, taking in previous outputs as inputs and the encoder outputs containing attention information from input. This continues until an end token is generated.\n\n### Summary\n\nTransformers leverage the power of attention mechanisms to make better predictions. They have outperformed RNNs in encoding or generating longer sequences, revolutionizing the natural language processing industry."
    },
    {
        "instruction": "<|SYSTEM|>You are a scholars that is taking markdown notes about some video, you would be giving some written scripts for the task.\nYou have finished writing the notes, and you want to extract the title from the notes.\nThe title should only be the name of the technology or algorithm, no extra words needed.\nPlease write the notes in markdown format. No need to write \"Title:\" for the Title. Make sure the title is within 10 words.\nHere is an example:\n## Hidden Markov Model\n<|ASSISTANT|>## ",
        "input": "<|USER|>### Introduction to Transformers in Natural Language Processing\n\nTransformers have revolutionized natural language processing, achieving various record-breaking results and pushing the state of the art. They have been used in machine translation, chatbots, and improving search engines. Transformers have outperformed RNNs, GRUs, and LSTMs in sequence problems.\n\n### Attention Mechanism\n\nThe attention mechanism allows the model to reference relevant words when generating text. Unlike RNNs, which only have a short window to reference from, the attention mechanism can use the entire context of input when generating text.\n\n### Transformer Architecture\n\nThe transformer architecture is an attention-based encoder-decoder model. The encoder maps an input sequence into an abstract continuous representation, while the decoder generates a single output using the representation from the encoder.\n\n### Positional Encoding\n\nThe inputs are fed into a word embedding layer, which maps words to a vector with continuous values representing each word. Positional information is injected into the embeddings using sine and cosine functions, giving the network information on the positions of each vector.\n\n### Encoder Layer\n\nThe encoder layer's job is to map all input sequences into an abstract continuous representation. It contains two submodules: multi-headed attention and a fully connected network. Additionally, there are residual connections and layer normalization.\n\n### Multi-Headed Attention Module\n\nMulti-headed attention uses self-attention, associating each word in the input with other words in the input. It computes the attention weights for the input and produces an output vector encoding information on how each word should attend to all other words in the sequence.\n\n### Decoder Layer\n\nThe decoder layer generates text sequences. It has two multi-headed attention layers, a point-wise feedforward layer, residual connections, and layer normalizations, as well as a linear layer acting like a classifier and a softmax to get word probabilities.\n\nThe decoder is autoregressive, taking in previous outputs as inputs and the encoder outputs containing attention information from input. This continues until an end token is generated.\n\n### Summary\n\nTransformers leverage the power of attention mechanisms to make better predictions. They have outperformed RNNs in encoding or generating longer sequences, revolutionizing the natural language processing industry.",
        "output": "## Transformers in Natural Language Processing"
    },
    {
        "instruction": "<|SYSTEM|>You are a scholars that is taking markdown notes about some video with Wiki style, you would be giving some written scripts for the task.\nAnd summarize each paragraph with a title.\nRemember to keep the technology or algorithm name, very important.\nPlease write the notes in markdown format.\nHere is an example:\n### Definiation\nA deadlock is a situation in which two computer programs sharing the same resource are effectively preventing each other from accessing the resource, resulting in both programs ceasing to function. \nThe earliest computer operating systems ran only one program at a time.\n\n### What it have to do with Relationsal Database ManagementSystem\nIn a database, a deadlock is an unwanted situation in which two or more transactions are waiting indefinitely for one another to give up locks. \nDeadlock is said to be one of the most feared complications in DBMS as it brings the whole system to a Halt. \nWhen you tries to insert or update data on database, sometime your record need to depend on the row record.  \n<|ASSISTANT|>#### ",
        "input": "<|USER|> What is CAP theorem? How useful is it to system design? Let's take a look. The CAP theorem is a concept in computer science that explains the trade-offs between consistency, availability, and partition tolerance in distributed systems. Consistency refers to the property of a system where all nodes have a consistent view of the data. It means all clients see the same data at the same time, no matter which node they connect to. Availability refers to the ability of a system to respond to requests from users at all times. Partition tolerance refers to the ability of a system to continue operating even if there is a network partition. But what is a network partition? A network partition happens when nodes in a distributed system are unable to communicate with each other due to network failures. When there is a network partition, a system must choose between consistency and availability. If the system parities consistency, it may become unavailable until the partition is resolved. If the system parities availability, it may allow updates to the data. This could result in data inconsistencies until the partition is resolved. Now let's go through a concrete example. Let's say we have a tiny bank with two ATMs connected over the network. The ATM supports three operations, deposit, withdrawal, and check balance. No matter what happens, the balance should never go below zero. There is no central database in this bank to keep the account balance. It is stored on both ATMs. When a customer uses an ATM, the balance is updated on both ATMs over the network. This ensures that the ATMs have a consistent view of the account balance. If there's a network partition and ATMs are unable to communicate with each other, the system must choose between consistency and availability. If the bank prioritizes consistency, the ATM may refuse to process deposits or withdrawals until the partition is resolved. This ensures that the balance remains consistent, but the system is unavailable to customers. If the bank prioritizes availability, the ATM may allow deposits and withdrawals to occur, but the balance may become inconsistent until the partition is resolved. When there is a network partition, the customer could withdraw the entire balance from both ATMs. When the network comes back online, the inconsistency is resolved and now the balance is negative. That is not good. Now let's go through another example and see how a social media platform could apply the CAP theorem. During a network partition, if two users are commenting on the same post at the same time, one user's comment may not be visible to the other users until the partition is resolved. Alternatively, if the platform prioritizes consistency, the commenting feature may be unavailable to users until the partition is resolved. For a social network, it is often acceptable to prioritize availability at the cost of users seeing slightly different views some of the time. The CAP theorem may sound very simple, but the real world is messy. As with many things in software engineering, this is all about trade-offs and the choices are not always so black and white. The CAP theorem assumes 100% availability or 100% consistency. In the real world, there are degrees of consistency and availability that distributed system designers must carefully consider. This is where the simplistic model of the CAP theorem could be misleading. Now back to the bank example. During the network partition, the ATM could allow only balance inquiries to be processed while deposits and withdrawals are blocked. Alternatively, the bank could implement a hybrid approach. For example, the ATM could allow balance inquiries and small withdrawals to be processed during a partition, but block large withdrawals and or deposits until the partition is resolved. It is worth noting that in the real world, reconciliation after a network partition could get very messy. The bank example above is simple to reconcile. In real life, the data structures involved could be complex and challenging to reconcile. A good example of a complex data structure is Google docs. Resolving conflicting updates could be tricky. So is the CAP theorem useful? Yes, it is a useful tool to help us think through the high level trade-offs to consider when there's a network partition. This is a good starting point, but it does not provide a complete picture of the trade-offs to consider when designing a well-rounded distributed system. Specifically, when the system is operating normally without a network failure, which is most of the time. There's an entire set of interesting trade-offs to consider between latency and consistency. This is covered by the Peckel theorem, which we should cover in another video. You would like to learn more about system design, check out our books and weekly newsletter. Please subscribe if you learned something new. Thank you so much and we'll see you next time.",
        "output": "### CAP Theorem Definition\nThe CAP theorem is a concept in computer science that explains the trade-offs between consistency, availability, and partition tolerance in distributed systems.\n\n### Network Partition\nA network partition happens when nodes in a distributed system are unable to communicate with each other due to network failures, forcing a system to choose between consistency and availability.\n\n### CAP Theorem Trade-offs\n- Consistency: All nodes have a consistent view of the data.\n- Availability: A system responds to user requests at all times.\n- Partition Tolerance: A system continues operating even if there is a network partition.\n\n### Example: Tiny Bank with Two ATMs\nA system chooses between consistency and availability when there is a network partition, leading to potential undesirable outcomes like negative account balances.\n\n### Example: Social Media Platform\nA social media platform may prioritize availability over consistency, resulting in users sometimes seeing slightly different views on the platform.\n\n### CAP Theorem Limitations\nThe CAP theorem assumes 100% availability or 100% consistency, but in reality, there are degrees of consistency and availability that distributed system designers must carefully consider.\n\n### Real-Life Trade-Offs\nDuring normal operation without network failures, there are additional trade-offs between latency and consistency, which are covered by the Peckel theorem.\n\n### CAP Theorem Conclusion\nThe CAP theorem is a useful tool to help think through high-level trade-offs when there's a network partition, but it does not provide a complete picture of the trade-offs to consider when designing a well-rounded distributed system."
    },
    {
        "instruction": "<|SYSTEM|>You are a scholars that is taking markdown notes about some video, you would be giving some written scripts for the task.\nYou have finished writing the notes, and you want to extract the title from the notes.\nThe title should only be the name of the technology or algorithm, no extra words needed.\nPlease write the notes in markdown format. No need to write \"Title:\" for the Title. Make sure the title is within 10 words.\nHere is an example:\n## Hidden Markov Model\n<|ASSISTANT|>## ",
        "input": "<|USER|>### CAP Theorem Definition\nThe CAP theorem is a concept in computer science that explains the trade-offs between consistency, availability, and partition tolerance in distributed systems.\n\n### Network Partition\nA network partition happens when nodes in a distributed system are unable to communicate with each other due to network failures, forcing a system to choose between consistency and availability.\n\n### CAP Theorem Trade-offs\n- Consistency: All nodes have a consistent view of the data.\n- Availability: A system responds to user requests at all times.\n- Partition Tolerance: A system continues operating even if there is a network partition.\n\n### Example: Tiny Bank with Two ATMs\nA system chooses between consistency and availability when there is a network partition, leading to potential undesirable outcomes like negative account balances.\n\n### Example: Social Media Platform\nA social media platform may prioritize availability over consistency, resulting in users sometimes seeing slightly different views on the platform.\n\n### CAP Theorem Limitations\nThe CAP theorem assumes 100% availability or 100% consistency, but in reality, there are degrees of consistency and availability that distributed system designers must carefully consider.\n\n### Real-Life Trade-Offs\nDuring normal operation without network failures, there are additional trade-offs between latency and consistency, which are covered by the Peckel theorem.\n\n### CAP Theorem Conclusion\nThe CAP theorem is a useful tool to help think through high-level trade-offs when there's a network partition, but it does not provide a complete picture of the trade-offs to consider when designing a well-rounded distributed system.",
        "output": "## CAP Theorem"
    },
    {
        "instruction": "<|SYSTEM|>You are a scholars that is taking markdown notes about some video with Wiki style, you would be giving some written scripts for the task.\nAnd summarize each paragraph with a title.\nRemember to keep the technology or algorithm name, very important.\nPlease write the notes in markdown format.\nHere is an example:\n### Definiation\nA deadlock is a situation in which two computer programs sharing the same resource are effectively preventing each other from accessing the resource, resulting in both programs ceasing to function. \nThe earliest computer operating systems ran only one program at a time.\n\n### What it have to do with Relationsal Database ManagementSystem\nIn a database, a deadlock is an unwanted situation in which two or more transactions are waiting indefinitely for one another to give up locks. \nDeadlock is said to be one of the most feared complications in DBMS as it brings the whole system to a Halt. \nWhen you tries to insert or update data on database, sometime your record need to depend on the row record.  \n<|ASSISTANT|>#### ",
        "input": "<|USER|> Microservices is probably the most popular architectural approach today. It's extremely effective. It's the approach used by many of the most successful companies in the world, particularly the big web companies. As a result, I see lots of organisations and teams attempting to adopt it. Or so they think. In reality, they often miss both the value and the costs of microservices and end up doing something that really isn't microservices at all and so kind of doing the microservice idea as something of a disservice. They don't gain the benefits that they were looking for very often. So what is the problem with microservices? Hi, I'm Dave Farley of Continuous Delivery. Welcome to my channel. If you haven't been here before, please do hit subscribe and like if you enjoy the content. In this episode, I want to explore the problem with microservices and particularly why this very valuable architectural idea is probably not quite what you think it is. It's simple, right? Microservices is all about small services, isn't it? No, not really. The first problem is that microservices is a distributed systems architecture. And however you wrap it up, distributed systems are way more complicated than non-distributed systems. There are all sorts of problems that you just automatically buy into when you start distributing computing across multiple different devices. One of the things that microservices are not are not services with simply a REST API. That's another thing. That's REST. It can be implemented as a microservice or not. REST may be a useful technique in implementing microservices or not. There are many different ways of approaching this, but a RESTful API does not define a microservice. And all services are not microservices. Services as an organizing principle for software systems has a very long history. We'll talk a little bit more about that later on. The defining characteristics of microservices are that they are small, as we've already mentioned. They're focused on accomplishing one single task. They're aligned with the bounded context in the problem domain. They are autonomous, independently deployable and loosely coupled. Let's talk about each of those ideas in a bit more detail. So what does small really mean? Everybody asks this question and it's kind of an obvious question from the name, right? Microservices. So what does micro mean? There is an answer that comes from the early pioneers of microservices, which is that a microservice fits inside of James Lewis's head. James was one of the people that first popularized the idea of microservices. He, along with Martin Fowler, wrote the first article that was published, I believe, online, or at least the best known article that describes them in more detail. And James, as you can see from this picture, has a reasonably large head. He's a smart guy. So a fair amount fits in his head. But the idea is to make them understandable, to compartmentalize a problem so that we can deal with it and understand what's going to happen. A reasonable rule of thumb to try and figure out how small is small, or how micro is micro, is to imagine throwing away your microservice and reimplementing it. How long would that take? If you can do that in a few days or maybe a couple of weeks, it's probably on the right kind of scale. If the idea scares you of throwing away the service and rewriting it from scratch, it's probably too big. The next in the list is focused on a single task and doing it well. This is really about separation of concerns, but at the level of the problem domain. What we're really trying to express here is the idea that a microservice accomplishes one task when viewed from the outside. Now, implementing that task may involve a bit more, more concerns perhaps. Maybe you're thinking about storage or getting data from another source or something like that. But from the outside, the service is going to be focused on accomplishing one task and achieving that well. The next in the list is aligned with a bounded context. Most teams miss this idea, and it's an important one. A bounded context is an idea that comes from Domain Driven Design, the fantastic book written by Eric Evans, which describes modeling the problem domain as an approach to designing the software. And one of the things that Eric talks about, one of the key ideas in Domain Driven Design is this idea of bounded context. Eric defines it as a defined part of software where particular terms, definitions, and rules apply in a consistent way. This is a part of the problem domain where the concepts are related to one another, and so it makes a cohesive unit. This is an important idea in lots of different contexts, but in the sphere of microservices it's a very important idea because those are the boundaries that we would prefer to align our services with. And this is important for several reasons. First, if we break a problem, a monolithic application say, into a collection of smaller technical services, that doesn't really qualify as microservices. They are not aligned with the bounded context, and you're going to suffer from increased coupling between those components. Bounded contexts are kind of like natural firebreaks in the problem domain. They are, if we align our software with those firebreaks, then it means that our system is going to naturally be less coupled because the problem domain is less coupled at those points. And so it means that we can write cleaner interfaces to our services and have less chatty interactions between them. One of Eric's pieces of advice in the Domain Driven Design book is that whenever you transition a bounded context, a boundary, then you should translate ideas, information that crosses the boundary. That means that every microservice should be translating its inputs and translating to its outputs in order for every interaction. Microservice should always make a break between these points. We should make a significant distinction between the external protocol with which the microservice exchanges information with other parts of the system versus the internal representation of that service and the model within. We need to really treat seriously the difference between the design of those APIs, those entry points into our services, and our consumption of those APIs from other services. If you aren't using ports and adapters in your implementation of microservices, you're probably making a big mistake. We want to always be translating at these points. The next in the list is that microservices are autonomous. The teams that look after these microservices can make progress alone without the need to interact with other services or other teams. I can change the implementation of my service without needing to coordinate with anybody or anything else. This is the biggest value of microservices. This is the thing that allows microservices to work in organizations that can scale enormously. But it's also the most commonly missed attribute of microservices. What I see frequently is teams claiming that they're implementing microservices, but building, testing, and deploying all of those pieces together. That's something else. Let's explore this idea for a moment of autonomy. Service-based design is not the same thing as microservices. Service-based design has a long history. It's been around for a very long time. If you'll forgive me, I'm a bit lazy and I haven't looked up the history of service-based design, so I'm going to do this from a personal point of view and tell you my interaction with it. I've been building distributed service-oriented systems, however you want to frame that, for about 30 years now. I started at the end of the 80s and 90s with my first foray into distributed computing. I moved reasonably quickly in the early 90s into ideas like remote procedure calls and then quickly into an idea of something called object request brokers, where we were simulating interactions with component-based systems in distributed systems. I got involved in a startup where we built some infrastructure to support what we would now think of as quite advanced microservices based on a concept called cooperative business objects, little bundles of domain logic that communicated through what we call a semantic data service, a bit like XML, but predating XML. I then moved on bigger commercial systems, working on component-based systems, message-oriented architectures. Service-oriented architecture came in at the beginnings of this century and became more popular. And then I moved into building high-performance systems based on event-driven architectures, sometimes using patterns like CQRS, microservices, and ending up with something that we called reactive systems. All of these are service-oriented systems. All of these models were based on that concept of little bundles of logic that were somehow more discreet from one another than the classes and functions within our implementation. And so, Brett had more abstract interfaces between them. The ideas of service orientation are extremely useful. They're an important idea, I would argue, in computer science and building bigger systems. And I think that they principally matter for two reasons. One of those is technical, the other is organizational. Now these are very closely related, as Mervyn Conway stated in 1967, any organization that designs a system, defined broadly, will produce a design whose structure is a copy of the organization's communication structure. So the architecture, the design of our systems, is closely related to the communication topologies that we employ in the organizations that we work in. This has an impact on the kinds of architectures that it's possible for teams to build. And the architecture mirrors your organizational structure. If we decide to divide up our teams on technical boundaries, we have a UI specialist team, and a middleware team, and a data team, then we're going to build a system that looks a bit like this, a tiered layered system. And the teams will have to coordinate and interact. More modern software development approaches tend to prefer smaller, more autonomous teams. If we're going to, for a variety of reasons, if we are going to organize ourselves along the lines of more modular autonomous teams, then we're going to build more modular software with more autonomous components, and it looks like this. But this is not necessarily a microservice organized system. We could choose how we were going to organize our teams around this. We could put everything into one big repo and have a distributed service oriented system, communicating with REST or messaging or whatever other technique. Or we could have independently deployable units of software, autonomous components that were microservices. They're not the same thing. So if we can have distributed monoliths organized as collection of services, which is probably my preferred architectural approach, if I'm honest, for many systems, what does microservices add to that? Well, the key thing is that they are independently deployable. Deployability is one aspect of autonomy for these services, but it's an incredibly useful one. If we can work on changes to our service and deploy it without necessarily communicating or interacting with other groups of people, that means that we can make progress without being constrained by those groups. It means that we are developmentally less coupled to the rest of the organization. That's really what independently deployable means. If I need to care about dependencies on other services before I release my change into production, my software is not really independently deployable anymore. Well, this is a big idea and this is the biggest stumbling block that I see with teams adopting microservices. And the biggest problem, the biggest cost of microservices, this has serious implications for the way in which we design software and the way in which we employ it. You don't get to test your services together before you release. You don't get to force changes that you make to your service on consumers of your service. You can't demand that they update in step with you. That's not part of a microservices model. If our services are autonomous and independently deployable, we build, test and deploy them independently. This is the real value of microservices. This is the step that decouples organizations both technically and culturally and allows them to grow and create software at an enormously fast rate. It's the thing that differentiates organizations like Amazon from more conventionally organized firms. And this is also the real cost because if you want to be able to independently deploy these services, then you've got to be able to take the ability to maintain that independence really very seriously. Microservices are an organizational decoupling play. That's what they are for. If you don't need to decouple your organization developmentally, they're probably not the right thing to do because they come with this cost. They allow development organizations to grow very big and to work as a large collection of small independent teams. But to achieve this, microservices must be loosely coupled. We need to take that separation of external and internal representation incredibly seriously and we're going to invest time and effort and development effort into keeping that separation clean. We need to treat the external interface of our services like a public API because it is. We can't demand that other people change in step with us when we want to make changes to our API. So we've got to be very, very cautious about the ways in which we can change that API. When we're consuming the API from such a service, we should be cautious not to couple ourselves too tightly to it. We should only take the information that is of interest to us and no further. We'd like to make those interfaces as clean and abstract as we can achieve. So we should be very, very, very cautious of breaking changes at any point. It brings in techniques like supporting multiple versions of interfaces on the same service or multiple versions of services in some approaches. These are the techniques of microservices. This is the cost of microservices, but for that cost you get some huge advantages. We need to create interfaces to our services that we can defend and rarely and only with great caution change. There's lots more to talk about on this topic. We'll certainly explore microservices and modular loose coupled design in future episodes. But if there's anything else that you think would be interesting, please do let me know in the comments. Thank you very much for watching. Thank you.",
        "output": "### Misconceptions about Microservices\n\nMicroservices is a popular architectural approach, but many teams often misunderstand its value and costs. The key defining characteristics of microservices are that they are small, focused on accomplishing a single task, aligned with a bounded context, autonomous, independently deployable, and loosely coupled.\n\n### Distributed Systems Architecture\n\nThe first problem is that microservices is a distributed systems architecture, making them more complicated than non-distributed systems. RESTful APIs alone do not define a microservice, and not all services are microservices. \n\n### Characteristics in Detail\n\n- **Small**: The size of a microservice should be such that it is understandable and can be reimplemented within a few days or weeks.\n- **Focused on a single task**: The microservice should be able to accomplish one single task when viewed from outside.\n- **Aligned with a bounded context**: Microservices should be aligned with natural firebreaks in the problem domain, making the system naturally less coupled.\n- **Autonomous**: Microservices should work in isolation, without the need to interact with other services or teams.\n- **Independently deployable**: This aspect of autonomy allows organizations to scale significantly and create software at a faster rate.\n- **Loosely coupled**: Microservices must be loosely coupled to maintain independence.\n\n### Microservices vs Service-Based Design\n\nMicroservices allow organizations to grow and create software at a faster rate due to their organizational and developmental decoupling. However, service-based designs are also valuable and have a long history in computer science. Service-oriented systems can be organized in various ways, such as distributed monoliths or microservices.\n\n### The Real Value and Cost of Microservices\n\nThe real value of microservices is in their ability to decouple organizations, allowing them to create software at a faster pace. The real cost lies in maintaining that independence, taking separation in external and internal representations seriously, and avoiding breaking changes. Microservices are beneficial for organizational growth, but they require investment in keeping their separation clean and interfaces abstract.\n\n### Summary\n\nMicroservices are powerful but often misunderstood. Their key characteristics are their small size, focus on accomplishing a single task, alignment with a bounded context, autonomy, independent deployability, and loose coupling. Properly implemented microservices can provide significant benefits to developmental processes and organizational growth, but they also come with challenges and costs that must be managed."
    },
    {
        "instruction": "<|SYSTEM|>You are a scholars that is taking markdown notes about some video, you would be giving some written scripts for the task.\nYou have finished writing the notes, and you want to extract the title from the notes.\nThe title should only be the name of the technology or algorithm, no extra words needed.\nPlease write the notes in markdown format. No need to write \"Title:\" for the Title. Make sure the title is within 10 words.\nHere is an example:\n## Hidden Markov Model\n<|ASSISTANT|>## ",
        "input": "<|USER|>### Misconceptions about Microservices\n\nMicroservices is a popular architectural approach, but many teams often misunderstand its value and costs. The key defining characteristics of microservices are that they are small, focused on accomplishing a single task, aligned with a bounded context, autonomous, independently deployable, and loosely coupled.\n\n### Distributed Systems Architecture\n\nThe first problem is that microservices is a distributed systems architecture, making them more complicated than non-distributed systems. RESTful APIs alone do not define a microservice, and not all services are microservices. \n\n### Characteristics in Detail\n\n- **Small**: The size of a microservice should be such that it is understandable and can be reimplemented within a few days or weeks.\n- **Focused on a single task**: The microservice should be able to accomplish one single task when viewed from outside.\n- **Aligned with a bounded context**: Microservices should be aligned with natural firebreaks in the problem domain, making the system naturally less coupled.\n- **Autonomous**: Microservices should work in isolation, without the need to interact with other services or teams.\n- **Independently deployable**: This aspect of autonomy allows organizations to scale significantly and create software at a faster rate.\n- **Loosely coupled**: Microservices must be loosely coupled to maintain independence.\n\n### Microservices vs Service-Based Design\n\nMicroservices allow organizations to grow and create software at a faster rate due to their organizational and developmental decoupling. However, service-based designs are also valuable and have a long history in computer science. Service-oriented systems can be organized in various ways, such as distributed monoliths or microservices.\n\n### The Real Value and Cost of Microservices\n\nThe real value of microservices is in their ability to decouple organizations, allowing them to create software at a faster pace. The real cost lies in maintaining that independence, taking separation in external and internal representations seriously, and avoiding breaking changes. Microservices are beneficial for organizational growth, but they require investment in keeping their separation clean and interfaces abstract.\n\n### Summary\n\nMicroservices are powerful but often misunderstood. Their key characteristics are their small size, focus on accomplishing a single task, alignment with a bounded context, autonomy, independent deployability, and loose coupling. Properly implemented microservices can provide significant benefits to developmental processes and organizational growth, but they also come with challenges and costs that must be managed.",
        "output": "## Microservices"
    },
    {
        "instruction": "<|SYSTEM|>You are a scholars that is taking markdown notes about some video with Wiki style, you would be giving some written scripts for the task.\nAnd summarize each paragraph with a title.\nRemember to keep the technology or algorithm name, very important.\nPlease write the notes in markdown format.\nHere is an example:\n### Definiation\nA deadlock is a situation in which two computer programs sharing the same resource are effectively preventing each other from accessing the resource, resulting in both programs ceasing to function. \nThe earliest computer operating systems ran only one program at a time.\n\n### What it have to do with Relationsal Database ManagementSystem\nIn a database, a deadlock is an unwanted situation in which two or more transactions are waiting indefinitely for one another to give up locks. \nDeadlock is said to be one of the most feared complications in DBMS as it brings the whole system to a Halt. \nWhen you tries to insert or update data on database, sometime your record need to depend on the row record.  \n<|ASSISTANT|>#### ",
        "input": "<|USER|> Suppose I give you two different lists of numbers, or maybe two different functions, and I ask you to think of all the ways you might combine those two lists to get a new list of numbers, or combine the two functions to get a new function. Maybe one simple way that comes to mind is to simply add them together term by term. Likewise with the functions, you can add all the corresponding outputs. In a similar vein, you could also multiply the two lists term by term, and do the same thing with the functions. But there's another kind of combination just as fundamental as both of those, but a lot less commonly discussed, known as a convolution. But unlike the previous two cases, it's not something that's merely inherited from an operation you can do to numbers. It's something genuinely new for the context of lists of numbers or combining functions. They show up all over the place, they are ubiquitous in image processing, it's a core construct in the theory of probability, they're used a lot in solving differential equations, and one context where you've almost certainly seen it, if not by this name, is multiplying two polynomials together. As someone in the business of visual explanations, this is an especially great topic, because the formulaic definition, in isolation and without context, can look kind of intimidating, but if we take the time to really unpack what it's saying, and before that actually motivate why you would want something like this, it's an incredibly beautiful operation. And I have to admit, I actually learned a little something while putting together the visuals for this project. In the case of convolving two different functions, I was trying to think of different ways you might picture what that could mean, and with one of them I had a little bit of an aha moment for why it is that normal distributions play the role that they do in probability, why it's such a natural shape for a function. But I'm getting ahead of myself, there's a lot of setup for that one. In this video, our primary focus is just going to be on the discrete case, and in particular building up to a very unexpected but very clever algorithm for computing these. And I'll pull out the discussion for the continuous case into a second part. It's very tempting to open up with the image processing examples, since they're visually the most intriguing, but there are a couple bits of finickiness that make the image processing case less representative of convolutions overall, so instead let's kick things off with probability. And in particular, one of the simplest examples that I'm sure everyone here has thought about at some point in their life, which is rolling a pair of dice and figuring out the chances of seeing various different sums. And you might say, not a problem, not a problem. Each of your two dice has six different possible outcomes, which gives us a total of 36 distinct possible pairs of outcomes, and if we just look through them all, we can count up how many pairs have a given sum. And arranging all the pairs in a grid like this, one pretty nice thing is that all of the pairs that have a constant sum are visible along one of these different diagonals. So simply counting how many exist on each of those diagonals will tell you how likely you are to see a particular sum. And I'd say, very good, very good, but can you think of any other ways that you might visualize the same question? Other images that can come to mind to think of all the distinct pairs that have a given sum? And maybe one of you raises your hand and says, yeah, I've got one. Let's say you picture these two different sets of possibilities each in a row, but you flip around that second row. That way, all of the different pairs which add up to seven line up vertically like this. And if we slide that bottom row all the way to the right, then the unique pair that adds up to two, the snake eyes, are the only ones that align. And if I schlunk that over one unit to the right, the pairs which align are the two different pairs that add up to three. And in general, different offset values of this lower array, which remember I had to flip around first, reveal all the distinct pairs that have a given sum. As far as probability questions go, this still isn't especially interesting because all we're doing is counting how many outcomes there are in each of these categories. But that is with the implicit assumption that there's an equal chance for each of these faces to come up. But what if I told you I have a special set of dice that's not uniform? Maybe the blue die has its own set of numbers describing the probabilities for each face coming up, and the red die has its own unique distinct set of numbers. In that case, if you wanted to figure out, say, the probability of seeing a 2, you would multiply the probability that the blue die is a 1 times the probability that the red die is a 1. And for the chances of seeing a 3, you look at the two distinct pairs where that's possible, and again, multiply the corresponding probabilities, and then add those two products together. Similarly, the chances of seeing a 4 involves multiplying together three different pairs of possibilities and adding them all together. And in the spirit of setting up some formulas, let's name these top probabilities a 1, a 2, a 3, and so on, and name the bottom ones b 1, b 2, b 3, and so on. And in general, this process where we're taking two different arrays of numbers, flipping the second one around, and then lining them up at various different offset values, taking a bunch of pairwise products and adding them up, that's one of the fundamental ways to think about what a convolution is. So, just to spell it out a little more exactly, through this process, we just generated probabilities for seeing 2, 3, 4, on and on, up to 12. And we got them by mixing together one list of values, a, and another list of values, b. In the lingo, we'd say the convolution of those two sequences gives us this new sequence, the new sequence of 11 values, each of which looks like some sum of pairwise products. If you prefer, another way you could think about the same operation is to first create a table of all the pairwise products, and then add up along all these diagonals. Again, that's a way of mixing together these two sequences of numbers to get us a new sequence of 11 numbers. It's the same operation as the sliding windows thought, just another perspective. Putting a little notation to it, here's how you might see it written down. The convolution of a and b, denoted with this little asterisk, is a new list, and the nth element of that list looks like a sum, and that sum goes over all different pairs of indices, i and j, so that the sum of those indices is equal to n. It's kind of a mouthful, but for example, if n was 6, the pairs we're going over are 1 and 5, 2 and 4, 3 and 3, 4 and 2, 5 and 1, all the different pairs that add up to 6. But honestly, however you write it down, the notation is secondary in importance to the visual you might hold in your head for the process. Here, maybe it helps to do a super simple example, where I might ask you what's the convolution of the list 1, 2, 3 with the list 4, 5, 6. You might picture taking both of these lists, flipping around that second one, and then starting with its lid all the way over to the left. Then the pair of values which align are 1 and 4, multiply them together, and that gives us our first term of our output. Slide that bottom array one unit to the right, the pairs which align are 1, 5, and 2 and 4, multiply those pairs, add them together, and that gives us 13, the next entry in our output. Slide things over once more, and we'll take 1 times 6, plus 2 times 5, plus 3 times 4, which happens to be 28. One more slide and we get 2 times 6, plus 3 times 5, and that gives us 27. And finally the last term will look like 3 times 6. If you'd like, you can pull up whatever your favorite programming language is, and your favorite library that includes various numerical operations, and you can confirm I'm not lying to you. If you take the convolution of 1, 2, 3 against 4, 5, 6, this is indeed the result that you'll get. We've seen one case where this is a natural and desirable operation, adding up to probability distributions. And another common example would be a moving average. Imagine you have some long list of numbers, and you take another smaller list of numbers that all add up to 1. In this case I just have a little list of 5 values and they're all equal to 1 5th. Then if we do this sliding window convolution process, and kind of close our eyes and sweep under the rug what happens at the very beginning of it, once our smaller list of values entirely overlaps with the bigger one, think about what each term in this convolution really means. At each iteration what you're doing is multiplying each of the values from your data by 1 5th, and adding them all together. Which is to say, you're taking an average of your data inside this little window. Overall the process gives you a smoothed out version of the original data. And you could modify this starting with a different little list of numbers, and as long as that little list all adds up to 1, you can still interpret it as a moving average. In the example shown here, that moving average would be giving more weight towards the central value. This also results in a smoothed out version of the data. If you do kind of a two-dimensional analog of this, it gives you a fun algorithm for blurring a given image. And I should say the animations I'm about to show are modified from something I originally made for part of a set of lectures I did with the Julia lab at MIT for a certain open courseware class that included an image processing unit. There we did a little bit more to dive into the code behind all of this, so if you're curious I'll leave you some links. But focusing back on this blurring example, what's going on is I've got this little 3x3 grid of values that's marching along our original image. And if we zoom in, each one of those values is 1 9th. And what I'm doing at each iteration is multiplying each of those values by the corresponding pixel that it sits on top of. And of course in computer science we think of colors as little vectors of three values, representing the red, green, and blue components. When I multiply all these little values by 1 9th and I add them together, it gives us an average along each color channel, and the corresponding pixel for the image on the right is defined to be that sum. The overall effect, as we do this for every single pixel on the image, is that each one kind of bleeds into all of its neighbors, which gives us a blurrier version than the original. In the lingo, we'd say that the image on the right is a convolution of our original image with a little grid of values. Or more technically, maybe I should say that it's the convolution with a 180 degree rotated version of that little grid of values. Not that it matters when the grid is symmetric, but it's just worth keeping in mind that the definition of a convolution, as inherited from the pure math context, should always invite you to think about flipping around that second array. If we modify this slightly, we can get a much more elegant blurring effect by choosing a different grid of values. In this case, I have a little 5x5 grid, but the distinction is not so much its size. If we zoom in, we notice that the value in the middle is a lot bigger than the value towards the edges, and where this is coming from is they're all sampled from a bell curve, known as a Gaussian distribution. That way, when we multiply all of these values by the corresponding pixel that they're sitting on top of, we're giving a lot more weight to that central pixel, and much less towards the ones out at the edge. And just as before, the corresponding pixel on the right is defined to be this sum. As we do this process for every single pixel, it gives a blurring effect which much more authentically simulates the notion of putting your lens out of focus, or something like that. But blurring is far from the only thing that you can do with this idea. For instance, take a look at this little grid of values, which involves some positive numbers on the left, and some negative numbers on the right, which I'll color with blue and red respectively. Take a moment to see if you can predict and understand what effect this will have on the final image. So in this case, I'll just be thinking of the image as grayscale instead of colored, so each of the pixels is just represented by one number instead of three. And one thing worth noticing is that as we do this convolution, it's possible to get negative values. For example, at this point here, if we zoom in, the left half of our little grid sits entirely on top of black pixels, which would have a value of zero, but the right half of negative values all sit on top of white pixels, which would have a value of one. So when we multiply corresponding terms and add them together, the results will be very negative. And the way I'm displaying this with the image on the right is to color negative values red and positive values blue. Another thing to notice is that when you're on a patch that's all the same color, everything goes to zero, since the sum of the values in our little grid is zero. This is very different from the previous two examples where the sum of our little grid was one, which let us interpret it as a moving average and hence a blur. All in all, this little process basically detects wherever there's variation in the pixel value as you move from left to right, and so it gives you a kind of way to pick up on all the vertical edges from your image. And similarly, if we rotated that grid around so that it varies as you move from the top to the bottom, this will be picking up on all the horizontal edges, which in the case of our little pie creature image does result in some pretty demonic eyes. This smaller grid, by the way, is often called a kernel, and the beauty here is how just by choosing a different kernel you can get different image processing effects, not just blurring or edge detection, but also things like sharpening. For those of you who have heard of a convolutional neural network, the idea there is to use data to figure out what the kernels should be in the first place, as determined by whatever the neural network wants to detect. Another thing I should maybe bring up is the length of the output. For something like the moving average example, you might only want to think about the terms when both of the windows fully align with each other. Or in the image processing example, maybe you want the final output to have the same size as the original. Now, convolutions as a pure math operation always produce an array that's bigger than the two arrays that you started with, at least assuming one of them doesn't have a length of one. Just know that in certain computer science contexts you often want to deliberately truncate that output. Another thing worth highlighting is that in the computer science context, this notion of flipping around that kernel before you let it march across the original often feels really weird and just uncalled for, but again, note that that's what's inherited from the pure math context, where like we saw with the probabilities, it's an incredibly natural thing to do. And actually I can show you one more pure math example where even the programmers should care about this one, because it opens the doors for a much faster algorithm to compute all of these. To set up what I mean by faster here, let me go back and pull up some Python again, and I'm going to create two different relatively big arrays. Each one will have a hundred thousand random elements in it, and I'm going to assess the runtime of the convolve function from the NumPy library. And in this case it runs it for multiple different iterations, tries to find an average, and it looks like, on this computer at least, it averages at 4.87 seconds. By contrast, if I use a different function from the SciPy library called fftconvolve, which is the same thing just implemented differently, that only takes 4.3 milliseconds on average, so three orders of magnitude improvement. And again, even though it flies under a different name, it's giving the same output that the other convolve function does, it's just doing something to go about it in a cleverer way. Remember how with the probability example, I said another way you could think about the convolution was to create this table of all the pairwise products, and then add up those pairwise products along the diagonals? There's of course nothing specific to probability. Anytime you're convolving two different lists of numbers, you can think about it this way. Create this kind of multiplication table with all pairwise products, and then each sum along the diagonal corresponds to one of your final outputs. One context where this view is especially natural is when you multiply together two polynomials. For example, let me take the little grid we already have and replace the top terms with 1, 2x, and 3x squared, and replace the other terms with 4, 5x, and 6x squared. Now, think about what it means when we're creating all of these different pairwise products between the two lists. What you're doing is essentially expanding out the full product of the two polynomials I have written down, and then when you add up along the diagonal, that corresponds to collecting all like terms, which is pretty neat. Expanding a polynomial and collecting like terms is exactly the same process as a convolution. But this allows us to do something that's pretty cool, because think about what we're saying here. We're saying if you take two different functions and you multiply them together, which is a simple pointwise operation, that's the same thing as if you had first extracted the coefficients from each one of those, assuming they're polynomials, and then taken a convolution of those two lists of coefficients. What makes that so interesting is that convolutions feel, in principle, a lot more complicated than simple multiplication. And I don't just mean conceptually they're harder to think about. I mean, computationally it requires more steps to perform a convolution than it does to perform a pointwise product of two different lists. For example, let's say I gave you two really big polynomials, say each one with a hundred different coefficients. Then if the way you multiply them was to expand out this product, you know, filling in this entire 100 by 100 grid of pairwise products, that would require you to perform 10,000 different products. And then when you're collecting all the like terms along the diagonals, that's another set of around 10,000 operations. More generally, in the lingo, we'd say the algorithm is O of n squared, meaning for two lists of size n, the way that the number of operations scales is in proportion to the square of n. On the other hand, if I think of two polynomials in terms of their outputs, for example sampling their values at some handful of inputs, then multiplying them only requires as many operations as the number of samples, since again it's a pointwise operation. And with polynomials, you only need finitely many samples to be able to recover the coefficients. For example, two outputs are enough to uniquely specify a linear polynomial, three outputs would be enough to uniquely specify a quadratic polynomial, and in general, if you know n distinct outputs, that's enough to uniquely specify a polynomial that has n different coefficients. Or if you prefer, we could phrase this in the language of systems of equations. Imagine I tell you I have some polynomial, but I don't tell you what the coefficients are. Those are a mystery to you. In our example, you might think of this as the product that we're trying to figure out. And then suppose I say, I'll just tell you what the outputs of this polynomial would be if you inputted various different inputs, like 0, 1, 2, 3, on and on, and I give you enough so that you have as many equations as you have unknowns. It even happens to be a linear system of equations, so that's nice. And in principle, at least, this should be enough to recover the coefficients. So the rough algorithm outline then would be, whenever you want to convolve two lists of numbers, you treat them like they're coefficients of two polynomials, you sample those polynomials at enough outputs, multiply those samples point-wise, and then solve this system to recover the coefficients as a sneaky backdoor way to find the convolution. And as I've stated it so far, at least, some of you could rightfully complain, Grant, that is an idiotic plan, because for one thing, just calculating all these samples for one of the polynomials we know already takes on the order of n-squared operations. Not to mention, solving that system is certainly going to be computationally as difficult as just doing the convolution in the first place. So, like, sure, we have this connection between multiplication and convolutions, but all of the complexity happens in translating from one viewpoint to the other. But there is a trick, and those of you who know about Fourier transforms and the FFT algorithm might see where this is going. If you're unfamiliar with those topics, what I'm about to say might seem completely out of the blue. Just know that there are certain paths you could have walked in math that make this more of an expected step. Basically, the idea is that we have a freedom of choice here. If instead of evaluating at some arbitrary set of inputs, like 0, 1, 2, 3, on and on, you choose to evaluate on a very specially selected set of complex numbers, specifically the ones that sit evenly spaced on the unit circle, what are known as the roots of unity, this gives us a friendlier system. The basic idea is that by finding a number where taking its powers falls into this cycling pattern, it means that the system we generate is going to have a lot of redundancy in the different terms that you're calculating, and by being clever about how you leverage that redundancy, you can save yourself a lot of work. This set of outputs that I've written has a special name. It's called the discrete Fourier transform of the coefficients, and if you want to learn more, I actually did another lecture for that same Julia MIT class all about discrete Fourier transforms, and there's also a really excellent video on the channel Reducible talking about the fast Fourier transform, which is an algorithm for computing these more quickly. Also Veritasium recently did a really good video on FFTs, so you've got lots of options. And that fast algorithm really is the point for us. Again, because of all this redundancy, there exists a method to go from the coefficients to all of these outputs, where instead of doing on the order of n squared operations, you do on the order of n times the log of n operations, which is much much better as you scale to big lists. And importantly, this FFT algorithm goes both ways. It also lets you go from the outputs to the coefficients. So, bringing it all together, let's look back at our algorithm outline. Now we can say, whenever you're given two long lists of numbers and you want to take their convolution, first compute the FFT of each one of them, which in the back of your mind you can just think of as treating them like they're the coefficients of a polynomial and evaluating it at a very specially selected set of points. Then multiply together the two results that you just got point-wise, which is nice and fast, and then do an inverse FFT. And what that gives you is the sneaky backdoor way to compute the convolution that we were looking for, but this time it only involves O of n log n operations. That's really cool to me. This very specific context where convolutions show up, multiplying two polynomials, opens the doors for an algorithm that's relevant everywhere else where convolutions might come up. If you want to add probability distributions, do some large image processing, whatever it might be. And I just think that's such a good example of why you should be excited when you see some operation or concept in math show up in a lot of seemingly unrelated areas. If you want a little homework, here's something that's fun to think about. Explain why when you multiply two different numbers, just ordinary multiplication the way we all learn in elementary school, what you're doing is basically a convolution between the digits of those numbers. There's some added steps with carries and the like, but the core step is a convolution. In light of the existence of a fast algorithm, what that means is if you have two very large integers, then there exists a way to find their product that's faster than the method we learn in elementary school, that instead of requiring O of n squared operations, only requires O of n log n, which doesn't even feel like it should be possible. The catch is that before this is actually useful in practice, your numbers would have to be absolutely monstrous. But still, it's cool that such an algorithm exists. And next up, we'll turn our attention to the continuous case, with a special focus on probability distributions.",
        "output": "### Introduction to Convolutions\nA convolution is a fundamental operation combining two lists of numbers or functions, ubiquitous in areas such as image processing, probability theory, and solving differential equations. Multiplying two polynomials together is also an instance of convolution.\n\n### Convolution in Probability\nThe convolution process can help compute the probability of various sums when rolling a pair of dice with distinct probabilities for each face. It involves mixing two sequences of numbers (probabilities for each face) to generate a new sequence. Alternatively, the process can be viewed as creating a table of pairwise products and summing its diagonals.\n\n### Moving Averages\nConvolutions can perform moving averages by choosing a list of numbers that add up to 1 and aligning smaller windows of values with the larger list, then multiplying corresponding terms and summing the results. In image processing, this produces a smoothed or blurred version of the original data. Modifying the list can change the average's weighting.\n\n### Image Processing Examples\nIn image processing, the convolution of an original image with a grid of values results in a new image with different effects like blurring, edge detection, or sharpening. Choosing the grid of values (kernel) and multiplying corresponding terms allows these effects.\n\n### Convolutions in Polynomials\nWhen multiplying polynomials together, convolutions are equivalent to expanding the product and collecting like terms. The complexity comes from translating from one viewpoint to another.\n\n### Fast Fourier Transforms (FFT)\nBy selecting a specific set of complex numbers (roots of unity) as inputs for polynomial evaluation, the discrete Fourier transform can be computed more effectively. This allows for a faster algorithm to compute convolutions in O(n log n) operations. This benefits areas like probability distributions, image processing, and large integer multiplication."
    },
    {
        "instruction": "<|SYSTEM|>You are a scholars that is taking markdown notes about some video, you would be giving some written scripts for the task.\nYou have finished writing the notes, and you want to extract the title from the notes.\nThe title should only be the name of the technology or algorithm, no extra words needed.\nPlease write the notes in markdown format. No need to write \"Title:\" for the Title. Make sure the title is within 10 words.\nHere is an example:\n## Hidden Markov Model\n<|ASSISTANT|>## ",
        "input": "<|USER|>### Introduction to Convolutions\nA convolution is a fundamental operation combining two lists of numbers or functions, ubiquitous in areas such as image processing, probability theory, and solving differential equations. Multiplying two polynomials together is also an instance of convolution.\n\n### Convolution in Probability\nThe convolution process can help compute the probability of various sums when rolling a pair of dice with distinct probabilities for each face. It involves mixing two sequences of numbers (probabilities for each face) to generate a new sequence. Alternatively, the process can be viewed as creating a table of pairwise products and summing its diagonals.\n\n### Moving Averages\nConvolutions can perform moving averages by choosing a list of numbers that add up to 1 and aligning smaller windows of values with the larger list, then multiplying corresponding terms and summing the results. In image processing, this produces a smoothed or blurred version of the original data. Modifying the list can change the average's weighting.\n\n### Image Processing Examples\nIn image processing, the convolution of an original image with a grid of values results in a new image with different effects like blurring, edge detection, or sharpening. Choosing the grid of values (kernel) and multiplying corresponding terms allows these effects.\n\n### Convolutions in Polynomials\nWhen multiplying polynomials together, convolutions are equivalent to expanding the product and collecting like terms. The complexity comes from translating from one viewpoint to another.\n\n### Fast Fourier Transforms (FFT)\nBy selecting a specific set of complex numbers (roots of unity) as inputs for polynomial evaluation, the discrete Fourier transform can be computed more effectively. This allows for a faster algorithm to compute convolutions in O(n log n) operations. This benefits areas like probability distributions, image processing, and large integer multiplication.",
        "output": "## Convolutions"
    },
    {
        "instruction": "<|SYSTEM|>You are a scholars that is taking markdown notes about some video with Wiki style, you would be giving some written scripts for the task.\nAnd summarize each paragraph with a title.\nRemember to keep the technology or algorithm name, very important.\nPlease write the notes in markdown format.\nHere is an example:\n### Definiation\nA deadlock is a situation in which two computer programs sharing the same resource are effectively preventing each other from accessing the resource, resulting in both programs ceasing to function. \nThe earliest computer operating systems ran only one program at a time.\n\n### What it have to do with Relationsal Database ManagementSystem\nIn a database, a deadlock is an unwanted situation in which two or more transactions are waiting indefinitely for one another to give up locks. \nDeadlock is said to be one of the most feared complications in DBMS as it brings the whole system to a Halt. \nWhen you tries to insert or update data on database, sometime your record need to depend on the row record.  \n<|ASSISTANT|>#### ",
        "input": "<|USER|> This paper is incredibly exciting because it illuminates a vision for the future of AI that's radically different from what we see today. It proposes a new alternative to backprop, which might not sound like such a radical change, but give me some time to get there because I don't think what I said is that much of an exaggeration. So what is so special about this paper, or specifically, why care about this backprop alternative, when there's actually a number of other papers that give their own versions of backprop alternatives? Well, for one, this paper is written by Jeffrey Hinton, who was one of the people who originally popularized backprop. Though even without a call to authority, there is a much better reason here, and that's that this method lays a possible framework for the future of AI that I think is more promising than the current paradigm. A future where we have hardware chips that learn. Not neural networks written in software that's, you know, running on a CPU or a GPU. I mean an actual standalone silicon chip that learns, nothing else required. And that's what I'm building up to in this video. But first, we need to rewind a bit and understand, one, what's the motivation that leads up to this? Two, how does this newly proposed method actually work, including what some of those results look like? And three, how this paves the way to that potential future I mentioned. Speaking of which, if you want to carve your own way through the future with ML, you should check out today's sponsor, ClearML, who makes videos like this possible. ClearML offers an end-to-end platform for MLOps, where you can do everything from tracking experiments to automating an entire machine learning pipeline through to deployment. Here's the code for one of my current projects, and with just an extra several lines of code, I can integrate it with ClearML. And with that, we can now pull up a dashboard and see the progress of my experiment as it runs. Here, I'm just logging the loss because I wanted to get this done in a few seconds, but there is a lot more you can do with this. But really, this is just the tip of the iceberg. You can also create entire pipelines that pull in different versions of your datasets, run hyperparameter sweeps, automatically set up new environments, and because of that last point, ClearML has no problem with scaling. As I run many experiments in my own research, I personally love some of the features that help me with that. For example, I love how ClearML not only tracks code version, but also uncommitted Git changes, which is very nice, and also how through the dashboard, I can inject configs into my experiments, which means I don't have to constantly be modifying my code to run a bunch of new experiments. If that sounds like something you're interested in, you can try out their product for free by following the link in the description below. Back to this paper and starting with the motivation, well, the motivation really all comes down to the brain. Because although there have been some very interesting ideas about how something like backpropagation could be done in the brain, these ideas all fall short in one way or another when compared to neural networks in real brains. For example, imagine the case of an agent interacting with the real, continual world. To be successful, it needs to learn on the fly, constantly processing information so that it can act, while at the same time, learning from top-down feedback signals based on the outcomes of its actions. With backprop, a single agent can't do that. It has to take a bunch of actions, then take a bit of a break, while it backpropagates info about the last few time steps to keep learning. Imagine going about your day, but all the time, constantly, you're just blacking out for fractions of a second because your brain needs to stop to backpropagate. It doesn't sound too ideal. And what I'm describing right now at a very, very high level is backpropagation through time, which, as Hinton points out in his paper, is implausible for just this reason. And another issue with backpropagation is that it requires perfect knowledge of the computation performed in the forward pass. And this is so that once the backpropagation phase does come, we can calculate derivatives which are essential for doing backpropagation. As pointed out here, if instead a black box is put into the forward pass, then, well, we can't do backpropagation anymore, which is quite limiting. Now, there is sort of one exception to this, and that is this case where we use reinforcement learning. The issue with reinforcement learning, as pointed out here, is that it suffers from very high variance. And what that means is because in reinforcement learning, when we're trying to maximize a reward, the reward we get is only a single number, something like 5 or 6 or negative 0.5. And just having one number for the signal of all the things you need to learn how to do, it's not very much information, which means that reinforcement learning can often be very, very slow. So to reiterate, backpropagation is an amazing algorithm that has gotten us very far, but it does have some issues when you want to use it like you would use a brain. For one, it can't learn on the fly the same way a brain does. And two, the requirement that we must have perfect knowledge of the forward pass computation is quite limiting when it comes to what we can actually put in our neural network. The purpose of the forward-forward algorithm, which is what will become the focal point of this paper, is to alleviate both of these problems. It, one, can learn without knowing precisely what the forward computation looks like. And two, it can also learn while constantly pipelining sequential data through the network, and it doesn't ever have to actually stop or store any neural activities to successfully learn. That being said, that does not mean that the forward-forward algorithm is just going to come up and replace backprop and be this perfect thing. Hinton even says this himself, it is unlikely to replace backpropagation for applications where power is not an issue. So why then am I so excited about the forward-forward algorithm? Well, it's for one of the two cases where Hinton believes this will be a useful algorithm. The two cases he gives for when this might be superior to backpropagation are, one, as a model of learning in the cortex, which is always good to have a model of the brain we always want to understand about how we work. But the one that I am even much more excited about is this, a way of making use of very low-power analog hardware. Now, it is probably not at all immediately obvious why I think this specific point is so important, but this is exactly what I was referring to when I was talking about how we could have standalone chips that learn and that are super energy efficient. We are going to get to all of that and talk a bit more about why this is so important. But first, let's go through the forward-forward algorithm and understand how it works. Now, as you might be able to guess by the name, the forward-forward algorithm does not have a forward and a backward pass like backpropagation does, but rather it has two forward passes, and just that. If we're doing two forward passes, you might ask, what is the difference between these two passes? And there are two differences. The first thing is that both of these forward passes work on different data, and the second point is that they have opposite objectives. Now, what this exactly means, what different types of data we'll be using, and what these different objectives are, we're about to dive into. But I'm just going to give you a warning that because this algorithm is probably not what you're accustomed to, it might be a little bit weird at first, and that's okay. Once we get through the entire thing, then I'll go over a summary and hopefully it will make more sense as we pull it all together. But just so that we're on the same page, the main point of the forward-forward algorithm is to learn a representation in a sort of unsupervised way. So if we're going to be feeding images, for example, into the forward-forward algorithm, then the purpose would be to learn a useful representation of that image. With that in mind, let's jump into this algorithm. As this starts out by saying, the aim of the learning is to make the goodness be well above some threshold for real data, well below that value for negative data. So already we're getting some different terms. One is goodness, which is certainly not like a very fancy word, but you know, nothing wrong with using simple words. I much prefer that. The other one is real data, which does imply, yes, we have negative data, which is going to be eventually some sort of fake data. Let's start just by talking about this. Going back to this idea of there being two different forward passes in both having different data, let's talk about the differences between them. For the first pass, what we have is we have real data, or what they refer to this in the paper is as positive data. So if we have an image, perhaps this would be a normal image, maybe something like a picture of a good old doggo. And I know my drawing is impeccable. Feel free to leave a compliment in the comments or even better, subscribe to the channel if you like my doggo drawings. And the second type of data is going to be negative data. Negative data is going to be data that is not from the positive data set. So that's pretty broad, right? And that is kind of one of the issues with this paper. Not one of the issues, but just one of the things that is yet to be resolved is what should be used as negative data is not super clearly defined and is probably actually going to be dependent on the situation. But let's say we want to learn representations about what are actual photos and what are not actual photos, what are like fake photos, then our negative data would be some fake photos. Now, how we make those is not entirely clear. It could just be complete noise, but that probably wouldn't give us too much signal. As we progress, we'll talk a little bit more about the different types of negative data you can use because it comes up in the paper. But for now, just know that we have these two different types of data. And now let's talk about this idea of goodness because this is the other word that comes up. This isn't specifically defined right here. There are multiple ways that this can be defined, but the way they do define it, if we scroll down a little bit, is this equation right here. To explain this, first, let me draw out a little neural network. And in this neural network, we're just going to have an input layer and a hidden layer. Notice we don't have an output layer because right now we're actually just going to be doing an unsupervised form of learning. So we don't really need to have some sort of output that is going to some label, at least not yet. If you are familiar with neural networks, which I hope you somewhat are if you're watching this video, you will probably know that we start with some values here during the forward pass they go through. And then we get some output values, maybe there's like three, a one, and then like a zero. And the measure of goodness that Hinton uses, at least the first measure of goodness that he tries, is just squaring all of these activations up. And these are the values after they're passed through the activation functions and then summing them together. So in this case, the goodness of this input would be three squared plus one squared plus zero squared, which gives us 10. And the goal is to get this 10 to be above some threshold. So let's say that our threshold is two. Well, then Lisa would take 10 and we would subtract two. And that would of course give us eight. Our objective when we're learning is going to be to get this goodness, this 10 right here, above our threshold, the two. Now in this case, that was the case. Our goodness was above the threshold and hence we got a positive number, this eight. So what happens is this eight is then passed through the sigmoid function. This sigmoid function will essentially squash everything to be between zero and one. So this can give us a probability. So an eight, because it's quite positive, well, I don't know the exact number, but it will give us something close to one, which is good. That means this has a high probability of being positive data. I'm not going to go through all this like simple math again for the negative data case, but the case for the negative data is that we want to have a goodness value that is below two, hence we'll get a negative number. So then when we put it through our sigmoid function, we will get a value close to zero, hence meaning that the probability of having positive slash real data is close to zero. So as we just discussed, the objective is to get the goodness of the positive data as high as possible and the goodness of the negative data as low as possible. If we aren't going to use back propagation, though, how do we make this happen? And the answer is actually very simple because we only have a one layer network here. And don't worry, we will get to multiple layers. But for now, because we only have a one layer network here, all we have to do is do gradient descent on one level. And because we don't have multiple levels to this neural network, well, there's no need for back propagation. It's just one layer. So we won't have to be doing all this chain rule, fancy stuff and whatnot. And once we do optimize that objective, we should end up with representations that show some meaningful difference between the positive data and the negative data. So let's move on from there, though. The next step here is we have our one layer network. Obviously, a one layer network isn't going to do much for us. That's pretty weak. So can we do better? How can we add more layers? And of course, we can add more layers here. So here we have our multi layer network. We again have the input at the bottom and the output at the top. So you might be wondering if we're going to have the objective here on the final layer. Well, then won't we just have to do back propagation to get this to go all the way back to the first layer? And the answer is yes, if the objective was just on the first layer. But that is kind of the key here. Instead of having an objective on the final layer of the neural network, the difference here is that we have an objective in every single different layer. So what happens is we start with our input in the input layer as we always do. We come up to the second layer, the second hidden layer. And in this second hidden layer, we calculate the goodness just like we did before. We then use gradient descent to make a step and update these weights. And then we essentially restart as if this second layer is our new input. So we come from the second layer up to the third layer. We calculate the goodness here, update the objective based off the previous layer, and we just keep doing this for each layer. And because each layer has its own objective that is only with respect to the previous layer, we don't have any global updates. There's no going from the very top layer to the bottom layer like you do in back propagation. Everything is local, which is much more plausible if we're trying to get something that is potentially similar to what the brain does. If you are astute, you might have noticed that there is a little issue here. And to identify that issue, let's go through an example. Starting with the first layer, let's say we have some positive data. So we go through this and we get out values like this, maybe 2, 2, and 3. We square all these, what do we get? We get 4 plus 4 plus 9, which gives us 17. And then let's say that we have a threshold of 2. Luckily, 17 is obviously greater than 2, which is great, right? That means that this layer has correctly identified that this is positive data. Now we need the next layer to do the same. The issue comes in here. Imagine now we take this 2 and we throw this forward, and we take this 2 and we throw this forward, and we take this 3 and we throw this forward. Now the issue is that it's going to be very, very easy for the next layer to figure out that this is positive data. And why is that? It's because the previous layer has already done all the work. It's already made this 2, made this 2, and made this 3. So if it doesn't change anything, but just keeps the exact same inputs as the outputs, I guess it can't because it has to squish it down. But the point being is that it just takes the magnitude of the previous layer. If the values have a high magnitude, then it knows the data is positive. And if they have a low magnitude, well then it knows that the input data is negative. So clearly that is not good. How can we fix this? Well, the way they solve this in the paper is very simple, and it's just a bit of normalization. So as they say here, to prevent this issue, V-of-4 normalizes the length of the hidden vector before using it as input to the next layer. Because if they normalize it, that removes all data about the magnitude. So if we want to think about this in sort of simpler terms, we can take what Hinton says here, which is the length is used to define the goodness for that layer, and only the orientation is passed on to the next layer. Let's look at an example. We've done a whole lot of drawing off these networks and stuff, but let's see how it actually works on a simple practice problem. And that simple practice problem for us is going to be MNIST. You're not familiar with MNIST. It's a very simple image data set. You have 28 by 28 is the dimensions of these images, and they're just black and white and they're hand drawn digits. There's kind of two questions we're trying to ask here with this MNIST test. The first one is if we have a good source of negative data, does this method learn effectively a multi-layered representation that captures the structure of the data? And then the second question we need to ask is where does that negative data come from? The where does negative data come from question, it's kind of hard to answer, mainly because there's like a billion ways you could do this. Who knows what the right way is going to be? For the purpose of this experiment, they come up with this sort of hard coded masking way of generating negative data or fake images. And you can see that in action right here. The way they do it is they take two real images. So in this case, it's a six and a seven. And then they have two masks. These masks are just randomly generated and then they combine the images with the masks and they get sort of a hybrid image that is partially both. So you can see the seven part of this is right here and then the six part of it is right here. So you end up with a weird monster number. And the reason they don't just use random data is because what we learn with the forward-forward algorithm is going to be very dependent on our positive and negative data. For negative data is just random data. You know, if it's just like noise, it's going to be very easy for the algorithm to identify the differences. Hence, it won't need to learn a good representation to do that. However, if the hybrid images, these negative images, are very similar to the real positive data, well, then the network is going to have to do a lot more learning about what specifically makes up a real number. It's going to be harder, hence it has to learn more, which is probably going to be more useful for a classification problem. And now with this source of negative data, we can start looking at how effectively this algorithm actually learns. They first learn using the forward-forward algorithm. So let's say we have this trained network right here. Well, then they put a new input in. They get out features from each hidden layer. So they take all the outputs of all the hidden layers, they combine them and they just kind of concatenate them. And then they use these as input to a linear classification problem where they train like you would normally train linear classification. And the idea behind this is that if they just trained with linear classification without extracting these features with a forward-forward algorithm, the linear only model probably wouldn't do as well because, you know, a linear model is quite limiting. And in the papers, they say when they use four hidden layers of 2000 ReLUs for the forward-forward network, over 100 epochs, they get a test error rate of 1.37%, which, you know, is not state of the art. But when you do MNIST, the whole point is definitely not to get state of the art. If you're trying to get state of the art on MNIST, you're probably not going about research the right way. This is more of a test to see if this works, and this certainly looks like it works. And then if they start using local receptive fields, which means that they have neurons that are connected to certain like spatial regions of the image, which can make you learn a bit more efficiently. They train for 60 epochs and they get a 1.16% test error, which means that it improves, which is good. So this is a very simple way of doing this, but already it shows us that the forward-forward algorithm is learning something. And if we want to know a little bit more about what it's learning, I think they have a nice image down here. These are some of the features it's learning. Now you can ignore this part right here because we will come back to this in a second. But you can see that it has some, I mean, I don't really know how to describe these things, but there are some patterns, some shapes that you could see would potentially be helpful, these filters for identifying images, more just to show that it's not learning something random, right? There's clearly some structure being learned here. What we just saw here was a way to do supervised learning by first extracting features and then doing supervised learning later. But what if we want to use the forward-forward algorithm specifically with supervised learning like classification in mind? And if we scroll down, they describe a way to do this. They achieve this with the forward-forward algorithm just by including the label in the input. And that's what we were actually just seeing a second ago. They include the label in the input like this. So if we have a, say a 10, we would have like 10 of these little boxes. Oh wait, they're digits. There's no 10. There's a 9. So 9 is the last digit. So that would just look like this. Or tenth if we're including 0. Pixel would be filled in with black and all the other ones would be white. So just combining the image and the label. And why would they do that? For the positive data, they can just use that as we just said. But for the negative data, they can actually use the incorrect label. So then what the network is going to learn to identify is the difference between images with the right label and images with the wrong label. Because the right labels are trying to maximize goodness and the images with the wrong labels while you're trying to minimize that goodness. Hence, it should learn the difference. But when you do this, they get a very similar test error after 60 epochs. So this way of doing this also works. They talk a little bit more of different ways how you can do this, be a bit more efficient about it and whatnot. Here they get the test error down to 0.64%. But again, the purpose here is not really to maximize your performance on this like very simple benchmark problem. It's more to show this is a working algorithm and there's a few different interesting ways you can approach this problem. So now we get into probably the most interesting part of this algorithm. And that is how they model top-down effects. So what do I mean when I say top-down effects? Let me give you an example that Hinton actually gave in a talk where he was talking about this. I have just drawn out two words. You can probably read them if you speak English. If you don't speak English, I don't know how you've gotten this far, but at and the probably know that this is an A and that this is an H. They are different yet I drew them at least to the best of my ability to be as similar as possible. So why is it that we can recognize these as different even though the low-level features the individual pixels? I mean, we don't see in pixels, right? But the individual like light cones are seeing the same thing. But the reason is that when we expand the context and we look at the entire word, the high-level features have these sort of top-down effects that allow us to classify in this way, give more signal to what we see and unfortunately, we don't have this in the forward-forward algorithm. The reason being is that every layer is well, they're local to their layers, right? So Hinton does propose a way to overcome this and it's quite interesting and it's to treat a static image as a rather boring video that is processed by a multi-layer recurrent neural network. The activity vector at each layer is determined by the normalized activity vectors at both the layer above and the layer below at the previous time step. Let's scroll down a bit because I think that's a bit confusing. But when we actually see an image of this illustrated, hopefully I can explain this so it will make a bit more sense. And just like a normal neural network, we start off with our image. And as we progress, we go through different layers, eventually getting to the label and the one is in the sixth index. Hence, this is a label for six. So it's positive data. The difference here is that you might notice we have multiple sixes because as Hinton mentioned, we're kind of treating this as a video and going from frame to frame, the video just never changes. What's kind of cool is this is kind of how humans are, right? Like we see a constant stream of images and that's kind of what this is modeled after. If we look at the steps of what's happening after we start with our image, we go through to the first layer. We do like the goodness maximization, you know, that thing over here. And then we come up to the next layer. But before we get to this next layer, you might notice there's an arrow here. That is because we can think of this all happening as like a continuous time. We don't need to wait to go through this full first layer, this full first pass before we actually start sending information to the next layer. So as this first layer at the first time step right here is getting information and passing it on, well by the time it gets to the second layer, this first layer is going to be getting new information from the next time step. So this is all really happening in parallel. And this idea is very important here. And the reason why is because we're going to focus on this hidden layer. If you look at this hidden layer, it's getting inputs from three different places. One, it is getting input from the layer below it. Now the layer below it is going to be slightly behind in time, but it's very similar time stop right here, right? So it's getting data from the sec. So this is going to be bottom-up information. It's going to have information about specific features. Maybe like there's an edge right here, or there's a little curve right here, or something like that. That's the idea. However, it's also getting information from this label, which means it's also going to be able to use that to predict what this image is. And that's where the top-down information comes from. What's going to happen is that right now it's only getting information from two layers. But if we come down to like the layer this time step, this next time step, well, this is going to be getting information from the layer above it, right? And it's also going to be getting information from like the sec right here. And the point being is that the information from the labels flows down over time, so that eventually you end up back at the bottom. As time progresses, you kind of have an upward flow of information, and you have the downward flow of information. It's a very neat, but kind of complicated way of thinking about it. That is how you end up getting these top-down and bottom-up connections, which is very cool because it's a way of doing that top-down feedback without ever actually having to do more than local updates, right? Each of these layers only connects one layer up or one layer down, which again means we never have to do back propagation, and we can do top-to-bottom connections, don't have to remember activations, don't have to remember the forward pass. It's kind of complicated looking at it like this, but the concept is remarkably simple, which I think is very neat, and you could see how, you know, a human cortex might do something similar to this. You could really feel the inspiration oozing out of this. Now if we actually start thinking about what the consequences of this top-down and bottom-up information are, it's very neat. There's two examples that Hindon gives of how this could be useful. The first example is in images, and this is kind of the example I gave with V and CAT. The top-down input will be determined by a larger region of the image and will be the result of more stages of processing, so it can be viewed as a contextual prediction, whereas the bottom-up input is more based on local regions of the image. So as those come together, you can get a much better idea of what the image is as a whole. And then as he points out here, because you have these things coming from the top and from the bottom, well, if you're trying to maximize the goodness of positive data, that means that for positive data, the objective is to have good agreement between the input from the layer above and the input from the layer below, and bad agreement for negative data. What that means is that if we come back up here, and we can again focus on this right here. So if we look at this one, we have one image of the data coming from below. So let's say this is positive data, in which case here it is. It's a 6, and it has a label for a 6. So that means that if we want high goodness for this layer, these should both predict the same thing. Because if they predict the same output and they're added together, well, those are going to combine. We're going to get like positive interference if they agree, which is going to mean that this is predicting positive data. However, let's say that instead of having a 6 down here, this is a 7, but the label is a 6. Well, then this data should not agree with this data. We should have negative interference, which will make this value lower, which will decrease the goodness. Hence, we should end up with a low goodness saying that this is negative data. It's kind of cool how you get like these interference patterns based on the differences between the top to down and the bottom to up data. And following this idea, this I thought was really cool, is if we reverse the sign of the objective function, so instead we're trying now to make the goodness be low for positive data and vice versa for negative data, well, then what should actually happen? Before, instead of trying to aim for positive interference, now the top-down input should learn to cancel out the bottom-up input on positive data, which is very much like predictive coding, which is a really cool idea in neuroscience, which is where you have like a little neuron here, right? So you got your little neuron, has its axon, so, or snap, and it has its dendrites where it gets information from. And I think the way predictive coding works is that when an input comes into these dendrites like this, and the neuron gets some input, it actually has a little thing right here that predicts what this input's going to be. And if it's wrong, I guess it uses that to update the neuron. I forget exactly how it works. But the idea being that neurons are constantly predicting what their inputs are going to be and learning based off that. And in this setup, we see something very similar. So predictive coding is cool, seeing it sort of materialize an algorithm like this, very neat. So far, this has only been used for MNIST, and MNIST is obviously very simple. We can move the difficulty up a little bit more if we go to CIFAR-10. So CIFAR-10 has, again, it's like 50,000 training examples, but the images are a little bit bigger, and more importantly, these images have more diversity. They have three color dimensions, so they're colored, not just black and white. And they're also, I believe, they're like most animals, planes. I forget the exact classes, but they're more complicated than digits. So this is typically thought of as the next step up from MNIST. And if we look at the comparison between backpropagation, which is right here, and then the feedforward, which is all of this, we can see, well, for one, backpropagation gets a zero training error rate, so it overfits very heavily on the training, but the test error rate is still 37%. Feedforward does not quite get as good of an error rate as backpropagation. It does a little bit worse, but it's not that much worse. So here, what is it? I guess here it's four percentage points off, and then here it's two percentage points off. So yes, not quite as good as backprop in these cases, but again, this is like the very first iteration of this algorithm, and it has many other benefits. So overall, I would say it's certainly showing that it is useful. This next experiment shows that you can also use the feedforward algorithm to do language modeling. For this task, the positive data consists of strings of 10 characters from a real text corpus, and the negative data also uses strings of 10 characters, but the last characters are replaced with a different character that is predicted by the model. And this serves as an example of how you can have a model generate its own negative data based on its own incorrect predictions. So we have now gone over how this method works, and we can see these different examples of how it could be useful. So now we can start asking some interesting questions about this method. And the first question that comes up here is when we should be doing positive updates versus when we should be doing negative updates. Up to this point, I believe all the positive updates and the negative updates were done in the same batches. Either that or something similar, where it's like positive data, negative data, and so on. And what we're looking at in this graph is three curves. The first one, the synchronous phase right here, and this is training on this next character task we just looked at, and as we train more, the perplexity goes down. That is good. So this black line is the normal one. And then above that, we have this red line. And this red line is what happens when we separate the positive and the negative training phases. Instead of doing it all together, we say let's do a bunch of positive and then a bunch of negative data. And I think in the talk, Hinton said something like doing 90,000 positive samples and 90,000 negative samples. And if you do make this too big, he mentions that this does collapse. But it is very interesting that you don't have to do them together, that they can be separated somewhat. If you want to go find Hinton's talk about this on, I think, the NERP site, it's actually very interesting. This is where he starts talking about how this is kind of his current theory for sleep. The general idea that he proposes is that during the day, we're learning from positive data, our actual experiences, learning new things, learning how to remember things that are going to be useful. And then when we sleep, what we do is we have all these things playing in our head that are not real, or in other words, the negative data. And that's why we end up not remembering dreams, right? Why would we want to remember negative data? The whole point is kind of to forget the stuff that is not real, except for this stuff, right? Right when we wake up, we have some memories in our short-term memory. But aside from that, we don't remember much of our dreams. I'm very much simplifying this, but I would definitely recommend checking out the talk. It was really funny. This is where he casually starts talking about how sleep deprivation is used as a torture method in some countries. Just, you know, very casually mentioning this and just talking about how if animals don't get enough sleep, they'll just drop dead and humans will go crazy. And what he points out is that these graphs would align with that sort of behavior. Although again, this is like very early-stage theory, you know, take it with a grain of salt. But it's interesting to see the parallels there. The next stage of this paper is moving on to talk about how feedforward relates to a lot of other ideas in the field of AI. The first one he talks about is the Boltzmann machine. These were a competitor to BACPROP back in the day that was originally proposed by Hinton, but they were eventually scrapped in favor of BACPROP. Hinton essentially just talks about how the feedforward algorithm pulls from this sort of contrastive learning that Boltzmann machines were doing, and it does that by doing the simple goodness function, which is more tractable than what Boltzmann machines were doing at the time, essentially just saying that this is somewhat building on those ideas, but taking away the bad stuff and putting in some new stuff instead. He also talks about how these are related to GANs. It's essentially just saying that the feedforward algorithm is a very specific version of a GAN. Not too much there though. He also talks about how this is related to contrastive methods. This is very much related to contrastive methods in that I would call this, I would call the forward forward algorithm itself a contrastive method. Though he does talk about how this method could be a bit better in the sense that it's a bit more flexible than some of the other contrastive learning methods. The next idea is, I think, a bit more interesting, and that is this idea of learning fast and slow. I'm going to be simplifying this idea quite a bit, but what this first part talks about is how if you do a weight update on a single example, the optimization of this goodness function is actually not going to have any effect on the layer normalized output, again, when you're using that one single sample. What this means is that if we take a sample, we run it through our whole network, we can actually go to each layer and perfectly optimize them so that the goodness is as high as we need it to be, so that it perfectly achieves right above that threshold. So now if we take this one example and learn perfectly for every single layer to this perfect example, yes, we're going to be learning very fast, but of course that is going to come at the consequence of overfitting incredibly heavily. So it's not that useful, you might think, but perhaps it would be interesting if we combine it with this second idea. And the second idea comes from one of the strengths of the feedforward algorithm, that strength that we can use black boxes in the forward pass because, again, we don't need to know the internal mechanism, so we can just throw anything in there and this will work. So imagine if these black boxes were neural networks. So we take neural networks with a few hidden layers and in between all of our hidden layers, we throw one of these in. Well, one thing you could do then is the outer loop of the feedforward algorithm, which again is the maximizing goodness using the local inputs. Well, that can adapt very quickly, as we just talked about. However, learning within these black boxes in these neural networks can happen at a much slower pace using something like reinforcement learning to improve the system. And this would be a much slower process, but perhaps it would be able to get you to a much higher performance ceiling. It's kind of idea, I'm not sure how realistic this is, if this would actually work very well, but certainly the idea of having a fast outer learning loop and a slower but more precise inner learning loop is interesting, especially because in machine learning right now, sometimes it can take a network a while to start learning in the beginning anything super useful because, you know, we want to start out with a somewhat small learning rate so that it doesn't overfit to early data. This might help alleviate some of that, kind of get the best of both worlds. I don't know, interesting idea for you to think about. And finally, we get down to the hardware stuff, which I think is the most exciting potential benefit of this kind of work. So this just starts by talking about how we could have an analog hardware system. So an energy efficient way to multiply an activity vector by a weight matrix is to implement activities as voltages and weights as conductances. Matrix multiplication is basically all that's happening on the hood of these giant neural networks. So if we can make this more energy efficient, you could imagine that would be huge, especially if we could do it by a substantial margin. If we are instead going to use digital computers, which is what we're doing right now, well, everything needs to be represented as individual bits. And if we want to represent things as individual bits, then we need O of N to the two operations if we want to multiply two in bit numbers together. Now, that's not ideal, right? Where if we have an analog system, something like that should in theory be a lot simpler. I'm not an electrical engineer, so it's hard for me to say for certain, but I'll take Hinton's word for it. Sounds reasonable to me. Now, if using current hardware, you want to do something using the analog method, you have to use a digital to analog converter and vice versa so that you can convert the digital signal to an analog one, then convert the analog signal back to a digital signal, I believe is how that would work. The issue is that these analog to digital and vice versa converters apparently take a lot of power. So if you could just do this all in the analog setting, which is something that you could do with two forward passes because the backward pass is the part that requires the conversion, then we should be able to cut out a lot of the energy use. So if you were like me, you might be thinking, oh, great, well, we can then just make these chips to be completely analog that do all these computations or stuff. But remember, you still need to at the end of the day, send this data back to a computer and the computer is always going to be digital. As Hinton points out here, general purpose digital computers were designed to faithfully followed instructions because it was assumed that this would be the only way to get a general purpose computer to perform a specific task or so on. And the reason computers can be so specific and do so many different things and that we can run software on top of them that will do exactly what we're telling them to do is because they are digital. Once we go analog, I think we lose lots of those properties. Again, this is where I started getting a bit out of my depth. But the point being is that we're probably not going to be going to analog computers anytime soon. I'm not even sure if that would be a good idea in the first place. So if we can't go to analog computers and we need these analog to digital converters, what if we just abandon the computer entirely? And this is where we get into the whole radical part of what I was talking about. And no, I don't mean abandoning computers as in we should get rid of computers. I mean, what if we develop AI that doesn't require a computer to run? Instead of having this idea where software should be separable from the hardware, what if we change that so that the weights themselves of our neural networks run on the physical hardware? They're not coded in some software that makes this much more inefficient because that's another inefficiency, right? We have to have the software that runs on top of the hardware layer. What if we have this encoded directly into the hardware? Now, you might be thinking, well, that's certainly not ideal for a number of reasons. For one, that means if you want to write an AI program, you have to have like a chip designing thing that you can use. Yeah, that's certainly not ideal. And it also means that we can't just, you know, copy something from one hardware to the other as easily. It means that the knowledge on the chip will actually die when the hardware dies. We can't just flash it onto a USB and transfer it to a bunch of other ones. So yes, there are a lot of downsides here. But, and I will quote this part, if however we are willing to abandon the immortality, it should be possible to achieve huge savings in the energy required to perform a computation and the cost of fabricating the hardware that executes the computation. Essentially this idea that has been super, super helpful in the past and is still super helpful for many things. And it's certainly not going anywhere. The idea of hardware and software separation is perhaps holding us back in the field of AI. And perhaps the reason not many people have been working on this is because something like backpropagation just doesn't work super well on an analog chip. It would be incredibly hard to implement. And it's because the current networks that we use, they have all sorts of large and unknown variations in their connectivity, their nonlinearities. There's all these differences. And when you want to do backpropagation, you have to have these different derivatives in this backpropagation. Everything gets very complicated. And that's where the forward-forward algorithm comes in. It says, look, we can do this whole process much more simply with just two forward passes. We don't have to retain any of this information. We can do this in a much cruder way, in an analog way, for all sorts of connectivity and nonlinearities. And because of that, this is the type of algorithm that would allow us to make that move to hardware chips that are much more efficient. Now, I will be honest with you. I have no clue if the forward-forward algorithm will be the algorithm that allows us to finally make that jump or if it will be the best one for making that jump. I don't know. But the point being is that there will, I'm almost certain, come a day where there is some algorithm like the forward-forward algorithm or maybe this one itself after a few iterations that allows us to be possible. And once we fully embrace it and make that jump, we will be able to create systems that are much, much more efficient than anything we can produce now. And that still means that you can create AI on a computer and you can simulate it. It just won't be as fast as if you were doing it on a chip. It's funny, and HiddensTocky even gives this example of running something like GPT-4 on your toaster because instead of having to, you know, have an entire computer inside your toaster, you just have one chip that does all your computations. These chips would have to be general purpose. We would have chips that could learn anything. You give it some input and then you give it some output and it can just learn from that. It would have to learn continually. It would have to be able to learn on the fly. It would have to be able to deal with all sorts of different issues. Each hardware chip would essentially become a unit of intelligence. It's really neat to think about the ideas that could come from this. For example, stacking chips together and having one massive chip that works the same way a human brain does where you have sort of all these different components that are working together. Or as Sandin points out here, the function itself could be transferred, and this is the function that is learned on the chip, could be transferred to another chip with a different piece of hardware just by using distillation. You could instead of, you know, we wouldn't be able to copy software, but you could copy one chip to another by distilling its weights kind of like we do in software right now. The possibilities here and the interesting things to think about are really endless. And working towards an algorithm like that, I think is an incredibly exciting endeavor. And I'm very happy that one of the big names in the field is looking into this very seriously. I think I'm blabbering on now, so I want to finish this off by quoting the last sentence of this paper. And it's, if you want your trillion parameter neural network to only consume a few watts, mortal computation may be the only option. Its feasibility rests on findings of a learning procedure that can run efficiently and hardware whose precise details are unknown. And the forward-forward algorithm is a promising candidate, though it remains to be seen how well it scales to large neural networks. If you've enjoyed this, consider subscribing to the channel to get more like this. Thank you so much for watching, and I hope to catch you next time.",
        "output": "### Excitement for Forward-Forward Algorithm\n\nThis paper is exciting as it proposes a new alternative to backpropagation with numerous applications. The forward-forward algorithm may have significant implications for future AI, allowing for standalone silicon chips that learn without anything else required.\n\n### Motivation\n\nThe motivation behind the forward-forward algorithm is to address the limitations of backpropagation in simulating how the brain learns. Backpropagation cannot learn on the fly and perform forward pass computations simultaneously, unlike a brain.\n\n### Methodology and Implementation\n\nThe forward-forward algorithm performs two forward passes with different data and objectives. It aims to learn a representation in an unsupervised way. The algorithm calculates a goodness value, which is maximizing the goodness for real data and minimizing it for negative data.\n\n### Standalone Chip Learning\n\nThe forward-forward algorithm could provide a foundation for creating low-power analog hardware that learns without any additional dependencies.\n\n### Unsupervised Learning\n\nUnsupervised learning can be performed using the forward-forward algorithm to extract meaningful features in different layers of a neural network through the concept of goodness.\n\n### Supervised Learning\n\nThe forward-forward algorithm can be used for supervised learning, e.g., image classification, by incorporating the label in the input and identifying the differences between images with the right label and images with the wrong label.\n\n### Modeling Top-Down Effects\n\nThe forward-forward algorithm can model top-down effects, which involves using higher-level context to influence lower-level perception and learning tasks."
    },
    {
        "instruction": "<|SYSTEM|>You are a scholars that is taking markdown notes about some video, you would be giving some written scripts for the task.\nYou have finished writing the notes, and you want to extract the title from the notes.\nThe title should only be the name of the technology or algorithm, no extra words needed.\nPlease write the notes in markdown format. No need to write \"Title:\" for the Title. Make sure the title is within 10 words.\nHere is an example:\n## Hidden Markov Model\n<|ASSISTANT|>## ",
        "input": "<|USER|>### Excitement for Forward-Forward Algorithm\n\nThis paper is exciting as it proposes a new alternative to backpropagation with numerous applications. The forward-forward algorithm may have significant implications for future AI, allowing for standalone silicon chips that learn without anything else required.\n\n### Motivation\n\nThe motivation behind the forward-forward algorithm is to address the limitations of backpropagation in simulating how the brain learns. Backpropagation cannot learn on the fly and perform forward pass computations simultaneously, unlike a brain.\n\n### Methodology and Implementation\n\nThe forward-forward algorithm performs two forward passes with different data and objectives. It aims to learn a representation in an unsupervised way. The algorithm calculates a goodness value, which is maximizing the goodness for real data and minimizing it for negative data.\n\n### Standalone Chip Learning\n\nThe forward-forward algorithm could provide a foundation for creating low-power analog hardware that learns without any additional dependencies.\n\n### Unsupervised Learning\n\nUnsupervised learning can be performed using the forward-forward algorithm to extract meaningful features in different layers of a neural network through the concept of goodness.\n\n### Supervised Learning\n\nThe forward-forward algorithm can be used for supervised learning, e.g., image classification, by incorporating the label in the input and identifying the differences between images with the right label and images with the wrong label.\n\n### Modeling Top-Down Effects\n\nThe forward-forward algorithm can model top-down effects, which involves using higher-level context to influence lower-level perception and learning tasks.",
        "output": "## Forward-Forward Algorithm"
    },
    {
        "instruction": "<|SYSTEM|>You are a scholars that is taking markdown notes about some video with Wiki style, you would be giving some written scripts for the task.\nAnd summarize each paragraph with a title.\nRemember to keep the technology or algorithm name, very important.\nPlease write the notes in markdown format.\nHere is an example:\n### Definiation\nA deadlock is a situation in which two computer programs sharing the same resource are effectively preventing each other from accessing the resource, resulting in both programs ceasing to function. \nThe earliest computer operating systems ran only one program at a time.\n\n### What it have to do with Relationsal Database ManagementSystem\nIn a database, a deadlock is an unwanted situation in which two or more transactions are waiting indefinitely for one another to give up locks. \nDeadlock is said to be one of the most feared complications in DBMS as it brings the whole system to a Halt. \nWhen you tries to insert or update data on database, sometime your record need to depend on the row record.  \n<|ASSISTANT|>#### ",
        "input": "<|USER|> In today's video, we'll explore the top seven distributed system patterns with real-world examples that can help us design more efficient and scalable systems. So let's dive right in. First is Ambassador. Picture yourself as a busy CEO with a personal assistant who handles all your appointments and communication. That's precisely what the Ambassador pattern does for our application. It acts as a go-between for our app and the services it communicates with, offloading tasks like logging, monitoring, or handling retries. For instance, Kubernetes uses Envoy as an ambassador to simplify communication between services. The Ambassador pattern can help reduce latency, enhance security, and improve the overall architecture of your distributed systems. Next is Circuit Breaker. Imagine a water pipe bursting in your house. The first thing you would do is shut off the main valve to prevent further damage. The Circuit Breaker pattern works similarly, preventing cascading failures in distributed systems. When a service becomes unavailable, the Circuit Breaker stops requests, allowing it to recover. Netflix's Hystric library uses this pattern. It ensures a more resilient system. This pattern can be particularly useful when dealing with microservices or cloud-based applications where failures are more likely to occur. The third pattern is CQRS or Command Query Responsibility Segregation. CQRS is like having a restaurant with separate lines for ordering food and picking up orders. By separating the command or write operations from the query or read operations, we can scale and optimize each independently. An e-commerce platform might have high read requests for product listings, but fewer write requests for placing orders. CQRS allows each operation to be handled efficiently. This pattern becomes especially valuable in systems where read and write operations have different performance characteristics with different latency or resource requirements. Next is Event Sourcing. Think of event sourcing as keeping a journal of the live events. Instead of updating a record directly, we store events representing changes. This approach provides a complete history of the system and enables better auditing and debugging. Event version control is a great example of event sourcing where each commit represents a change. With event sourcing, we can also implement advanced features like time travel debugging or replay events for analytic purposes. Number five is Leader Election. Imagine a classroom of students electing a class representative. In a distributed system, the leader election pattern ensures only one note is responsible for a specific task or resource. When a leader note fails, the remaining notes elect a new leader. Zookeeper and SED use this pattern to manage distributed configurations. By having a designated leader, we can avoid conflicts and ensure consistent decision making across the distributed system. Next is PubSub. The publisher subscriber pattern is like a newspaper delivery service. Leaders emit events without knowing who will receive them, and subscribers listen for events they're interested in. This pattern allows for better scalability and modularity. For example, Google Cloud PubSub enables asynchronous messaging between services, making it easier to maintain and scale complex applications. PubSub systems are well suited for scenarios where we need to propagate changes or updates across multiple components, for example, updating a user's profile across various services. Number seven is Sharding. Sharding is like dividing a large pizza into smaller slices, making it easier to handle. It's a technique for distributing data across multiple notes in a system. It improves performance and scalability. This shard contains a subset of the data, reducing the load on any single note. Databases like MongoDB and Cassandra use sharding to handle large amounts of data efficiently. Sharding can also help us achieve better data locality, reducing network latency, and speeding up query execution. Now here's a bonus pattern, the strangler fake pattern. This pattern is inspired by the strangler fake tree, which grows around other trees and eventually replaces them. In software, the strangler fake pattern is a method for gradually replacing legacy systems with new implementations. Instead of performing a risky big bang migration, we can incrementally replace parts of this old system with new components. This approach can help us manage the risks and complexities associated with system migrations. And that's it on this topic. These top seven distributed system patterns, along with their real-world examples, can help us build more robust and scalable applications. Remember, understanding our specific system needs and applying suitable patterns is crucial. Let's leverage these patterns in our design and create better distributed systems. If you like our videos, you may like our system design newsletter as well. It covers topics and trends in large-scale system design. Trusted by 300,000 readers, subscribe at blog.buybuygold.com.",
        "output": "### Top 7 Distributed System Patterns\n\n#### 1. Ambassador\nThe Ambassador pattern acts as a go-between for your application and the services it communicates with, offloading tasks like logging, monitoring, or handling retries. Example: Kubernetes uses Envoy as an ambassador.\n\n#### 2. Circuit Breaker\nThe Circuit Breaker pattern prevents cascading failures in distributed systems by stopping requests to an unavailable service. Example: Netflix's Hystric library.\n\n#### 3. CQRS (Command Query Responsibility Segregation)\nCQRS separates command (write) operations from query (read) operations, allowing for independent scaling and optimization. Example: E-commerce platforms.\n\n#### 4. Event Sourcing\nEvent Sourcing stores events representing changes, providing a complete history of the system for better auditing and debugging. Example: Git version control.\n\n#### 5. Leader Election\nThe Leader Election pattern ensures that only one node is responsible for a specific task or resource. Example: Zookeeper and etcd for distributed configurations.\n\n#### 6. PubSub (Publisher Subscriber)\nThe PubSub pattern allows for better scalability and modularity by emitting events without knowledge of their recipients. Example: Google Cloud PubSub.\n\n#### 7. Sharding\nSharding is a technique for distributing data across multiple nodes in a system, improving performance and scalability. Example: MongoDB and Cassandra for efficient data handling.\n\n#### Bonus: Strangler Fig Pattern\nThe Strangler Fig Pattern is a method for gradually replacing legacy systems with new implementations. It helps manage risks and complexities associated with system migrations."
    },
    {
        "instruction": "<|SYSTEM|>You are a scholars that is taking markdown notes about some video, you would be giving some written scripts for the task.\nYou have finished writing the notes, and you want to extract the title from the notes.\nThe title should only be the name of the technology or algorithm, no extra words needed.\nPlease write the notes in markdown format. No need to write \"Title:\" for the Title. Make sure the title is within 10 words.\nHere is an example:\n## Hidden Markov Model\n<|ASSISTANT|>## ",
        "input": "<|USER|>### Top 7 Distributed System Patterns\n\n#### 1. Ambassador\nThe Ambassador pattern acts as a go-between for your application and the services it communicates with, offloading tasks like logging, monitoring, or handling retries. Example: Kubernetes uses Envoy as an ambassador.\n\n#### 2. Circuit Breaker\nThe Circuit Breaker pattern prevents cascading failures in distributed systems by stopping requests to an unavailable service. Example: Netflix's Hystric library.\n\n#### 3. CQRS (Command Query Responsibility Segregation)\nCQRS separates command (write) operations from query (read) operations, allowing for independent scaling and optimization. Example: E-commerce platforms.\n\n#### 4. Event Sourcing\nEvent Sourcing stores events representing changes, providing a complete history of the system for better auditing and debugging. Example: Git version control.\n\n#### 5. Leader Election\nThe Leader Election pattern ensures that only one node is responsible for a specific task or resource. Example: Zookeeper and etcd for distributed configurations.\n\n#### 6. PubSub (Publisher Subscriber)\nThe PubSub pattern allows for better scalability and modularity by emitting events without knowledge of their recipients. Example: Google Cloud PubSub.\n\n#### 7. Sharding\nSharding is a technique for distributing data across multiple nodes in a system, improving performance and scalability. Example: MongoDB and Cassandra for efficient data handling.\n\n#### Bonus: Strangler Fig Pattern\nThe Strangler Fig Pattern is a method for gradually replacing legacy systems with new implementations. It helps manage risks and complexities associated with system migrations.",
        "output": "Distributed System Patterns"
    },
    {
        "instruction": "<|SYSTEM|>You are a scholars that is taking markdown notes about some video with Wiki style, you would be giving some written scripts for the task.\nAnd summarize each paragraph with a title.\nRemember to keep the technology or algorithm name, very important.\nPlease write the notes in markdown format.\nHere is an example:\n### Definiation\nA deadlock is a situation in which two computer programs sharing the same resource are effectively preventing each other from accessing the resource, resulting in both programs ceasing to function. \nThe earliest computer operating systems ran only one program at a time.\n\n### What it have to do with Relationsal Database ManagementSystem\nIn a database, a deadlock is an unwanted situation in which two or more transactions are waiting indefinitely for one another to give up locks. \nDeadlock is said to be one of the most feared complications in DBMS as it brings the whole system to a Halt. \nWhen you tries to insert or update data on database, sometime your record need to depend on the row record.  \n<|ASSISTANT|>#### ",
        "input": "<|USER|> To understand what Dijkstra's algorithm is and how it works, let's start with a scenario. Let's imagine that we have a variety of different towns scattered in different locations. Some of those towns are connected to other towns with roads, such that we could travel along a road to go from one town to another. Not all roads have the same travel time, though. One road might take three minutes to travel through, while another might take four. And our question is this. What's the fastest way to get from town A to town B? This is a specific example of a more general problem in graph theory, where we have vertices – in this case, our towns – that are connected by edges – in this case, our roads. Together, the vertices and edges form a graph. And in particular, this graph is a weighted graph, because each edge has some value, or weight, to indicate how costly it is to travel along that edge, measured in either distance or time. And with this graph, we'd like to determine the shortest path from one vertex to another. So how does Dijkstra's algorithm allow us to do this? Ultimately, we need to figure out how many minutes it will take to get to town B. In doing so, though, it might be useful to figure out how many minutes it would take to get to the other towns, too. So ideally, we'd want to label each town with a number, representing the shortest time required to get there. But of course, we don't know that information yet. So instead, we'll label each town with the shortest time we've found so far to get to that town. Before we've started the algorithm, we don't have any known way of getting to the towns, so we'll label them all with infinity. The only town that we can label correctly is our source, town A, which we can easily label. We're starting there, so it takes zero minutes to get to this town. Once we've found the shortest path to a town, let's call that town explored. For all other towns, we'll consider them unexplored. We don't yet know what the shortest path to them is. So what can we do next? Dijkstra's algorithm repeats a two-step process. First, we update our estimates, and second, we choose the next vertex to explore. So let's start with the first step, updating our estimates. We know we can get to town A in zero minutes, and because we can get from town A to town C in three minutes, we know we can get to town C in three minutes. That's much better than our current estimate of infinity, so we'll update our estimate to be three. And we can do something similar for town F. We can get there in two minutes, so we'll update our estimate. After we've considered all possible roads connected to town A, we're done with this step. And now the next step is choosing which town to explore next. And the rule is quite simple. The town we explore will always be the unexplored town with the smallest estimate. In other words, the next town we explore should be among all of the unexplored towns, the town we know we can get to the quickest. In this case, that's town F, which we can get to in two minutes. And now the two-part process of Dijkstra's algorithm repeats. We first update our estimates by looking at the towns connected to town F. It takes five minutes to get from town F to town G, so if we can get to town F in two minutes, then we can get to town G in seven minutes, the sum of those two values. Likewise, we can get to town B in eight minutes, and we can get to town E in five minutes. We can also get to town C in four minutes, two minutes to go from A to F, and then another two to go from F to C. But four minutes is actually more than our current best estimate for town C, which is three minutes. So we don't want to update anything here. Remember, for each town, we want to store our best estimate for the shortest amount of time required to get there. So let's keep going. The second part of Dijkstra's algorithm tells us to explore the unexplored town with the smallest value. And in this case, that's town C, which we can get to in just three minutes. And the process repeats again. Going through town C, we can get to town D in seven minutes, and we can also get to town E in four minutes, three minutes to town C, plus one minute from C to E. That four-minute estimate is better than our current estimate of five minutes, so we'll update our estimate for town E from five to four. The next town to explore is town E, among all the unexplored towns, it has the smallest value. Notice that each time we explore a new town, it's guaranteed that we will have found the shortest path to get to that town. Since if there were some shorter path, we would have explored that shorter path first because we prioritize exploring these smaller values. And through town E, we can get to town B in just six minutes, four minutes to town E, plus two minutes to go from E to B. So we update our estimate for town B to be six, which is better than the previous estimate of eight. And now the next town to visit is town B, our destination. And that tells us that the shortest time needed to get from town A to town B is therefore six minutes. And that's Dijkstra's algorithm for finding the shortest path in a graph. Now there are a few things worth pointing out here. The first is that technically, our algorithm has only given us how long it will take to get from town A to town B, and hasn't actually given us which path to follow. In order to actually reconstruct that path, we'll need to add a step to our algorithm. Every time we update our estimate for a town, we should also keep track of what previous town we visited in order to get to this new estimate. If we keep track of that for each of the towns as we progress through the algorithm, then at the end, we can figure out what the best path is just by following those arrows backwards to the beginning. Also, it's worth noting that this algorithm only works correctly if the weights of all of the edges are non-negative, since otherwise, some negative length path might result in a shortest path that our algorithm wouldn't be able to find. Another thing worth thinking about is how we efficiently determine which unexplored town to visit next. We want to always pick the unexplored town with the current smallest estimate, so some kind of priority queue that gives us efficient access to the next town to visit can make our algorithm faster. And that's Dijkstra's algorithm for finding the shortest path in a graph. We keep track of a distance and a previous town for each vertex. The distance for all vertices starts at infinity, except for the source, which starts at zero. And then, as long as we haven't explored the destination yet, we pick the unexplored vertex with the smallest distance to explore next. We consider all edges connected to that vertex. And if we can come up with a shorter path to a neighboring vertex, then we'll update our distance estimate and remember which town we came from. Once we've explored the destination, we have our answer. We'll know how quickly we can get to the destination, as well as what the shortest path actually is.",
        "output": "### Dijkstra's Algorithm Introduction\n\nDijkstra's algorithm is a graph theory solution for finding the shortest path between two vertices in a weighted graph. It is commonly used to determine the quickest route from one location to another, based on factors such as distance or time.\n\n### Dijkstra's Algorithm Process\n\n1. Initialize all vertex distances to infinity, except the source vertex which has a distance of zero.\n2. Mark all vertices as unexplored.\n3. Repeat the following two steps:\n   - Update the distance estimates for unexplored neighboring vertices.\n   - Choose the unexplored vertex with the smallest distance to explore next.\n4. Continue the repetition until the destination vertex is explored.\n5. Find the shortest path by tracing back the previous vertices from the destination vertex.\n\n### Important Notes\n\n- The algorithm's output includes both the shortest distance and the path itself.\n- Only works correctly with non-negative edge weights.\n- Utilize a priority queue to efficiently choose the next unexplored vertex.\n- To reconstruct the path, track the previous vertex visited for each updated distance estimate."
    },
    {
        "instruction": "<|SYSTEM|>You are a scholars that is taking markdown notes about some video, you would be giving some written scripts for the task.\nYou have finished writing the notes, and you want to extract the title from the notes.\nThe title should only be the name of the technology or algorithm, no extra words needed.\nPlease write the notes in markdown format. No need to write \"Title:\" for the Title. Make sure the title is within 10 words.\nHere is an example:\n## Hidden Markov Model\n<|ASSISTANT|>## ",
        "input": "<|USER|>### Dijkstra's Algorithm Introduction\n\nDijkstra's algorithm is a graph theory solution for finding the shortest path between two vertices in a weighted graph. It is commonly used to determine the quickest route from one location to another, based on factors such as distance or time.\n\n### Dijkstra's Algorithm Process\n\n1. Initialize all vertex distances to infinity, except the source vertex which has a distance of zero.\n2. Mark all vertices as unexplored.\n3. Repeat the following two steps:\n   - Update the distance estimates for unexplored neighboring vertices.\n   - Choose the unexplored vertex with the smallest distance to explore next.\n4. Continue the repetition until the destination vertex is explored.\n5. Find the shortest path by tracing back the previous vertices from the destination vertex.\n\n### Important Notes\n\n- The algorithm's output includes both the shortest distance and the path itself.\n- Only works correctly with non-negative edge weights.\n- Utilize a priority queue to efficiently choose the next unexplored vertex.\n- To reconstruct the path, track the previous vertex visited for each updated distance estimate.",
        "output": "## Dijkstra's Algorithm"
    },
    {
        "instruction": "<|SYSTEM|>You are a scholars that is taking markdown notes about some video with Wiki style, you would be giving some written scripts for the task.\nAnd summarize each paragraph with a title.\nRemember to keep the technology or algorithm name, very important.\nPlease write the notes in markdown format.\nHere is an example:\n### Definiation\nA deadlock is a situation in which two computer programs sharing the same resource are effectively preventing each other from accessing the resource, resulting in both programs ceasing to function. \nThe earliest computer operating systems ran only one program at a time.\n\n### What it have to do with Relationsal Database ManagementSystem\nIn a database, a deadlock is an unwanted situation in which two or more transactions are waiting indefinitely for one another to give up locks. \nDeadlock is said to be one of the most feared complications in DBMS as it brings the whole system to a Halt. \nWhen you tries to insert or update data on database, sometime your record need to depend on the row record.  \n<|ASSISTANT|>#### ",
        "input": "<|USER|> What do all recent super powerful image models like Dolly, Imogen or Midjourney have in common? Other than high computing cost, huge training time and shared hype, they are all based on the same mechanism, diffusion. Diffusion models recently achieved state of the art results for most image tasks including text to image with Dolly, but many other image generation related tasks like image in painting, style transfer or image super resolution. Also there are a few downsides, they work sequentially on the whole image, meaning that both the training and inference times are super expensive. This is why you need hundreds of GPUs to train such a model and why you wait a few minutes to get your results. It's no surprise that only the biggest companies like Google or OpenAI are releasing those models. But what are they? I've covered diffusion models in a couple of videos which I invite you to check for a better understanding. They are iterative models that take random noise as inputs which can be conditioned with a text or an image so it's not completely random. It iteratively learns to remove this noise by learning what parameters the models should apply to this noise to end up with a final image. So the basic diffusion models will take a random noise with the size of the image and learn to apply even further noise until we get back to a real image. This is possible because the model will have access to the real images during training and will be able to learn the right parameters by applying such noise to the image iteratively until it reaches complete noise and is unrecognizable. Then, when we are satisfied with the noise we get from all our images, meaning that they are similar and generate noise from a similar distribution, we are ready to use our model and feed it similar noise in the reverse order to expect an image similar to the ones used during training. So the main problem here is that you are working directly with the pixels and large data input like images. Let's see how we can fix this computation issue while keeping the quality of the results the same as shown here compared with Delhi. But first, give me a few seconds to introduce you to my friends at Quack sponsoring this video. As you most certainly know, the majority of businesses now report AI and ML adoption in their processes, but complex operations such as model deployment, training, testing and feature store management seem to stand in the way of progress. ML model deployment is one of the most complex processes. It is such a rigorous process that data scientist teams spend way too much time on solving backend and engineering tasks before being able to push the model into production, something I personally experienced. It also requires very different skill sets, often requiring two different teams working closely together. Fortunately for us, Quack delivers a fully managed platform that unifies ML engineering and data operations, providing agile infrastructure that enables the continuous productization of ML models at scale. You don't have to learn how to do everything end-to-end anymore thanks to them. Quack empowers organizations to deliver machine learning models into production at scale. If you want to speed up your model delivery to production, please take a few minutes and click the first link below to check what they offer, as I'm sure it will be worthwhile. Thanks to anyone taking a look and supporting me and my friends at Quack. How can these powerful diffusion models be computationally efficient? By transforming them into latent diffusion models. This means that Robin Rombach and his colleagues implemented this diffusion approach we just discovered, within a compressed image representation, instead of the image itself, and then worked to reconstruct the image. So they are not working with the pixel space or regular images anymore. Working in such a compressed space does not only allow for more efficient and faster generations, as the data size is much smaller, but also allows for working with different modalities. Since they are encoding the inputs, you can feed it any kind of input like images or text, and the model will learn to encode these inputs in the same subspace that the diffusion model will use to generate an image. So yes, just like the clip model, one model will work with text or images to guide generations. The overall model will look like this. You will have your initial image, shear x, and encode it into an information dense space called the latent space, or z. This is very similar to a GAN where you will use an encoder model to take the image and extract the most relevant information about it in a subspace, which you can see as a downsampling task, reducing its size while keeping as much information as possible. You are now in the latent space with your condensed input. You then do the same thing with your conditioned inputs, either text, images, or anything else, and merge them with your current image representation using attention, which I described in another video. This attention mechanism will learn the best way to combine the input and conditioning inputs in this latent space, adding attention, a transformer feature, to diffusion models. These merged inputs are now your initial noise for the diffusion process. Then you have the same diffusion model I covered in my Imagen video, but still in this subspace. Finally, you reconstruct the image using a decoder, which you can see as the reverse step of your initial encoder, taking this modified and denoised input in the latent space to construct a final high-resolution image, basically upsampling your results. And voila! This is how you can use diffusion models for a wide variety of tasks like super resolution, inpainting, and even text-to-image with the recent stable diffusion open-sourced model through the conditioning process, while being much more efficient and allowing you to run them on your GPUs instead of requiring hundreds of them. You heard that right. For all devs out there wanting to have their own text-to-image and image synthesis model running on their own GPUs, the code is available with pre-trained models. All the links are below. If you do use the model, please share your tests, IDs and results or any feedback you have with me. I'd love to chat about that. Of course, this was just an overview of the latent diffusion model, and I invite you to read their great paper linked below as well to learn more about the model and approach. Huge thanks to my friends at Quack for sponsoring this video, and even bigger thanks to you for watching the whole video. I will see you next week with another amazing paper.",
        "output": "### Latent Diffusion Models: An Overview\nDiffusion models, including Dolly, Imogen, and Midjourney, have achieved state-of-the-art results in various image tasks. However, their training and inference times are computationally expensive, generally requiring hundreds of GPUs. Latent diffusion models transform the image into a compressed latent representation, making the process more efficient and allowing for different modalities, like text, to be used as input.\n\n#### Diffusion Models: The Basics\nDiffusion models iteratively learn to remove noise from random inputs, conditioned with text or an image. They apply further noise to the image iteratively until it becomes unrecognizable. Once the noise from all images is similar, the model is ready to generate new images from similar noise in reverse order.\n\n#### Transforming into Latent Diffusion Models\nLatent diffusion models work within a compressed image representation, reconstructing the image with the same quality as other diffusion models but at a fraction of the computation cost. They can also work with different input types, like text or images.\n\n#### The Overall Model\nThe model takes an initial image and encodes it into a latent space, a compressed representation of the information in the image. Conditioning inputs (such as text or other images) are merged with the image representation in latent space, creating the initial noise for the diffusion process. A diffusion model is then applied in this latent space, and finally, a decoder reconstructs the image by upsampling the results.\n\n#### Applications and Availability\nLatent diffusion models can be used for super-resolution, inpainting, text-to-image generation, and more. The authors of the research have provided open-source pre-trained models for developers to run on their own GPUs."
    },
    {
        "instruction": "<|SYSTEM|>You are a scholars that is taking markdown notes about some video, you would be giving some written scripts for the task.\nYou have finished writing the notes, and you want to extract the title from the notes.\nThe title should only be the name of the technology or algorithm, no extra words needed.\nPlease write the notes in markdown format. No need to write \"Title:\" for the Title. Make sure the title is within 10 words.\nHere is an example:\n## Hidden Markov Model\n<|ASSISTANT|>## ",
        "input": "<|USER|>### Latent Diffusion Models: An Overview\nDiffusion models, including Dolly, Imogen, and Midjourney, have achieved state-of-the-art results in various image tasks. However, their training and inference times are computationally expensive, generally requiring hundreds of GPUs. Latent diffusion models transform the image into a compressed latent representation, making the process more efficient and allowing for different modalities, like text, to be used as input.\n\n#### Diffusion Models: The Basics\nDiffusion models iteratively learn to remove noise from random inputs, conditioned with text or an image. They apply further noise to the image iteratively until it becomes unrecognizable. Once the noise from all images is similar, the model is ready to generate new images from similar noise in reverse order.\n\n#### Transforming into Latent Diffusion Models\nLatent diffusion models work within a compressed image representation, reconstructing the image with the same quality as other diffusion models but at a fraction of the computation cost. They can also work with different input types, like text or images.\n\n#### The Overall Model\nThe model takes an initial image and encodes it into a latent space, a compressed representation of the information in the image. Conditioning inputs (such as text or other images) are merged with the image representation in latent space, creating the initial noise for the diffusion process. A diffusion model is then applied in this latent space, and finally, a decoder reconstructs the image by upsampling the results.\n\n#### Applications and Availability\nLatent diffusion models can be used for super-resolution, inpainting, text-to-image generation, and more. The authors of the research have provided open-source pre-trained models for developers to run on their own GPUs.",
        "output": "## Latent Diffusion Models"
    },
    {
        "instruction": "<|SYSTEM|>You are a scholars that is taking markdown notes about some video with Wiki style, you would be giving some written scripts for the task.\nAnd summarize each paragraph with a title.\nRemember to keep the technology or algorithm name, very important.\nPlease write the notes in markdown format.\nHere is an example:\n### Definiation\nA deadlock is a situation in which two computer programs sharing the same resource are effectively preventing each other from accessing the resource, resulting in both programs ceasing to function. \nThe earliest computer operating systems ran only one program at a time.\n\n### What it have to do with Relationsal Database ManagementSystem\nIn a database, a deadlock is an unwanted situation in which two or more transactions are waiting indefinitely for one another to give up locks. \nDeadlock is said to be one of the most feared complications in DBMS as it brings the whole system to a Halt. \nWhen you tries to insert or update data on database, sometime your record need to depend on the row record.  \n<|ASSISTANT|>#### ",
        "input": "<|USER|> Dear Fellow Scholars, this is Two Minute Papers with Dr. Károly Zsolnai-Fehér. Doctor Papers. Earlier, we talked about two Text-to-Video systems, Google's amazing Image & Video, and also NVIDIA's Video Maker AI. However, to the best of my knowledge, as of today, there are no easy ways of trying them out. Now, before we start, just to make sure tons of video AI-related techniques are available right now, many of them for free, I'll show you those, but their text-to-video system is still in testing phase as of the making of this video. I will show you some results for that too, however, good news, it is also expected to open to the public within a few weeks. So this is Runway. They had a hand in the creation of the amazing free text-to-image AI tool, Stable Diffusion, and they have an incredible creative suite for all kinds of AI-based video magic. I will tell you about text-to-video in a moment. First, they introduced Gen1, this is their video-to-video system. Here, you can take a video of yourself, then choose a style, and bam, you have a completely new stylized video. I think just this one feature makes it worth trying this system as it has tons of uses, but it has a ton of other really creative modes as well. We can mask out a dog and only change that part of the video, which will get tracked as this good boy moves around in the scene. Lovely. It can also remove the background behind us in a video and track both the foreground and the background over time. Also, if we have an image that we would like to expand, it can also perform text-to-image with the appropriate constraints, and it will expand it in a way that we often can't even tell that something happened here. We can also erase parts of an image and replace them with a text prompt as well. Now, wait a second…these are two image-related operations, but we are talking about videos here. So, what is going on? Well, one, these photos can be used for Gen 1 as a style to a previously existing video, or even better, get this. You can also create a video by adding a few photos that were taken a few seconds apart, and like magic, it creates a video out of them. Yes, that means understanding what these photos depict and synthesizing every pixel of every image between these photos. Absolutely incredible. And their suite is full of features we haven't even discussed yet. Colorization with or without using a text prompt, blurring faces while tracking them, and so much more. Now this is what you've been waiting for, onwards to text-to-video. Note once again that this is still in testing phase and will hopefully be part of the main suite within a few weeks. I was lucky enough to be able to try it, and it did not disappoint. Before you ask, the answer is yes, of course, I generated a video of fellow scholars holding on to their papers. Wow. I was told that it is not yet that good in generating humans, and if this is the not good, I can only imagine how good the launch version will look like. This is excellent. My first text-to-video experience ever. Loving it. What a time to be alive! And here is my other favorite, fluid simulations, well, in this case, of splashing liquids, which also worked really well. Some of the results were not as great, so a little cherry picking was necessary here, but in the end, I got some really tasty results. I was also told that the system is improved every week, so by the time you're watching this video, it will likely have improved a great deal already. The current system supports videos up to 1500x900 in resolution. Not bad. And that is easily good enough so that some folks are already creating incredibly good movies with it that are several minutes long. Just imagine that soon, every creative person will have access to such an amazing system and can let their imagination run wild. All of us. This is the future of video making. So what do you think, what will you make with this? Let me know in the comments below, and for now, let the experiments begin! Thanks for watching and for your generous support, and I'll see you next time!",
        "output": "### Introduction to Two Minute Papers\nDr. Károly Zsolnai-Fehér presents a discussion about Text-to-Video systems, Google's Image & Video system, and NVIDIA's Video Maker AI. Unfortunately, as of the creation of this video, there are no easy ways to try these systems out yet.\n\n### Runway Creative Suite & Gen 1 Video-to-Video System\nRunway offers a range of AI-based tools, including the text-to-image AI tool Stable Diffusion. The Gen 1 video-to-video system in Runway allows users to style and manipulate videos in creative ways, such as masking specific objects or removing backgrounds.\n\n### Image Operations in Runway\nRunway also provides various image operations that can be used in combination with the Gen 1 video-to-video system, such as expanding images or erasing parts of an image and replacing them with a text prompt.\n\n### Creating Videos from Photos\nRunway's creative suite allows users to generate videos from a few photos taken a few seconds apart. The system understands the content of the images and synthesizes the pixels in between to create a video.\n\n### Additional Runway Features\nRunway offers many other features like colorization, blurring faces while tracking them, and more.\n\n### Text-to-Video Functionality (In Testing Phase)\nThe text-to-video functionality in Runway is still in the testing phase and expected to be part of the main suite within a few weeks. Dr. Zsolnai-Fehér shares his first experience with the text-to-video function by generating videos of fluid simulations and fellow scholars holding on to their papers.\n\n### Potential Impact of Runway's Text-to-Video Feature\nWith the release of Runway's text-to-video feature, it is anticipated that creative individuals will have access to an incredible system to bring their ideas to life in video form, potentially revolutionizing the future of video making."
    },
    {
        "instruction": "<|SYSTEM|>You are a scholars that is taking markdown notes about some video, you would be giving some written scripts for the task.\nYou have finished writing the notes, and you want to extract the title from the notes.\nThe title should only be the name of the technology or algorithm, no extra words needed.\nPlease write the notes in markdown format. No need to write \"Title:\" for the Title. Make sure the title is within 10 words.\nHere is an example:\n## Hidden Markov Model\n<|ASSISTANT|>## ",
        "input": "<|USER|>### Introduction to Two Minute Papers\nDr. Károly Zsolnai-Fehér presents a discussion about Text-to-Video systems, Google's Image & Video system, and NVIDIA's Video Maker AI. Unfortunately, as of the creation of this video, there are no easy ways to try these systems out yet.\n\n### Runway Creative Suite & Gen 1 Video-to-Video System\nRunway offers a range of AI-based tools, including the text-to-image AI tool Stable Diffusion. The Gen 1 video-to-video system in Runway allows users to style and manipulate videos in creative ways, such as masking specific objects or removing backgrounds.\n\n### Image Operations in Runway\nRunway also provides various image operations that can be used in combination with the Gen 1 video-to-video system, such as expanding images or erasing parts of an image and replacing them with a text prompt.\n\n### Creating Videos from Photos\nRunway's creative suite allows users to generate videos from a few photos taken a few seconds apart. The system understands the content of the images and synthesizes the pixels in between to create a video.\n\n### Additional Runway Features\nRunway offers many other features like colorization, blurring faces while tracking them, and more.\n\n### Text-to-Video Functionality (In Testing Phase)\nThe text-to-video functionality in Runway is still in the testing phase and expected to be part of the main suite within a few weeks. Dr. Zsolnai-Fehér shares his first experience with the text-to-video function by generating videos of fluid simulations and fellow scholars holding on to their papers.\n\n### Potential Impact of Runway's Text-to-Video Feature\nWith the release of Runway's text-to-video feature, it is anticipated that creative individuals will have access to an incredible system to bring their ideas to life in video form, potentially revolutionizing the future of video making.",
        "output": "## Runway Text-to-Video"
    },
    {
        "instruction": "<|SYSTEM|>You are a scholars that is taking markdown notes about some video with Wiki style, you would be giving some written scripts for the task.\nAnd summarize each paragraph with a title.\nRemember to keep the technology or algorithm name, very important.\nPlease write the notes in markdown format.\nHere is an example:\n### Definiation\nA deadlock is a situation in which two computer programs sharing the same resource are effectively preventing each other from accessing the resource, resulting in both programs ceasing to function. \nThe earliest computer operating systems ran only one program at a time.\n\n### What it have to do with Relationsal Database ManagementSystem\nIn a database, a deadlock is an unwanted situation in which two or more transactions are waiting indefinitely for one another to give up locks. \nDeadlock is said to be one of the most feared complications in DBMS as it brings the whole system to a Halt. \nWhen you tries to insert or update data on database, sometime your record need to depend on the row record.  \n<|ASSISTANT|>#### ",
        "input": "<|USER|> Basically I'm just going to do essentially what we did with the handheld version, just now that it's on the robot. Today we are looking at the Frontier device. It has an embedded computer, a multi-camera system, a lidar, and inside there is hidden an inertial measurement unit. The cameras work pretty much as our eyes. The lidar basically allows you to get a point cloud of the environment by firing lasers at the environment and by measuring the return time it can get you a metric measurement of what's around you. And the IMU works pretty much as our internal ear. It gives us an idea of where gravity is and how fast we are rotating. All of these sensors can be fused together to answer the main question in robotics called SLAM, simultaneous localization and mapping. So the localization bit is knowing where you are and mapping is what's around you. SLAM is pretty much like a chicken egg problem. You don't know where you are, you want to know where you are in the environment, but at the same time you also want to know how the environment looks like. If you walk around you're going to see an orange line that shows where we were before. So if I keep walking around and I come back to the same place I was before, I can show you the main ingredient of SLAM, which is loop closure. There you go. So here you see a red line. This red line means that the system who recorded all of our previous positions recognizes if we were back to the same place and in this way we can correct all of the errors we have accumulated this far and have a better map by adjusting it, knowing that we were there before. The screen was just to quick visualization, but all of the problems are being solved in 3D. So here is the exact same problem we have solved before. So here you see the table, we see all of the map in 3D being built and the loop closure is right here. So after just a very quick walk we have a complete 3D map of the environment and we also have our past trajectory. The map is mainly built using the LiDAR because it gives you a continuous stream of point clouds, but to accurately know where we were we also need to fuse the IMU because there is physics inside. So the IMU tells us your acceleration for instance. So IMU is like an inertial measurement unit. If for instance there is an error of registration between two point clouds and we don't use the IMU, the system might just think we teleported somewhere else. But the IMU is telling us no, your acceleration was like that, so your velocity should have been like that and you should be here instead of somewhere else. So if you use the IMU on its own it would be kind of dead reckoning, it would be guessing your rough LiDAR. Yes, so there is a component of dead reckoning, but this is fused together with the other sensors. So if you were doing dead reckoning all of the time then we would end up on the moon because there are many smaller errors that get integrated over time. So the IMU is just giving us an idea of where we should be between two camera images or between two LiDAR point clouds and then the measurement from the point clouds and the cameras kind of correct those and are merged together. So we can have a walk and go downstairs in the lab. So we unofficially call it the bat lab and the reason for that is because we have a huge elevator that can carry an entire car. Here we are going to show the quadrupeds section of our lab. What's funny is the camera can see some of the LiDAR. Yes because it's near infrared and cameras don't have infrared filters. There you go. So here we have different quadrupeds, today we are going to see the spot one. This is all the way from where we were before here with the round table, the loop closures and we went downstairs. So all of the computation is inside here but for ease of visualisation we have some interfaces on the phone and on the computer. Most of our software is compatible with ROS which is a framework that is widely used in robotics community. There are many libraries and programs ready to use to accelerate robotics. This one in particular is called ROS board and it's running a little server on the computer and on the other side there is a client that gets the outputs from our algorithms as messages and these messages are then relayed and visualised in the web page, web browser interface. So on the phone that's just basically running? The phone is just an optional that we use especially when we do hand held missions, we want to have an immediate feedback of what's going on, everything is going alright so we have developed this interface. SLAM is very useful for robotics in particular because it allows us to know where the robot is and what the map around it looks like. So I'm just going to drive around the lab a little bit and then we'll be able to see kind of what the lab looks like. And now that we're kind of done, again you can see we have a map of the environment, we have a loop closure and SLAM is essentially a key component in enabling robot autonomy so a lot of the work that we're doing in kind of remote inspection, mission planning, all of those kind of things relies on the ability of the robot to know where it is and the structure of the map to actually do useful things, you know nuclear decommissioning, those kind of things is what we're looking at. The handheld has its uses but also if we can do this with a robot, obviously we don't have to send people into the same places, sometimes it's easier to access places with humans so the handheld device is useful but other times we prefer to use a robot so we have both options. On the front we've got what we call a frontier device, it's running the exact same mapping software but we're just not running the autonomy part of the system right now. What I'm going to show is the algorithm that is running inside the frontier and the main technique that is using it which is called factor graph. So a factor graph is a way to solve this SLAM problem by modelling the unknowns as nodes so we might have our position x1 and then position x2, position x3 and so on and this is what we want to know in addition to landmarks which are basically points in the environment which eventually will make up our map so we can think of the landmarks as L1, L2, L3 and so on and the measurements that come from camera or IMU or any other sensor create a relationship between these unknowns. The IMU might just tell us okay so between x1 and x2 your position should have incremented by this amount because your acceleration was this much over this period therefore velocity and therefore position and at the same time you can have multiple IMU measurements so you connect this and from x1 you might just see also things from the camera, features in the environment for instance a corner that you can track over time so this creates also some other relationships between our position and the landmark and you can see the same landmarks over different times so you can create these sort of relationships and if you keep adding all of these constraints then you have a strong idea of where you should have been and what was around you because all of these relationships are merged into an optimisation problem where the unknowns are exactly x1, x2, x3 and L1, L2, L3 and in this way the algorithm will find the best solution that makes sense for all of these relationships together. And then the last bit comes to solve the SLAM problem. Imagine that we walked a very very long way so imagine I've made this chain very long and at some point we are at time x50 and in reality we should be in x2 because we have kept track of all of the measurements we have collected so far and so we have a map of them and we close the loop by thinking okay I am exactly in the same place as x2 so I can create a relationship between x2 and x50. In this way all of the error we have accumulated up to x50 gets corrected, the map gets warped to match reality and we have solved the SLAM problem. It's oversimplified if I say the SLAM problem is solved many professors will become angry. Yeah you have solved the SLAM problem in this specific situation, yes you did. But not generally solved it. Yeah so in practice it's yet not solved because there are many problems related to noise in the sensors and inaccuracy in the calibration so there are techniques to make this solution more robust for instance. So if you have a SLAM system working then you know where you are and what's around you and also robots do know where they are and what's around them so we can make do autonomous things. Jane Street is a research driven trading firm with offices in New York, Hong Kong and London. Full of impossibly creative and clever people they like to think the next star individuals that may be working for them could be watching this computerphile video right now. That's why they're hoping you might peruse their website including this section for internships. Are these the sorts of jobs that might launch your career? Check out Jane Street there's a link in the video description and our thanks to them for supporting this episode of Computerphile.",
        "output": "### Frontier Device Introduction\nThe Frontier device consists of an embedded computer, multi-camera system, lidar, and an inertial measurement unit (IMU). These sensors help in solving the SLAM (simultaneous localization and mapping) problem, which is crucial for robotics.\n\n### Lidar, IMU, and Camera Fusion for SLAM\nThe cameras function as our eyes, while the lidar provides a point cloud of the environment by measuring the return time of fired lasers. The IMU is like our inner ear, giving us a sense of gravity and rotation speed. By fusing the data from these sensors, localization and mapping can be achieved.\n\n### Loop Closure in SLAM\nLoop closure is the recognition of the system returning to a previous position. This allows the correction of accumulated errors and leads to a better map.\n\n### 3D Map Building\nThe 3D map is mainly built using the LiDAR, which provides a continuous stream of point clouds. The accurate knowledge of the robot's position is achieved by fusing the IMU.\n\n### SLAM Applications\nSLAM is essential for robotic autonomy, enabling remote inspection, mission planning, and dangerous tasks such as nuclear decommissioning.\n\n### Factor Graph - Solving for SLAM\nFactor graph is a technique used in solving the SLAM problem by modelling unknowns as nodes and the measurements from the sensors create relationships between these nodes. By combining these relationships, an optimization problem is formed, providing the best solution for the unknown positions and landmarks.\n\n### Loop Closure Correction\nLoop closure correction warps the map to match reality by recognizing the same position in the map, which solves the SLAM problem for a specific situation but not generally due to noise and inaccuracy in the sensors."
    },
    {
        "instruction": "<|SYSTEM|>You are a scholars that is taking markdown notes about some video, you would be giving some written scripts for the task.\nYou have finished writing the notes, and you want to extract the title from the notes.\nThe title should only be the name of the technology or algorithm, no extra words needed.\nPlease write the notes in markdown format. No need to write \"Title:\" for the Title. Make sure the title is within 10 words.\nHere is an example:\n## Hidden Markov Model\n<|ASSISTANT|>## ",
        "input": "<|USER|>### Frontier Device Introduction\nThe Frontier device consists of an embedded computer, multi-camera system, lidar, and an inertial measurement unit (IMU). These sensors help in solving the SLAM (simultaneous localization and mapping) problem, which is crucial for robotics.\n\n### Lidar, IMU, and Camera Fusion for SLAM\nThe cameras function as our eyes, while the lidar provides a point cloud of the environment by measuring the return time of fired lasers. The IMU is like our inner ear, giving us a sense of gravity and rotation speed. By fusing the data from these sensors, localization and mapping can be achieved.\n\n### Loop Closure in SLAM\nLoop closure is the recognition of the system returning to a previous position. This allows the correction of accumulated errors and leads to a better map.\n\n### 3D Map Building\nThe 3D map is mainly built using the LiDAR, which provides a continuous stream of point clouds. The accurate knowledge of the robot's position is achieved by fusing the IMU.\n\n### SLAM Applications\nSLAM is essential for robotic autonomy, enabling remote inspection, mission planning, and dangerous tasks such as nuclear decommissioning.\n\n### Factor Graph - Solving for SLAM\nFactor graph is a technique used in solving the SLAM problem by modelling unknowns as nodes and the measurements from the sensors create relationships between these nodes. By combining these relationships, an optimization problem is formed, providing the best solution for the unknown positions and landmarks.\n\n### Loop Closure Correction\nLoop closure correction warps the map to match reality by recognizing the same position in the map, which solves the SLAM problem for a specific situation but not generally due to noise and inaccuracy in the sensors.",
        "output": "## Sensor Fusion for SLAM"
    },
    {
        "instruction": "<|SYSTEM|>You are a scholars that is taking markdown notes about some video with Wiki style, you would be giving some written scripts for the task.\nAnd summarize each paragraph with a title.\nRemember to keep the technology or algorithm name, very important.\nPlease write the notes in markdown format.\nHere is an example:\n### Definiation\nA deadlock is a situation in which two computer programs sharing the same resource are effectively preventing each other from accessing the resource, resulting in both programs ceasing to function. \nThe earliest computer operating systems ran only one program at a time.\n\n### What it have to do with Relationsal Database ManagementSystem\nIn a database, a deadlock is an unwanted situation in which two or more transactions are waiting indefinitely for one another to give up locks. \nDeadlock is said to be one of the most feared complications in DBMS as it brings the whole system to a Halt. \nWhen you tries to insert or update data on database, sometime your record need to depend on the row record.  \n<|ASSISTANT|>#### ",
        "input": "<|USER|> Hi, in this video we are going to take a deep dive into one of the most seminal papers regarding the diffusion models, which if you have heard about things such as Dali or any sort of painting application that takes for example text or some other thing and generates very realistic and beautiful images that are semantically meaningful to a human person, you probably have indirectly been introduced to diffusion models. So this paper is called Denoising Diffusion Probabilistic Models. This work has been done by UC Berkeley and the general idea is that you take an image and you try to add small amounts of noise to it in a step-by-step process and you want to basically have a model that is able to iteratively denoise a completely noisified image. For example you take let's say a basically a tensor at the same size of the image but with all the pixels let's say randomly sampled from a Gaussian distribution and you pass it to this model for example 1000 times and you want to obtain a picture which is going to be meaningful to a person. Now what that means is that considering that there is a probability distribution over for example the space of images and emissions or for example the distribution corresponding to realistic images, what you want to do is to generatively model that, meaning that you want to interact with that probability distribution and in a parameterized fashion model it and modeling a distribution in parameterized fashion in that case would be giving you the ability to sample images from this data. As further information regarding the generative models it's noteworthy that they are especially also important in the healthcare domain. When you have applications for which data, especially supervised data, are less available it's it will be beneficial to be able to generate synthetic data that are going to make you able to train several models without having to you know wait for let's say let's say some private data sets to be anonymized and then released publicly and like signing 200 forms and things like that so having a generative model has a lot of advantages and that is one for example that I've been used in learning with noisy labels that have been used in many cases. And generally there have been the three domains of VAEs, Valuational Auto-encoders, there have been the generative adversarial networks with the min-max game that they used to play and the currently the diffusion based models and they like the connections between the non-equilibrium thermodynamics and deep learning has been first investigated I believe in the paper in ICLR 2015 and this paper here is also building on top of that and yeah so diffusion models are pretty important these days. In the example here you can see that the image is generated by you know the generative modeling of images in the deep learning have been pretty successful as the images are semantically very meaningful. So let's talk about briefly what the overview of this paper is about so you have you know denoising diffusion probabilistic models and the idea is that you have something resembling a Markov chain like this and let's say XT is going to be a sample pure noise data and X0 be a semantically nice image you will have something like this that you want to get from XT to X0 this this this you know sequence of random valuables that you can see here X0 to XT so X1 to XT so this is going to comprise your diffusion model and X0 for example will be a semantically nice image and XT again would be a noise from a prior distribution. Now this paper is about discussing the reverse process and the forward process of it is going to discuss the connection between this and denoising score matching and also with the non-equilibrium thermodynamics. It's going to show that there's going to be some equivalence in there and it's going to discuss from certain information theoretical perspectives how a diffusion model is actually good. Let's start from the abstract. The paper first starts by talking about this goal which is high quality image synthesis and quality in this case means basically the image being semantically adequate so that when you look at it you feel like it's a real image and the way it is done is by a generative modeling of general modeling of this distribution. It does have a focus on non-equilibrium thermodynamics and its connection with the deep learning and it's going to show a novel connection between the diffusion probabilistic models or diffusion models and denoising score matching with also with the Langevin dynamics. They do certain information theoretical analysis ensure that their model could basically resemble a progressive loss CD compression scheme and it could be interpreted as a generalization of autoregressive decoding. The code is also available in this URL and one thing to note from the start is that their method results in high quality images being actually generated so from the qualitative standpoint it's known that it's a good model. First let's talk about a definition for the concept of diffusion models or diffusion probabilistic models. Per the paper a diffusion probabilistic model which they call diffusion model for brevity is a parameterized Markov chain trained using variational inference to produce samples matching the data after finite time. Okay so now we know the definition of the diffusion models. The idea is that you want to reverse the diffusion process and basically you are dealing with a Markov chain that gradually adds noise to the data in the opposite direction of sampling until the signal is destroyed. In other words you want to reverse this information destruction process. One thing to note here is that this assumption of gradual addition of noise and the fact that you can now basically add it as a small amount of Gaussian noise it is sufficient to set the sampling chain transitions to conditional Gaussians too as is said in the paper allowing for a particularly simple neural network parameterization so having this Gaussian conditional assumption is going to enable you to efficiently parameterize your neural network models and utilize it in this process. And diffusion models are straightforward to define and efficient to train which makes a good case for the use of them in genetic modeling of images. And this paper is going to show that even though they have not been a demonstration that they are capable of generating high quality samples that it is possible to make use of them in order to do that. Another important thing that they mention is that, let's read this part, we show that a certain parameterization of diffusion models which basically the thing that they have here is going to reveal an equivalence with denoising score matching over multiple noise levels during training this is like one equivalence that they show and the other one is with the annealed Langevin dynamics this comes back to the non-equilibrium thermodynamics. I believe this model is mainly for modeling molecules etc but they basically want to show that diffusion models have this type of equivalence with this concept as well and basically they focus on these equivalences and demonstrating those as their primary contribution in this domain. There are certain important points in here. One, you know that the sample qualities, the generated images are semantically nice because you can see them as a human you can see that basically the images that are generated using these diffusion models are great. Now this means that from the qualitative standpoint they're good. Now their models do not have competitive log likelihoods compared to the likelihood based models so that is that means that in terms of a certain quantitative measure their models are not competitive but at the same time there are high quality images generated by them. Now one thing that they have found out with this regard is that they, let's read this part, we find that the majority of our models lossless code lengths are consumed to describe imperceptible image details. Which reminds you of what? Yes, compression. And then they presented a more refined analysis of this phenomena in the language of lossy compression. They showed that the sampling procedure of the diffusion model is a type of progressive decoding that resembles autoregressive decoding along the bit ordering that vastly generalizes what is normally possible with autoregressive models. So this part of information theoretical analysis of this also from the perspective of rate distortion shows that basically diffusion models are good. One important thing to note in here regarding the DDPM is that it is trained using variational bounds. So when you read the paper you encounter that they mentioned that we use the usual variational bound and that is ELBO or evidence lower bound. Let's go over the overview of that as a background information to see what exactly it is and what does it mean and then let's go back to the paper. So with regards to ELBO, consider the generative modeling concept. You have X which is a random variable which corresponds to your observation. For example X could be the random variable of your image, the tensor of the image for example, and Z would be a latent variable which is unobserved. For example the CNN representations or like a label or something like that. So you have X and you have Z. You assume that X and Z are assumed to be basically jointly distributed. In other words you assume that there is a connection between them otherwise the whole thing would be not exactly meaningful. Your goal in generative modeling of this concept is to approximate, learn, and interact with the true distribution that these observations actually come from. So you assume that there is this PSR of X that you don't know about. And the idea is that in a parameterized fashion you want to introduce basically a model to basically try to optimize it from a family of models to resemble the P star. So when you're trying to estimate this probability distribution the idea is that you consider a family of models for example let's say theta is the weight set for let's say a resonant 50 or something like that. You want to assume that there is a value from this set of values from this model family that can be found for example by a sort of optimization that models the joint distribution of X and Z or the observation and its corresponding latent. So when you define this sort of generative model you have access to the p theta of X and Z. Now the thing that is important is that you want to eventually deal with this margin meaning that you want to interact with a modeled version of the true distribution over the space of these observations. For example the distribution of images so you can sample from it and interact with it. Now if you look at this given that you have this p theta here this marginal is going to correspond to this meaning that you look at the joint distribution by the way this is the definition but it's also true so if you take p theta of X and Z you can basically integrate over the values of latent and obtain this marginalized distribution p theta X. Usually it is not something that is tractable to because you have a lot of choices for the Z here. Again the goal of p theta is to often present a nice log likelihood for the observed data so this is one one way to measure how good this distribution this parameterized distribution is for the model and the way it is is that you wanted to obtain you know a good log likelihood value meaning assigning high probability to the observed value. Now let's see what the ELBO wants to say. The idea is that if you sample any data point from this distribution that you model let's say an image if you look at the log likelihood of the image for example p theta of X which is the marginalized result over there that's going to show you the likelihood of the p theta of X and then you have basically the logarithm of that. By the way the p theta of X basically is the likelihood that you have observed example X the probability of observing example X given theta which is the likelihood of theta over X that's essentially the thing that you are centering your attention around. So the idea for the ELBO is that this value that you're trying to maximize is lower bounded by this variational bound and often focusing on optimizing this is easier. So let's see what this is. The bound says that if you consider a Q another parameterized distribution parameterized by the parameter set Phi which parameterizes the probability distribution of Z the latent random variable given X so if you have a parameterized model Q like this and you sample these and take the expected value over these in saturated Z's and internally you have this value over there you have basically this lower bound over there so on the top you have the joint distribution of X and latent and at the bottom you have the Q of Z. So this log likelihood over there it's going to be the evidence for X and this term over there is going to be the evidence lower bound for X. Assume that the p star is the true distribution what you want to do is that you want to parameterize and choose a model family let's say p theta of X and you want to model that so what ideally you want to do is that first of all you want to define some sort of loss function that measures the similarity of this this probability distribution that you have introduced with the original true probability distribution and how do you want to measure this so assuming that you have L you want to keep sampling random samples from this true probability distribution and you want to take the expected value of this this results in a loss that in general measures to what extent this probability distribution models the true distribution and you want to pick the theta in a way that this this sort of divergence is minimized I didn't specifically use the word distance here but the idea is generally you want to make these as similar as possible so it means that we want to consider implicitly parameterized probability distributions what does that mean consider the PZ which is going to be a distribution for the latent for example that could be a Gaussian distribution and then define F theta it could be for example a convolutional neural network or something like that and use it to model the P theta of X and Z now you sample Z from this prior distribution for example Gaussian and then now with that you parameterize the P theta of X given this latent distribution so for sampling you first pick a Z and then you put it inside this you deal with this conditional probability in the sample X from it now how do we construct this one note that you are going to deal with this marginal over there so you would want to compute this function over here over here which is impossible to compute again for the aforementioned reason of having a lot of latents to go over so let's see what do we need to do so when we want to model the P theta of X you can write it like this using the base rule introduce latent Z conditioned X on Z times probability of C what this gives you is the probability of X and Z and the bottom you have the probability of Z given X which is probability of Z and X which will be simplified with the one in the denominator it's also divided by the probability of X so it goes up and theoretically these two are equal this is the written version of that so you can see that you open this up you cancel these two again Z and Z and then bring the P of X to the denominator and you will have this now we need to approximate this so having Z you have this probability and this basic this budget model gives you this P of Z is going to be a prior so both of these are easy to compute so we gotta have to figure out a way to deal with this one in order to deal with that and in order to make the problem tractable we introduced Q which is another parameterized model which is going to in a discriminative fashion because you have Z condition on X rather than dealing with the joint distribution which would say a generative modeling which is what we were doing prior to this this parameterized discriminative model is going to focus on the probability distribution of latent given observation so this is going to model that and make the problem tractable let's go over it again so you have the prior over latent for example there would be a Gaussian distribution or something like that a multi-dimensional Gaussian distribution independent etc then you have the probability distribution of X the observation given the latent Z which is something that is easy to compute given that you have parameterized the model to to deal with that now X is your the random variable corresponding to your observation let's say X is observed you have the P theta of X given Z this is going to give you the likelihood of the latent so you have latent Z you have the observed X this is going to sorry this is going to be the so if you look at the P theta of Z given X what this gives you is the posterior probability meaning that if you have a prior probability of Z being the latent value assuming that you have X conditioning on you having X this is going to give you the posterior probability and as you may know posterior probability calls so you can use the base rule write it this way what this means is that if you look at the posterior probability is going to be a result of a prior times the probability distribution of X given Z which is the likelihood of the latent Z or the likelihood basically that to what extent Z is describing the process of X so what this also might remind you of is things such as the Bayesian logistic regressions or any cases in which the prior information takes takes a role so yeah this is basically what you have over there and then you want to say that okay I have P theta of Z and X I want to model it in this parameterized fashion with Q of Phi one thing to note here is that this is basically a proportion it's not exactly identical okay so we want to approximate this like this so we need to do this because otherwise we have to compute like P theta of Z and X and what this means is that you would need P of X which is computed this way which is impossible so let's write this again do you want to parameterize P theta of Z and X how can we write this we can write this as this you know since you write it as this you would need this denominator value which is hard to compute so that's the motivation for Q so you want to find a good approximation for that using the Q parameterized by the parameters of Phi and Z condition on X so this is also called amortized inference okay let's write it differently then so looking at the P theta of Z and condition on X you can write it like this so P Z is easy to compute P of X given Z is also easy to compute you want to deal with this one so you model this one with the Q all right let's proceed you have to expect that okay so you basically have this expected value that you want to maximize and you want to figure out what exactly is the relationship over there so if you consider this observations being sampled from the true distribution and you're looking at this you know you can plug in this P star as multiplication and division over there then you can open it up like this using the logarithm rules and calculus okay so on the left side you will have this thing over there I'm sorry it's actually the reverse of this order so this one is going to be ln of P star so you would bring the P star into it because you're taking expected value and then integrate over it this is going to give you the negation of the entropy of the true distribution it's not something that is under your control you can kind of ignore it then on this part of the deal you can actually have another negation sign and bring the P star over there to the top and make it a kale divergence let's see how it would work so as you can see now you have the kale divergence between the target true distribution and the reference that thing that you're optimizing the P theta of X meaning that you can basically write it like this so you can see that focusing on this log likelihood and focusing on the scale divergence is kind of equal to each other so trying to maximize this is trying to essentially minimize that so the goal is to obtain a theta which maximizes this kind of like a energy function or the negation of the loss function and the way to do is that is that you sample a lot of examples from the true distribution for example you take a lot of images and then you compute this ln P theta of Xi divided by n so basically you get this empirical expected value and you try to sort of assume that this is the same as this and then you focus on this probability T you use the multicolor integration and basically focus on e of the expected value of Z being sampled from this latent and this conditional over there which is P theta of X and Z Q of the NX so you basically are focusing on the variational bound over there one thing to note is that looking at this you can see that this thing over here is going to be an unbiased estimator of P theta now note that the logarithm function is nonlinear so you cannot directly go from this to that and one thing it means is that if you take an example from the X from the P star if you take the logarithm of ln P theta of X it'll be equal to the ln of this X this expected value over here so again you have X and Z with the parameterized of Z and X so you have this equation over here so you want to use the convexity of the logarithm function and see what exactly it would get you to. Note that the logarithm function is concave so if you look at this curve is going to be something like this for example take two points if you take the function logarithm the value of the function applied over the let's say the x-axis value of the average of these two which is here the value of this function is going to be larger than this so this goes back to Jensen's inequality and you can write it like this so given that you have a logarithm of average for example it'll be function applied on this this is going to be lower bounded by the version of this in which you have brought this one out so it'll be larger than equal to this expectation over there so thus you end up with the ELBO which was this. Going back to the concept of diffusion models you have this sort of a Markov chain over there and the probability distribution over the X 0 this is going to be essentially that the thing that you're trying to model meaning X 0 is your semantically meaningful image you want to use theta parameterization in order to have a probability distribution P that models this you're trying to model the true distribution so you can interact with it you know that it's basically a marginalized version over there. Note that this additional noise addition is going to give you that X 1 until X T are your latent unobserved random valuable so this marginalization essentially equals this so you can assume that in our previous discussion Z over here is X 1 to X T. This thing over there is called the reverse process which is the probability joint probability distribution over X 0 all the way until X T so X 0 again is your image and X 1 to X T are all of the latents of the same dimensionality as X. And you assume that there is a true distribution the images are being sampled from thus you're focusing on the generative modeling of that using diffusion models. Okay so P theta of X 0 and T is the reverse process so there's a Markov chain of basically Gaussian distributions sorry the learned Gaussian transitions in order to denoise the basically the noise-defined image into a meaningful one. What that means is that you let's say you sample X T from a distribution a well-defined distribution for example Gaussian independent Gaussian distributions 0 and I 0 being the average I being the covariance matrix. You define the P theta of X 0 and T or in other words the reverse process like this so what this means are multiple things that one thing is that looking at this you you can see that this is basically equivalence of a Markov assumptions or Markovian assumptions so you have this independence sort of going from T to 0 for example for T minus 1 from let's say T plus 1 and and so on so read it like this you sample P X T sorry you sample X T from this prior distribution and then you keep doing this transition going from X T to X T minus 1 now in order to model the X T minus 1 with X T they use the Gaussian transitionals so this is read like this you have a parameterized model that models the probability of going from T to T minus 1 or denoising what this means is that if you look at this which is which gives you the value of the probability of observing X T minus 1 a value for X T minus 1 given that you have observed the value for X T given the parameters of theta it is modeled as a Gaussian distribution which in which you plug in this X T minus value and get your result which is parameterized by a mu theta using X T and T not that is independent from the T plus 1 etc and you have the covariance metrics again parameterized like that and you have the forward process which is you know focused on Q and you have it like this going from 1 to T you are noisifying essentially the image so this transition is defined again with another Gaussian conditional so when you have X T minus 1 what this basically says is that you take X T minus 1 you scale it by this parameter by the way beta going from 1 to all the way to the last step T which is usually 1000 this is called the variance of schedule what you have here is that you use this as a multiplier of the I so you're sort of independently adjusting values with this variance and then here you are scaling the average over there so you basically are adding Gaussian noise in additive iterative steps to your previously observed observations this over there again remember the Q of Z and X or the Q of latent given observation this is the approximate posterior over there note that you have Q of latent given observation this is called the posterior of latent it is fixed to the Markov chain of gradually adding Gaussian noise as shown above and again B1 sorry beta 1 to beta T is called the variance of schedule now let's look at the variational bound or elbow for this so you have the logarithm of the likelihood over there and this is going to be the upper bound on this loss and you can open this up and write it like this and note that in this type of expansion you now have this probability of XT separated from these transitionals that describe the forward and reverse process now with that loss you need to define the variance of schedule or beta T's which are hyper parameters that can either be fixed or be learnable parameters and then parameterizing the Gaussian conditionals for the reverse process in their notation you have 1 minus beta T being the alpha T and alpha bar T would be the multiplication of all the alphas before the advantage of that is that in the forward process now you can have a closed form and sample XT T not necessarily being just one increment of the previous index and just go from X 0 directly to XT and it will be basically so the probability of having a noisified XT given an observed X 0 for example an image would be the Gaussian with the X 0 being scaled with square root of alpha bar T and this factor by multiplying the identity matrix as the covariance the reason why that's the case is that if you consider two Gaussians for example mu 1 and Sigma 1 being the average and covariance of the first normal distribution and the two index and the index is basically the second one if you define another random variable as a summation of X and Y the distribution corresponding to that would be a normal distribution with the summation of their averages and summation of the covariance matrices now let's look at X 0 to X 2 this is not the same notation as the X 0 before this is just showing how exactly this is scaling process would do so let's say you have an X 0 from an sample from a normal distribution with the average of 0 and the standard the covariance matrix of I X 1 let's say X 1 is defined this way meaning that it is sampled from this normal distribution with X 0 being scaled by square root of 1 minus beta 1 and the covariance of beta 1 I you can write it as this so essentially you can write it as this basically that the baseline plus this other random variable which is sampled from average of 0 and the covariance matrix you can do the same with X 2 and expand it and you will get to this based on X 0 by substituting what we found for X 1 in the position of X 1 now you can see that with this multiplication you essentially have the square root of alpha 1 times square root of alpha 2 so this will be the square root of the alpha bar 2 and as you can see that the square root was missing over there but yeah that would be what it would be and then you have two normal distributions one with this covariance the other one with this so you can essentially sum them up and by doing so you can basically see that you will have 1 minus alpha 2 times alpha 1 or in other words 1 minus alpha bar 2 and this essentially could be served as some sort of an induction step and it showed a closed form that you solve for again what this means is that you can pick an arbitrary time step T and basically get the Q XT condition on X 0 like that now let's go back to the original variational objective which was this and this is a thing that we want to focus on optimizing we showed that it could be opened up and written like this and this could be further extended and written like this so you have to look at them of this so P or the probability the rest probably the XT is sampled from is a known prior so you have it like that and then you have these transitionals and you have sort of the initial transitions over there so you have basically the three separate terms now let's look at this how else can we write the Q of XT given XT minus 1 so it was shown in the paper in ICOLO 2015 that you need to introduce the X 0 to make distractible and given the Markov assumption you can essentially add the X 0 over there and then you can use the base rule to write it like this so you have Q of XT minus 1 given XT and X 0 Q of XT and X 0 Q of XT minus 1 given X 0 so we know the closed forms for these two so we also need to kind of put a major focus on this one so let's combine by writing the Q of XT minus 1 given XT like this you have this new formulation which will be simplified like this and as you can see this Q of X 1 and X 0 is kind of been removed and on the left side also you have this additional Q of XT given X 0 if you look at it more closely the first one you can essentially take this negative sign and kind of like move this on the top you will get the KL divergence between this reference distribution which is the Q of XT given X 0 with the prior you had on XT on this part you will have the KL divergence between these this this transitional and these distribution so you can you have like a summation of certain KL divergence terms and the last one would be this term another diversion which is not trackable but it's theoretically interesting is like this so you can use the Bayesian rule but you do not introduce the X 0 and it can be written like this essentially we'll boil down to the KL divergence between the Q of XT and P of XT and then here you will have the average of all these KL divergences between this basically the corresponding transitionals and eventually for this one because you have the ln of Q of X 0 and you're getting the expected value over this EQ you will have the entropy of X 0 over there and then notations that refer to first one which is the the the forward distribution on XT given X 0 KL divergence of it with the P of XT they refer to it as LT with the uppercase T for each one of these terms inside this summation you have LT minus 1 which corresponds to the KL divergence between the Q of XT minus 1 given XT and X 0 we'll talk about this more and the P theta of XT minus 1 given XT for the reverse process and then for the L 0 you will have the P theta of X 0 given X 1 now here's an important derivation so we want to show that if you look at this Q of XT minus 1 given XT and X by the way this is X 0 that you will have it as a normal distribution with a average and basically a factor for the covariance of scale now this mu for this time step T the mu tilde is a function of XT which was one of the conditions and X 0 the other one of the conditions and can be written like this so it's kind of like a complicated formulation and then you have the covariance one the beta tilde T being this scaled version of beta T now let's see why this is the case first of all you cannot remove X 0 or XT because if you look at the probabilistic graphical model of this you are interested in XT minus 1 and you the variables that you observe are XT and X 0 so the presence of each one of them does not make XT 1 independent from the other one so both of them play a role here so in order to compute that you use the Bayes rule and basically write it like this meaning that you take XT on the other side you can kind of ignore the X 0 X 0 will be the condition so if you for example to find another renamed distribution of XT minus 1 given XT it would be essentially the Bayes rule for that so keep the X 0 in the conditions use the Bayes rule on this to you bring the XT on the other side so you have the Q of XT given XT minus 1 and X 0 and you multiply this by the Q of XT minus 1 so you can have the joint distribution on the top and then on the bottom you will have a Q of XT condition on X 0 so if you for example ignore the X 0 for now it'll be like Q of the probability of XT condition on XT minus 1 now we know the closed form versions of this this and therefore one other thing we know is that when we look at the Q of XT given XT minus 1 and X 0 it will essentially be the XT given XT minus 1 because of the Markov assumption and it's going to be proportional to this exponent over there so you have this average based on the rule because you are looking at the transitional XT given XT minus 1 in the forward process so you have this a scaled version of XT minus 1 serving as your average and you will have this beta T to kind of serve as your scale for the covariance now again when you look at this you can write it as a Bayes rule like this note that we remove the X 0 from this one due to the Markov assumption so you have this term and we can sort of combine them and see what exactly would be the corresponding statistics for the Gaussian that results from this to have the X from the first one you have this term then the second one and the third one you will expand it and become something like this and let's add the last term as well so it becomes something like this just nothing complicated going on over here so you just essentially expand this one which becomes this expand this becomes this and expand this one which becomes this term note that you can rewrite the above like this so you will have this factor being for example constant multiplying by XT minus 1 squared you have minus 2 times this factor over here times XT minus 1 plus some other some other terms that are not a function of XT minus 1 so you do not see XT minus 1 appear over there thus you can see that assuming that it's going to be something like it XT minus 1 minus a mu to the power of 2 you will have to be able to find it by dividing this whole term by 2 meaning to losing this factor and then dividing this by this so if you do that you will have this term and this term so this term will be here this term would be here and you can simplify it like this so this is kind of again if you want to write this something like this XT minus 1 minus mu into 2 so in the bottom here you will have this term so alpha T plus beta T would be 1 alpha T times alpha bar T minus 1 becomes alpha bar T so you can simplify it and write it like that and add the other term to make it like so you have the basically the mu tilde of T shown as for beta you basically want the inverse of the factor that multiplies the XT minus 1 squared which is the inverse of this term which can be written like this since this paper is going to focus on a concept cost-sufficient statistic I just felt like it would be good to just go over what exactly sufficient statistic meant as some sort of background information first so assume that theta is a parameter that you're trying to estimate for example let's say that the average population the average height of a population or something like that and X be the norm basically that a random variable corresponding to your observation for example you could have in sampled heights of this population for example and that would basically be an instantiation of this random variable now TX or a function of this random sample is that gives you a value any function of this random sample is a statistic now let's see what does sufficient the statistic mean consider this equation over here so on the left you see f of X so basically the probability distribution condition on the sufficient statistics value for the random sample so what this basically means is that let's say X is an instantiation of this random sample for example let's say n is 100 and X could be for example height of the first person is 80 the second person is 90 something like that and these values actually don't mean much yeah so let's assume the first one is like 150 centimeters the other one is 180 centimeters things like that and then T is a value for example for the EDD the statistic that you're considering and theta is the parameter that you're trying to estimate the value for that so you know that you can basically open this up and write it like this so on the bottom you're going to have the probability of T given theta and on the top you have X so basically you don't touch theta and you use the beige rule for this one thing to note is that given that T is a function of X assuming that you have X you have T so you don't need to like write f of X and T that bond will be vanishing from here so roughly it'll point out it could boil down to something like this it will be the probability parameterized by theta the thing that you're trying to estimate for example a Bernoulli distribution and a Gaussian distribution something like that and then on the top you have the probability values for essentially obtaining the instantiated sample of this random variable of your random samples so for example the probability that the 100 randomly sampled people that you have measured their heights correspond to X1 till XN and then on the bottom is the basically a theta for example let's say you have the statistic that you're considering be mean and let's say it's the probability for that average should be for example hundred and sixty centimeters or something like that now when do we call TX a sufficient statistic the condition for that and it's a definition so not necessarily have to prove it but you can kind of intuitively see what this is about TX is a sufficient statistic if if you consider this probability meaning that having TX you're considering the probability of X you know that there's a theta here if this probability does not depend on theta meaning that it if it is a function of only X and T then you can say the T is T of X is a sufficient statistic one thing it means is that you're considering your random samples and then intuitively TX one is basically a thing that you need in order to estimate the parameters theta the point here is that T of X is sufficient if it contains all of the information that you need in order to estimate theta meaning that you cannot do better do better of a job at estimating theta if I give you the entire values for the random samples that kind of means that there's no branch in this probabilistic graphical model from X to theta and one other thing this conditional here means is that when you have the value for this T these two become separate in this probabilistic graphical model and therefore you will have this conditional assuming the TX is sufficient the statistic again this G of XT means that you are no longer dependent on theta one other thing that I mentioned is that when you have two Gaussians that you're comparing and you're computing for example K-divergence or cross entropy or something you don't necessarily need to do the expensive Monte Carlo estimation you can use closed form formulas for example if you look at two Gaussians QX and PX parameterized by this mu and Sigma and if you want the cross entropy between them which is this formula over there you don't need the Monte Carlo estimation you can derive it like this you can write the formula for PX and when you expand it it becomes this so the point is if you have something like this the computations have become simpler you do not need to necessarily do Monte Carlo estimations the first thing we need to choose is the variance scheduler the better T's over here you also don't need to define a model architecture and parameterized Gaussian distributions for the reverse process they also showed a explicit connection between diffusion models and denoising score matching one important thing in here is that they do not learn the betas here and they focus on adding small Gaussian noise to a large number of steps so in defining the forward process variance the schedule better T instead of making it learnable they fixed them to constants and therefore because these betas are fixed to constants this posterior Q is not going to have any learnable parameters therefore the LT when you have on top remember the PF XT is something like a Gaussian distributions independently sampling each one of the dimensions so this LT here is not going to have any sort of term that you can optimize over so this can be constant and removed from the from the loss function as for the reverse process which is supposed to do the denoising so you have the P theta of XT minus 1 given XT it will be basically parameterized as a Gaussian with normal distribution of XT minus 1 you have mu theta and you have Sigma theta parameterized with XT and T let's see what the design choices made in this paper are so you know that for the Sigma of XT T they chose to make it basically having the scale times the the covariance matrix identity and they experimented with this Sigma T to be better T and another option to be the better tilde of T and the results they showed that were similar the choices for the seed were being sampled from the Gaussian distribution of zero and I and they also tried to fix point they kind of were playing with the lower bound and upper bonds of the reverse process entropy now another important question is the parameterization of the mu theta of XT and T in this reverse process you can remember you want to reverse this diffusion process thus you need this to parameterize the P theta of XT minus 1 XT which plays the important role of the fuse reversing this diffusion process you note that this P theta of XT minus 1 XT had a major role in LT minus 1 in the definition of the loss we defined based on the elbow bound and that loss could be written as something like this given the fact that you are looking at KLI versions of caution so again you do not need to do the multi-color sampling once you have the information regarding the parameterization of the average and standard deviations so this term is not going to be dependent on Tessa the parameters of the model that you're trying to optimize so you can ignore this one and focus on this know that when you're looking at this reverse process you basically want to go from XT to XT minus 1 so let's write XT based on X 0 so you know that this based on this closed form formula you can essentially if you have X 0 you know that the average is going to be like this so you can separate this and write it as this baseline plus this error term so let's say you sample epsilon randomly from Gaussian distribution of 0 and I you could scale it by 1 minus alpha bar T and add it to this therefore you will have X 0 written as XT like this now let's come back here in your paper here you have LT minus 1 minus C so you removed C here and you're focusing on only the parts of LT minus 1 that depends on the theta on which you're trying to optimize and you will have this term over here and here you would have the XT like this you know that the mu tilde of T was parameterized based on XT and X 0 so you can kind of you know write it like this and this would be purely writing mu T based on XT in which XT itself is a function of X 0 and this error term now note that you want mu to essentially minimize this loss so you want basically this to optimize this okay so you now have the mu theta based on the formulation we just showed it's going to predict this term over here given XT okay now this means that you can parameterize mu theta to be something like this so here what this means is that everything is essentially the same you just have this parameterization of this error here which is a function approximator that's supposed to predict the epsilon or that error from XT now let's read this part to sample XT minus 1 what you do is that you take the XT minus 1 from this this is what you want to do what you do is that you compute the XT minus 1 based on this formulation in which Z is sampled from Gaussian distribution with 0 and I and epsilon theta is a parameterized version that's going to basically predict epsilon from XT and the time step T now the other important thing that they do is showing the connection between this and the Langmuir dynamics which is in the sampling the algorithm to the Langmuir dynamics which is I believe a model that is proposed for modeling molecules what you have is that you have a forward process in which you have this term here which is the gradient of the logarithm of the distribution of it and then you have this additional terms they show the connection between this dynamics and this sampling formulation over here and basically showing to this epsilon tilde the parameterization is supposed to learn the kind of like a parameterized estimator of the gradient of the logarithm of the distribution now one other thing to do is that if you go back to this LT minus 1 definition let's say you want to kind of bypass this mu tilde over there and write everything based on epsilon you will have this simplified formulation of LT minus 1 which brings you down to this note that now you have this epsilon minus parameterized version of epsilon tilde now this formulation resembles denoising score matching over multiple noisy scales indexed by T thus they show that we see that optimizing an objective resembling denoising score matching is equivalent to using variational inference to fit the finite time marginal of a sampling chain resembling Langevin dynamics and they show that the epsilon prediction parameterization both resembles Langevin dynamics and also simplifies the diffusion models variational bound to an objective that resembles the denoising score matching so they show the connection to one to the Langevin dynamics and non-equilibrium thermodynamics in deep learning and the other one is the denoising score matching so it could be balanced between showing this type of diffusion model with these two for the reverse process they considered image data essentially as U into 8 so the values are between 0 and 255 they scale it linearly to minus 1 and 1 and then to obtain discrete log-likelihoods because you want to predict something like this they consider the last sem of the reverse process to be an independent discrete decoder derived from this specific Gaussian over here now if you look at this formulation with this basically means is something like this you have a Gaussian distribution with certain mean and average any X that you pick given the fact that you have basically mapped it to minus 1 and plus 1 you consider a width of 2 divided by 255 2 being 1 minus 1 minus 1 and you consider this surface to be the probability you associate with that X there are two boundary cases of course 1 if X is 1 in which instead of the 2 by 255 you'd consider the past one per 255 width plus everything that comes after and the minus 1 coming back here they mentioned that this choice here ensures that variational bound is a lossless code length of discrete data without need of adding noise to data or incorporating the Jacobian of scaling operation into the log-likelihood one thing that they have found is that the sample quality it was beneficial with the sample quality to focus on this simpler simpler to implement thing which is a simplified version of the loss that focus on epsilon and epsilon theta and yeah one thing to notice that in this expectation T is uniform between 1 and T T is usually 1000 one thing that is important about this is that there was a weight over there which was discarded so this simplified objective discards the vein that it had before so it's itself a weighted variational bound sorry a variational bound that emphasizes different aspects of reconstruction compared to the standard variational bound so it's not necessarily optimizing the same thing therefore it's okay to see two different results and what happens in here is that the diffusion process that they have causes a simplified objective which is shown here to down weigh the loss terms that correspond to small T now why is it good for the sample quality is that they conjecture that okay so these terms train the network to denoise data with very small amounts of noise so when you have T being small means that the amount of noise that you have to reconstruct is very small so it is beneficial to down weight them compared to when you have like more difficult tasks and you know noisier things and larger values for T as for their experiments again they said T or a number of steps 1000 to all the experiments they choose these values for better one to get to T scale them linearly from 10 minus 4 to 0.02 they make sure that the signal to noise ratio at the X T is as small as possible so if you look at the KLA versions of this Q meaning that having a zero when you reach the last information at the last stage when you have the XT you kind of want to make sure that all the information is sort of lost so if you look at this prior distribution of P of XT this scale divergence between these two is very small and that shows that most of the information is kind of lost in that diffusion and therefore it will teach them effort to interact with a probability distribution rather than memorize intense they use a unit backbone and train the model with that which was the reverse process parameterization and training objective populations there are three important findings one is that when you chose to predict a mutility instead of the ER epsilon it was the baseline option so if you focus on this it works well only when trained under true variational bound instead of the unweighted mean square there second was that learning reverse process variance by incorporating a parameterized diagonal sigma theta into the variation bound led to unstable training and poor sample quality so they figured out that choosing a fixed variance schedule is better and three predicting epsilon as posed performs approximately as well as predicting the mutil there when trained on a variational bound with fixed variances but much better when trained with simplified objective they also focus from the progressive coding perspective as well and there are multiple interesting things so first of all if you look at the code links of the cipher 10 models so the gap between train and test is going to be very small which means that the model is not overfitting given that this gap is small so that's one thing to note one they mentioned that their lossless code links are better than largest miss reporter for energy based models and score matching using a Neil's more annual important samplings they mentioned that they are not competitive with the other types of likelihood based generative models now having that plus the fact that there are samples are of high quality they can check it is that okay diffusion models have an inductive bias makes them excellent lossy compresses from the rate distortion theory analysis they focus on these terms of these plots over here and with the algorithm strain for that they mentioned a discussed that this distortion decreases deeply in low rate regions of the rate distortion plot indicating that the majority of beats are indeed allocated to imperceptible distortions after that they also focus on progressive generation so for example look at the figures here such as this one they mentioned that we predict the results of the reverse process x0 wild sampling from the reverse process using algorithm 2 so you can see when you go from the image to this one and going you know back to this as you go forward so for example when you get to this part that large context of it for example I don't know if it's a bird or something that the main object is still somehow parent but then most of the information gets lost as you go forward they also show the connection to the auto aggressive decoding for that they have a specific formulation for example they consider the setting the diffusion process like T to the dimensionality of data and then they consider the forward process so that XT given X 0 places all probability mass on X 0 with the first T coordinates masked out and masks the Tth coordinate so this is specific formulation that they discuss here kind of shows that in a reformulation version of the this diffusion model training the P theta with this particular diffusion is kind of training and not too aggressive model lastly they also mentioned that you can interpolate source images so for example if you take X 0 and X 0 prime two images you can have Q as a stochastic decoder and interpolate these two interladen space meaning that you get the code of them we're kind of like bring them back to the sample to XT using the forward process of XT given X 0 and then you can linearly interpolate them and reconstruct them using the reverse process to reverse the diffusion process with correspondingly interpolated latent and get the result and that concludes the discussion on the DDPM paper now let's look at some of the generated images using this type of diffusion model this for example is an image of the interpolation",
        "output": "### Introduction to Denoising Diffusion Probabilistic Models (DDPM)\n\nDenoising Diffusion Probabilistic Models (DDPM) are generative models that take an image and iteratively denoise it by adding small amounts of noise. These models can generate semantically meaningful images, making them particularly useful in domains such as healthcare where supervised data may be scarce. Diffusion models fall into the broader category of generative models, alongside Variational Auto-encoders (VAEs) and Generative Adversarial Networks (GANs).\n\n#### Overview of DDPM\n\nDDPM involves training a Markov chain using variational inference to produce samples matching the input data after a finite time. The model aims to reverse the diffusion process by gradually adding noise and then reversing the information destruction process. The Gaussian conditional assumption allows for efficient neural network parameterization and training.\n\n#### Information Theoretical Analysis and Connection to Compression\n\nDDPM's high-quality image generation capabilities can be seen as a progressive decoding process resembling autoregressive decoding. The majority of the model's lossless code lengths describe imperceptible image details, making the model similar to a lossy compression scheme.\n\n#### ELBO: Evidence Lower Bound\n\nThe evidence lower bound (ELBO) is used to optimize parameterized models that describe the joint distribution of data points and their latent variables. ELBO is a variational bound on the log likelihood of the data, making it easier to compute than the original likelihood. The ELBO is calculated by finding the expected value of the log likelihood function over the generation process.\n\n#### Defining and Parameterizing Diffusion Models\n\nDiffusion models are defined as a Markov chain comprising a sequence of random variables from X0 (a semantically nice image) to XT (a noise from a prior distribution). The model parameters are optimized to represent this distribution, learning the true distribution of the images in the process. The variational bound can be computed using the forward and reverse processes and the Gaussian conditionals. Variance schedules (beta T's) can be defined as either learnable or fixed hyperparameters.\n\n---\nThe video notes above provide an overview of denoising diffusion probabilistic models, their applications, and the mathematics behind their optimization. The video also discusses the information theoretical analysis of the diffusion models and how they relate to compression schemes in the context of lossy compression. Additionally, the video covers the concept of the evidence lower bound (ELBO) and how it is used in optimizing the models. Finally, the video explains how diffusion models are defined and parameterized, touching upon concepts such as Markov chains, Gaussian conditionals, and variance schedules."
    },
    {
        "instruction": "<|SYSTEM|>You are a scholars that is taking markdown notes about some video, you would be giving some written scripts for the task.\nYou have finished writing the notes, and you want to extract the title from the notes.\nThe title should only be the name of the technology or algorithm, no extra words needed.\nPlease write the notes in markdown format. No need to write \"Title:\" for the Title. Make sure the title is within 10 words.\nHere is an example:\n## Hidden Markov Model\n<|ASSISTANT|>## ",
        "input": "<|USER|>### Introduction to Denoising Diffusion Probabilistic Models (DDPM)\n\nDenoising Diffusion Probabilistic Models (DDPM) are generative models that take an image and iteratively denoise it by adding small amounts of noise. These models can generate semantically meaningful images, making them particularly useful in domains such as healthcare where supervised data may be scarce. Diffusion models fall into the broader category of generative models, alongside Variational Auto-encoders (VAEs) and Generative Adversarial Networks (GANs).\n\n#### Overview of DDPM\n\nDDPM involves training a Markov chain using variational inference to produce samples matching the input data after a finite time. The model aims to reverse the diffusion process by gradually adding noise and then reversing the information destruction process. The Gaussian conditional assumption allows for efficient neural network parameterization and training.\n\n#### Information Theoretical Analysis and Connection to Compression\n\nDDPM's high-quality image generation capabilities can be seen as a progressive decoding process resembling autoregressive decoding. The majority of the model's lossless code lengths describe imperceptible image details, making the model similar to a lossy compression scheme.\n\n#### ELBO: Evidence Lower Bound\n\nThe evidence lower bound (ELBO) is used to optimize parameterized models that describe the joint distribution of data points and their latent variables. ELBO is a variational bound on the log likelihood of the data, making it easier to compute than the original likelihood. The ELBO is calculated by finding the expected value of the log likelihood function over the generation process.\n\n#### Defining and Parameterizing Diffusion Models\n\nDiffusion models are defined as a Markov chain comprising a sequence of random variables from X0 (a semantically nice image) to XT (a noise from a prior distribution). The model parameters are optimized to represent this distribution, learning the true distribution of the images in the process. The variational bound can be computed using the forward and reverse processes and the Gaussian conditionals. Variance schedules (beta T's) can be defined as either learnable or fixed hyperparameters.\n\n---\nThe video notes above provide an overview of denoising diffusion probabilistic models, their applications, and the mathematics behind their optimization. The video also discusses the information theoretical analysis of the diffusion models and how they relate to compression schemes in the context of lossy compression. Additionally, the video covers the concept of the evidence lower bound (ELBO) and how it is used in optimizing the models. Finally, the video explains how diffusion models are defined and parameterized, touching upon concepts such as Markov chains, Gaussian conditionals, and variance schedules.",
        "output": "## Denoising Diffusion Probabilistic Models"
    },
    {
        "instruction": "<|SYSTEM|>You are a scholars that is taking markdown notes about some video with Wiki style, you would be giving some written scripts for the task.\nAnd summarize each paragraph with a title.\nRemember to keep the technology or algorithm name, very important.\nPlease write the notes in markdown format.\nHere is an example:\n### Definiation\nA deadlock is a situation in which two computer programs sharing the same resource are effectively preventing each other from accessing the resource, resulting in both programs ceasing to function. \nThe earliest computer operating systems ran only one program at a time.\n\n### What it have to do with Relationsal Database ManagementSystem\nIn a database, a deadlock is an unwanted situation in which two or more transactions are waiting indefinitely for one another to give up locks. \nDeadlock is said to be one of the most feared complications in DBMS as it brings the whole system to a Halt. \nWhen you tries to insert or update data on database, sometime your record need to depend on the row record.  \n<|ASSISTANT|>#### ",
        "input": "<|USER|> Hello there. Look at these objects right here. What if I told you that I'm going to give you a bunch of pictures of these objects from different sides. And what you have to do is you have to come up with a system that generates me the picture as if the object was viewed from any direction. So something like this, right? Any direction, you can get me a picture of that object from just a few input pictures. This is a pretty daunting task specifically, look at the ship, for example, right here, you can see in the water, there's specularities that only appear if you view it from a very particular angle, right? Also the drum kit, you see that the microphone on the left, it has very specific structure to it. So this is not at all like a trivial task. Where there's very, there's very, very intricate things here. And this not only with toy data, but here you can see real world scenes. So this isn't some kind of abstract thing. You can actually use this in the real world. Now don't look at these things too long. They tend to make me dizzy. But that's ultimately the goal input a few pictures and then being able to synthesize any kind of view. So the paper we're going to look at, it's a bit of an older paper, but I think it's pretty cool. And it's relevant. And there is a bunch of follow up work to this. This is very popular right now. This is the paper introducing nerf, representing scenes as neural radiance fields for view synthesis. And it's by Ben, sorry, Ben Mildenhall, Pratul P. Srinivasan, Matthew Tuncik, Jonathan T. Baron Ravi Ramamurthy and Ren Eung. This as you can see, the task is called view synthesis. And what you can do with view synthesis, or with this paper specifically is you can, it can also it takes into account your viewing direction, which gives a much more realistic impression. We've already seen this with kind of the the lighting here. But in order to really show you this on the left, you're going to see this novel view that is rendered. And on the right, it's sort of like a fake thing that you couldn't do in reality. But what we're going to do is we're going to keep the camera at the same position, but we're going to tell the scene that the camera is at a like switching around. And that makes you able to see just how different a pic like a room can look like if viewed from different directions. So the right one is really kind of physically impossible. It's just meant to show you how different things look differently if they think they are viewed from a different direction, right. So the same thing here. And it just looks amazing. What you get automatically out of the systems are depth maps. These are notoriously hard to get, especially for complex scenes such as this one. Also this one right here. It's it's very complex, and it handles it fairly well. Sorry. You can even do something like AR right here, since you now have a representation that tells you how far everything is away and you have it from different views, you can see. Yeah, and you can even get meshes. So I should be able to move that around here. This is now a mesh. It's not only view synthesis, but you can actually fill out the voxels, which is a slightly different task. And if you have pictures from all around, you can synthesize kind of any view in between as you can see right here. So we're going to switch away from the fancy videos to the paper. Now the special thing about this paper and this is it's in the spirit of something like Sirens. So Sirens, we've I've made a video about it. And the special thing right here is it uses deep learning in a little bit of a different way than we would normally use it. So first of all, what does the abstract say? We present a novel sorry, a method what it is novel, that achieves state of the art results for synthesizing novel views of complex scenes by optimizing an underlying continuous volumetric scene function using a sparse set of input views. So the task description is the view synthesis, right synthesizing novel views. Also you're given a sparse set of input views. So you're given, you have a scene, let's say you have a tree or something like this. So here's a tree. I know beautiful. And you're given a bunch of images. So maybe someone, you know, stood here and took a picture. So the picture kind of views in in this direction, it pictures depicts the tree and someone stood here and took a picture of the same tree, maybe the same person, someone flew up here, took a picture of that tree. So you get a bunch of those, maybe you get 20 or something around the tree, maybe more, maybe less. So from these pictures, you want to build a thing that can generate any view from anywhere. And the way they do it is by optimizing an underlying continuous volumetric scene function. This is a cryptic way. But it goes along the direction of the sirens and kind of a bigger trend in, I think in the AI in these in these neural rendering papers, and so on, which is that we want to overfit a neural network to a single data point. This is really different from classic deep learning. If you know, if you ask someone, how would you go about this problem with deep learning, what they would tell you is, okay, I need a data set, I need a data set of these, you know, different scenes and the input. And I have my x and my y. So the input x is going to be always like, you know, 30 images of a scene, and y is going to be the scene itself, or whatnot, like the tree or the mesh of the tree or something like this. And I need this many, many times. So I need a data set with 30 images of, I don't know, a, a house, and the y is the house, and so on. So that's my training data set, then I might test data set, it can be something else, right? So it can be things that are now want to test. However, in this particular case, this is not the case. Here the it is one neural network that is fit to one scene. So what we have is a neural network that has a bunch of layers. And all the neural network cares about is this particular scene, right? If we want to render a new scene, we take a new neural network. That's what I mean, we overfit a single neural network to this particular scene, we use the 30 images or so we got to train to completely overfit this neural network. And the goal is going to be that the tree itself, like the scene itself is going to be in the weights of this neural network. So the weights of the neural network now represent the scene. And this has various advantages, right? If the we already saw this with the sirens, that very often, this is a much, much better representation more compact representation of the entire mesh than any other way, like if you store it in voxels or something. But I hope this is a bit clear. Now of course, the question is, what's the input and what's the output of this neural network? So the input is the following. Imagine you have a coordinate system here. So you get you get a coordinate system x, y, and z. Okay, and the neural network gets two things as an input, it gets as an input a position in that coordinate system, which we call we call x, and x is a is actually x, y, z is a three dimensional vector, right? For example, right here, this is our x now. And also we get an D, which is a viewing direction. Okay, so the for example, if my camera is the top camera right here, the viewing direction would be this ray here, well, everything's orange, and I make that blue. So the viewing direction, D would be that, okay, so that the angle here. We care about the angle. It's actually two angles, you need to describe this viewing direction. So a position and the viewing direction, and the output of the neural network, what does it output, the output of the neural network is going to be a color C, like what color is at that particular location, and the density, is there even something at that particular location, right? So the density tells you whether there is something or not. And if there is something, the color tells you what color it is. All right. This is a really different way. I want to stress that again of using neural networks, there is no longer images going in and you know, something coming out, what goes in is a position and the direction. So you ask the neural network, hey, neural network, you in your entirety, you represent the scene you represent, if you're trained well, if you're overfit, well, you're overfit on the tree. Now, I want to know at a particular location in this scene, viewed from a particular angle, what am I going to see? So on this picture right here, I'm wondering for this pixel, if I send array to this location, what am I going to see? And the network will tell you, you're probably not going to see anything, because there's nothing there. Or if there is something there, you're going to see the color, I don't know, red. Okay, so how from this, you can pretty easily get a picture, namely, if I have my frame of the picture, for each pixel, I need to send a ray through the scene. So I send a ray through the scene. And what I need to do is I need simply need to query this model at each location. So here, here, here, here, here, here, here, and so on. At each location, I will ask the neural network, is there something there? And if there is, what kind of color am I gonna, what am I gonna see? And what you'll get is a bit of a curve, thank you, is a bit of a curve. So if here is your zero, and you send the ray out into the scene, and this is the density going up, they have these graphs in the paper, by the way, I'm not, I'm not smart enough to come up with them by myself. But they say, well, maybe at the beginning, you're not going to see anything, because there's nothing there. But then, you know, at some point, you're going to see something, there is something there, you get you hit the tree, right? Then you're inside the tree, and then you're out of the tree again. At the same time, at every point, it gives you color. Now here, it actually doesn't matter what the color is, it will still output a color, but it doesn't matter. And here, it's going to say, green, right? It's going to say at every point here is going to say green, green, green, green. And here, I guess it doesn't matter, it's probably gonna say green as well. In any case, what you can now do is you can simply look at where do I hit the first time the object which is here, right when the density goes up, and what colors there and now I know what I need to render at that particular pixel. Now you can simply do this for all pixels, and you got yourself an image. And the neural network is powerful enough that for the same location, you can see this right here, it can give you different results depending on the different viewing directions. So that makes it such that it can kind of depend on where you view it from, it can capture these lighting effects, these reflections. And also it can capture transparency, because imagine you have a curve that is not as clear as this one, but you have a curve that is something like here. So here is a one wall of a glass and here is another wall of the glass. And they go up in density, but they're not fully dense, right? And the front of the glass is maybe blue. And the back of the glass is red. And now, if you integrate your ray along this, and you integrate weighted by the density, you're going to get a mixture of, you know, preferably blue, because that's in the front, but also a little bit of red, right? You can see that, like if a ray goes through here, you can handle transparency. And so this is a really powerful model, right here. And again, there's no need for a data set other than the date the scene that is right in front of you. So the goal is going to be that, if in the future, we want to, we want to make augmented reality applications, we want to make games, and so on, you are not actually going to store a mesh or kind of a voxel grid of some scene, what you're going to store is a neural network that can be queried from anywhere you want to look at the scene and the neural network will tell you what you're going to see, it just happens that these things work extraordinarily well. So here's the process. Again, the task, you get a set of input images, right here. You want to find out where they're taken from. So for each input image, you need to determine where was the camera and in which direction did it look, this is this is a known problem, you can so all these kind of classic also structure from motion, slam and so on, they need to determine the camera positions from the pictures. And so that that's a, that's a thing you can take from existing research. And then you want to render the new views. And yeah, here is I think where they get into it, where this is, yeah, we represent they say a continuous scene as a 5d vector valued function. And this vector function is going to be a neural network, it has a five dimensional input. And it has a the output is going to be a color, which is three dimensions and a density, which is one dimension. Okay, so the input is a 3d location, and a 2d viewing direction. And the output is a color and a volume density. So in practice, we express direction as a 3d Cartesian unit vector. And they say we approximate this continuous 5d scene representation with an MLP network. So the network, as we said, this is the input, this is the output, and we optimize its weights to map from each input 5d coordinate to its corresponding volume density and directional emitted color. Now the, the only question is, of course, we have these images, we don't actually we don't actually have, we don't actually have the as a training set, kind of the the, the densities at that place. So everything needs to be sort of grounded into the images that we have. Now luckily, the whole process that I've described here, which you see again here, so if you want to render an image, you take an image, you pick a pixel, you shoot array, and you sample along the ray, and you ask your network, what's there, the network will tell you if there's something there. And if so, what color, you're going to see the density over time. And then you can render an image. Now you if, if you already have an image, right, which is we are given a set of these images. If you already have one, you can now calculate a loss, namely, what do I see? And what does the network tell me I should see, right, if the network is not trained yet, that's going to be a pretty big loss. And if you make the loss as something differentiable, then this whole process is in fact differentiable. That's the next cool thing about this, the whole process of sending the ray, sampling the position integrating over it, and at the end coming up with a pixel color, that is a differentiable process if of course, if you do it correctly. But that means we can use those 30 images or 50 or whatever we have, in order to construct a big loss, right, every ray. So every pixel in every picture that we have defines array. So every ray essentially, is a data point that we can fit to. So at the end, we get a pretty sizable data set for the network, which is going to be number of pixels times number of pictures. However, again, it is a different problem than having a data set of many of these scenes. So the whole process is differentiable. And that means you can just fit the neural network to this scene, you overfit it to these 30 images that you have. And that's going to be your network. And this network then is going to represent the scene in its weights. So the weights are the scene at the end. There is a bit of a, so there are lots of engineering tricks here. So for example, we encourage the representation to be multi view consistent by restricting the network to predict the volume density as a function of only the location x, while allowing the RGB color to be predicted as a function of both location and viewing direction. So the reasoning here is that the volume density is not dependent on the direction like either, even if something is kind of transparent, it's going to be transparent, it's going to be the same transparent in from different direction, there's only very limited amount of materials where that is not the case, right. So as a simplifying concept, we're going to see the transparency of the object is always the same, which is kind of where stuff is, is independent of where you look from, it's only how stuff looks, that is dependent. So the RGB color is going to be a function of both location and viewing direction. And what they do is essentially so they input x, right here, they. So the location, they yank this through a network, they get out two things. So they first get out this density, and they also get out a hidden representation, that hidden representation, they then concatenate with the viewing direction. And that goes through another stack of layers in order to give them the color. I think it's also, you know, you could do something with a transformer here and some causal masking, though, I'm pretty sure someone has already done this, given that the paper is almost ancient at one year of age, in the machine learning world, that's really old. So that so this is the formula for new for rendering. This is a technique called volume rendering with radiance fields. If you have a radiance field, a radiance field is a function that tells you exactly what we train in our network to do namely, you know, if I look from here, and I look at that point, what do I see, what you want to do is you want to send a ray through the scene, and you want to integrate along that race, you have kind of a far bound and a near bound. And you want to integrate from the near bound to the far bound. So that means you send the ray through the thing you want to integrate. This thing, this T thing right here, that tells you, you can see the density is in here along the ray from the beginning to the point where you are, that is the probability that the ray doesn't hit anything, right? It's the probability that the ray goes on through that room, but basically, it's the probability of empty space. So or, you know, the inverse of that, like, this distinguishes whether there is something or not, whether the ray continues up until the point T or not. So you have whether or not the ray is actually at that particular point, how dense that particular point is, so how much stuff there is in terms of occludents for your ray. So if this is high, your ray is going to stop and you're going to adopt the color that is there, you can see it's this is multiplied by the color at that particular place. So you send the ray and as soon as your system determine, you know, there's something here, you're going to, since this is multiplied, the density is multiplied by the color, your ray is going to adopt the color of whatever's there. And then after that, this quantity here is going to be small, because this quantity is again an inner integral that tells you whether or not the ray even reaches that location. So the ray reaches the first location, at which point it's going to adopt the color. And after that, the it, even though there is stuff, right, even though the density is high, the ray is not reaching it. So the whole formula captures all of this. And as we said, with a bit of nuance, it like, if this is not always 01, it can handle transparency as well. And here they demonstrate again, from the scene. So you have two different points in the same scene, but viewed from different locations. And on the right, they show you, this is all the same point in the scene, but the circle represents kind of different angles at which you can view it from. And you can see that the color is really different depending on the angle where you look from. There are, what do we have here? There are a lot of tricks. Oh, yeah, so they, they approximate the integral with like quadrature, which also has existed. And they have a bunch of tricks. So the first trick to really get this to work is a novel, like not a novel, but kind of the employment of positional encoding, that a positional encoding is not the same as you might know it from transformers or something. The positional encoding here, it simply means that you send the input data point, which is this thing right here, XYZ theta phi Greek letter, you send that to a higher dimensional space, right, in a very deterministic way. So if you have these low dimensional input, and especially if you want to represent this, this is really fine structure right here, you can see that this stuff right here, it's quite fine grained, okay. And so you need a way to handle fine differences between things. But you also need a way to handle, you know, coarse differences. And just a single floating point number probably isn't going to do it for a continuous function like this. So what you do is you send this to a higher dimensionality with these positional encodings that we know from transformers. So these encodings right here, they will send. So what you do, and so in my video on attention is all you need, I explain those in detail, but you construct a hierarchy of sine waves, or like sine and cosine waves, but we can we can just do it with with sine waves. So the the lowest hierarchy is like this, and then the next thing in the hierarchy would be like double as fast. And then the next thing, well, this is four times as fast, isn't it? Well, you get the point, right? It's a, so I need up down, wow. And then up down, up, down, up. This is not a sine wave. But you, I hope you get the point. And then you want to take a look, for example, your x, you take your x, you put it here, like okay, x is so this is like, negative, I think they go from negative one to one, the coordinates they have, and your high dimensional output is going to be, you know, this point, this point, this point, and this point in the in their respective coordinate systems, right? So that's, you can, what this does is you can still clearly identify every point here. In fact, yeah, you can. You can identify every single point in your input space. By you know, looking at looking at the combination of where it is in the sine waves, but it gives the network a better chance to focus, for example, on details, if it wants to focus on details, is going to look at this scale right here, because tiny changes in the underlying x is going to result in a large change in this feature. If you want to focus on coarse grained stuff, then you look at this, where you can, you know, you have to move pretty far to have a change. Whereas if you look at this scale for coarse grained things, it means almost nothing. Because you know, if you want to make little difference between these two things, if you look at coarse grained structure, but they have, as you can see, like, there's a lot of difference between those like this may be zero, and this is maybe negative one. However, if you look at the two data points right here, sorry about that. So the same, let's say the orange distance and the blue distance, you can see that the two aren't so different in this representation. So it gives the network a choice at which scale it wants to look at for particular positions. So ultimately, you're going to map this five dimensional vector into a higher dimensional vector and they consider like 10, 10 layers or four layers of these, how many of these different sine wave and cosine waves they construct. So again, they call it positional ketting, they say, this is referred to as a positional encoding. However, transformers use it for a different goal of providing discrete representations as input to an architecture, yada yada yada. In contrast, we use these functions to map continuous input coordinates into a higher dimensional space to enable enable our MLP to more easily approximate a higher frequency functions. The second thing they do is they do hierarchical volume sampling. So when we said I send a ray through the scene, and then I sample along, this either would take a lot of time, or it would not be accurate enough. So what they do is they have actually two layers of neural network, one they call a course, and one they call a fine. And as I understand it, here is a ray, they first sample with the course one at rather course locations. And then they use that to evaluate where they should sample more. Let's say this thing right here has a real high density in the course network, they then sample around that a lot more, maybe one here too, but a lot more, you know, sampling around where the course network things the important stuff is. They optimize both networks at the same time. And that actually works out well. So here you see the loss, the loss is a combination now of the course network and the fine grained network. And you need to optimize both even though the final view is only going to come from the fine grained network, right? You need to optimize both because the coarse grained network can tell you where the important stuff is. So the results you have already seen, there are a bunch of metrics that prove that this one is really good. And it can, as you can see, like it can handle fine grained structure right here in the microphone that others can't. And it also so they say it fits into a few. So one neural network of one scene fits into like a few megabytes. And this is so it fits into five megabytes. And this is a lot better than things that use like voxel grid representations, which I think this other thing they compare to uses over 15 gigabytes for the same scene. Which and this is interesting, which is even less memory than the input images alone for a single scene from any of our data sets. So this is really like, it's even smaller than the pictures, right? So even if you maybe want to show this to another human, it'd be better you send the train nerve than the pictures if space is a consideration, though I don't know how they measure the pictures like you can probably compress if it's different pictures from the same scene. I guess there's some compression potential if you want to transmit them as a bond, never mind. So they'd also do ablations. And the only downside here is that it does take a long time to fit one of these neural networks. I don't exactly remember where they say it. But they say they calculate like, oh, here. So it's not too bad. But the optimization for a single scene typically take around 100 to 300 k iterations to converge on a single Nvidia v 100 GPU, which is about one to two days. So it's a single GPU. So it is, you know, you don't need a data center for it. But you're going to wait a while until you train one, though, you only need to train it once and then you can render new views as you please. Right. So the idea I think is going to be that, let's say you make a video game or so you're going to render this, you know, app your servers, then you transmit the neural network to the clients and the clients can just render it out right there. And yeah, there's a bunch of results and a bunch of ablations, where they kind of leave away different parts. And they show that especially kind of the positional encodings, I think this is the positional encodings are really important. As you can see on the right, there is no positional encodings. The view dependence is also quite important. You see if there's no view dependence, as you can see here, you do get the fine grained structure since you do have positional encodings, but you don't get these kind of light effects right this is this thing here is not a different color. It's simply the fact that the line shite light shines on it. And it's just not there here because, you know, all the network can do is in output the same color for all directions and most directions simply don't have that reflection. Alright, so that is it. The code is available on this website that I've showed you. I'm certainly going to link it. Tell me what you think. I think this is pretty cool. I know this has given rise to a lot of work following up on this. I have very little overview over what's going on in the nerve space. But I think it's cool and I want to dive deeper into it. Thanks for being here. Bye bye.",
        "output": "### NERF: Neural Radiance Fields for View Synthesis\n\n#### Overview\nThe paper, \"Representing Scenes as Neural Radiance Fields for View Synthesis\", introduces a method for synthesizing novel views of complex scenes by overfitting a neural network to a single scene, instead of using traditional datasets. The method, called NERF - Neural Radiance Fields, optimizes a continuous volumetric scene function using a sparse set of input views.\n\n#### Method\nThe input of the neural network is a 3D position and a 2D viewing direction. The output is a color and volume density at that location. The network is trained by shooting rays through every pixel in a given set of images, sampling positions along the ray, and calculating the difference between what is seen and what the network predicts to be seen.\n\n#### Benefits & Results\n1. A compact and accurate representation of the scene is stored in the neural network weights.\n2. The method can handle fine-grained structure, reflections, and transparency.\n3. The optimized network can generate new views quickly (once training is complete)\n4. The model can provide depth maps, AR, and actual meshes for complex scenes.\n\n#### Limitations\nTraining a single neural network for a scene can take 1-2 days on a single GPU, but it only needs to be done once. Once trained, the network can render new views as needed.\n\n#### Conclusion\nNERF is a novel method for view synthesis that overfits a neural network to a specific scene, optimizing a continuous volumetric scene function. This method is highly efficient and can handle complex information, such as fine-grained structure, reflections, and transparency."
    },
    {
        "instruction": "<|SYSTEM|>You are a scholars that is taking markdown notes about some video, you would be giving some written scripts for the task.\nYou have finished writing the notes, and you want to extract the title from the notes.\nThe title should only be the name of the technology or algorithm, no extra words needed.\nPlease write the notes in markdown format. No need to write \"Title:\" for the Title. Make sure the title is within 10 words.\nHere is an example:\n## Hidden Markov Model\n<|ASSISTANT|>## ",
        "input": "<|USER|>### NERF: Neural Radiance Fields for View Synthesis\n\n#### Overview\nThe paper, \"Representing Scenes as Neural Radiance Fields for View Synthesis\", introduces a method for synthesizing novel views of complex scenes by overfitting a neural network to a single scene, instead of using traditional datasets. The method, called NERF - Neural Radiance Fields, optimizes a continuous volumetric scene function using a sparse set of input views.\n\n#### Method\nThe input of the neural network is a 3D position and a 2D viewing direction. The output is a color and volume density at that location. The network is trained by shooting rays through every pixel in a given set of images, sampling positions along the ray, and calculating the difference between what is seen and what the network predicts to be seen.\n\n#### Benefits & Results\n1. A compact and accurate representation of the scene is stored in the neural network weights.\n2. The method can handle fine-grained structure, reflections, and transparency.\n3. The optimized network can generate new views quickly (once training is complete)\n4. The model can provide depth maps, AR, and actual meshes for complex scenes.\n\n#### Limitations\nTraining a single neural network for a scene can take 1-2 days on a single GPU, but it only needs to be done once. Once trained, the network can render new views as needed.\n\n#### Conclusion\nNERF is a novel method for view synthesis that overfits a neural network to a specific scene, optimizing a continuous volumetric scene function. This method is highly efficient and can handle complex information, such as fine-grained structure, reflections, and transparency.",
        "output": "## NERF -  Neural Radiance Fields"
    },
    {
        "instruction": "<|SYSTEM|>You are a scholars that is taking markdown notes about some video with Wiki style, you would be giving some written scripts for the task.\nAnd summarize each paragraph with a title.\nRemember to keep the technology or algorithm name, very important.\nPlease write the notes in markdown format.\nHere is an example:\n### Definiation\nA deadlock is a situation in which two computer programs sharing the same resource are effectively preventing each other from accessing the resource, resulting in both programs ceasing to function. \nThe earliest computer operating systems ran only one program at a time.\n\n### What it have to do with Relationsal Database ManagementSystem\nIn a database, a deadlock is an unwanted situation in which two or more transactions are waiting indefinitely for one another to give up locks. \nDeadlock is said to be one of the most feared complications in DBMS as it brings the whole system to a Halt. \nWhen you tries to insert or update data on database, sometime your record need to depend on the row record.  \n<|ASSISTANT|>#### ",
        "input": "<|USER|> Data structures have always been some of my favorite things in computer science. There is something really satisfying about turning a slow program into a fast one just by changing how you store your data. But there is one data structure I always found both fascinating and intimidating at the same time. When I learned about Fibonacci heaps, the first thing I thought was, how can anybody come up with that? A few seemingly unrelated ideas thrown together and suddenly you get a really efficient data structure. While Fibonacci heaps are certainly not the prettiest or most elegant data structure, they sure involve some really smart ideas that are worth the be appreciated. Which is why in this video, we will create Fibonacci heaps from scratch. Step by step we will look at all the key ideas necessary to invent an extremely clever data structure. And yes, we will also learn what on earth they have to do with Fibonacci numbers. What is the problem Fibonacci heaps try to solve? Let's say we are a network router and we need to handle thousands of messages per second. Some messages might be more important than others and need to be handled more quickly. So we give any incoming message a number based on its priority, called a key. Important messages get a small key like 1 and less important messages a larger key. We always send the message with the smallest key, meaning the highest priority first. A structure like this, where we only care about the smallest value at any point, is called a priority queue. Priority queues are really useful. They're used for example in Dijkstra's algorithm for finding shortest paths. By far the most common implementation of a priority queue is the binary heap. A binary heap is a binary tree where each node stores exactly one element with its key. Because it's a binary tree, every node has up to two children. But there are a few additional properties. First, every level stores as many values as possible, except for possibly the last one, which is filled from left to right. This way, we never use more tree levels than necessary. If we double the number of values in the heap, we only need one more level. This means the number of levels increases logarithmically. The second property is the so-called heap property. Every value is at least as small as its children. Therefore, the smallest value is always the root. This allows us to easily access the minimum element, which is exactly what we want from a priority queue. How do we insert a new element into the heap when a new message arrives? There is only one valid position that doesn't violate property 1. But we still violate property 2, because the new element is smaller than its parent. We can fix this by bubbling up the new element until property 2 is restored. What happens when we want to send the most important message? We'll call this operation extractMin. Simply removing the minimum leaves a gap, and there is no easy way to close it without violating our two properties. We instead swap the root with the rightmost element of the last level. This allows us to delete the element while maintaining property 1. We can fix property 2 by repeatedly swapping the new root with the smaller one of its children until it fits. One thing we might want to do is to change priorities of our elements. Maybe when a message hasn't been sent for a while, we want to give it a smaller key and therefore a higher priority. This operation is called decreaseKey. We can apply the same bubble up approach we use for inserting an element. Granted, to decrease a key, we first need to find it in the tree. But this is not too hard to do. You can imagine maintaining a hash table, in which for every value we keep a pointer to its position in the tree. decreaseKey turns out to be an essential part of many algorithms which use priority queues. We now know the four basic operations of priority queues. Accessing the minimum value can easily be done in constant time. We simply return the value of the root node. For the other operations, we might need to move one node through multiple tree levels, top to bottom for extractMin and bottom to top for insert and decreaseKey. In the worst case, we need to walk through every level, but we know that the number of levels only grows logarithmically, so the running times are logarithmic in the size of the heap. Logarithmic running time is pretty good. It's the running time we also have for something like binary search. But can we do any better? Let's imagine implementing the simplest, laziest priority queue possible. First of all, a priority queue is just a collection of values. We definitely want direct access to the smallest element. This allows us to implement getMin in constant time. To insert an element, we just add it to our collection. We then check if the new element is smaller than the current minimum and update it if necessary. And that's it. In theory, there's nothing stopping us from implementing insert in constant time as well. What about decreaseKey? As long as we get access to the element by using something like a hash table, we can just decrease its key in place. Then again, we only need to check if we have a new smallest element. This also seems doable in constant time. Things only start to get tricky when implementing extractMin. The problem is not deleting an element, but finding the new smallest element quickly, because remember, we want getMin to have constant time. To do this, our collection needs to have a certain structure that allows us to find the smallest element quickly. A binary heap has such a structure. The problem is, with a binary heap, we need to restore its two properties after every single operation, even insert and decreaseKey. So to improve the running times, we want a structure that is less strict and that allows us to be lazy, doing as little work as possible and only when necessary, especially during insert and decreaseKey. But it should still be strict enough, so we can find the smallest element quickly. The Fibonacci heap is exactly the kind of data structure with these properties. As the name suggests, it's based on the binary heap, but it has a much looser structure. The first thing we get rid of is property 1. We now allow trees of any shape. And they don't even have to be binary trees. Every node can have as many children as it wants. And why stop there? We even allow the heap to contain multiple trees. We store the trees in a doubly linked list which we call the root list. The children of every node will also be stored in a linked list. This allows us to quickly add and remove nodes. Additional references which point up and down in each tree allow us to easily access every value in the heap. The one thing from our binary heap we'll keep is the heap property. Inside every tree, values will only grow bigger the deeper we go into the tree. This means the smallest element of the heap is always one of the roots. We'll keep a pointer to it to quickly access it. Let's imagine we remove the smallest element. Which element is the next smallest one? Because each tree satisfies the heap property, we know it must either be one of the root nodes or one of the children of the previously smallest element. So for each extract min call, we will need to iterate over all these nodes to find the next minimum. This means we want to keep both the number of trees in the root list and the number of children per node relatively low. Keep this in mind for later. You might still be wondering what this has to do with Fibonacci numbers. Don't worry, we'll get to that. But let's first start implementing some actual Fibonacci heap operations. Implementing getMin is trivial, since we store a reference to the minimum element. But what about insert? Remember we want to be lazy. What is the simplest thing we can do to add a new element? That's right, we just add a new tree consisting of a single node to our root list, and we update the minimum if the new element is smaller. Great, we just implemented insert in constant time. But you might already spot a problem. Remember we want to keep the number of trees low, but adding a new tree for every element, basically it is the opposite. After adding a lot of elements, the next extract min call will need to check all of these nodes, because they're all potential candidates for the next minimum. And here comes the first clever idea of Fibonacci heaps. If the next extract min call after a lot of inserts is low, why not use that extra time to do some cleanup? We need to iterate over the entire root list anyway to find the next minimum, so while doing that, let's merge single nodes and small trees to form bigger trees. This reduces the number of trees in the root list significantly, and makes subsequent extract min calls a lot faster. Let's see how this be lazy and clean up the mess afterwards approach might look like in practice. Let's assume every insert call takes one unit of time and adds one node to our root list. As we've seen, every extract min call takes at least time proportional to the number of roots, plus maybe some extra time, say 10 units. But during that time, it will clean up the root list significantly, reducing the number of trees to let's say 5. This is what a typical sequence of priority queue operations might look like. A lot of inserts, a couple of extract min's and so on. When we look at how much time every operation needs, we see that after a lot of inserts, the next extract min will be slow because it needs to check every root that was previously inserted. But thanks to our cleanup, most calls are actually quite fast. Here's another way to look at it. What if we don't consider single operations, but the entire sequence as a whole? The first 100 inserts are directly responsible for the extra time of the next extract min. Maybe we shouldn't attribute that extra time to the extract min, but rather to the inserts, evenly distributing it among them. You can think of every insert paying in advance for the additional but constant work it will cause during the next extract min. The same of course also goes for the other inserts. Keep in mind that we're not actually changing anything about what each operation does, we're just looking at the entire sequence of operations, redistributing the total work. This kind of perspective shift is called amortized analysis, and it is key for understanding Fibonacci games. Now a single extract min never takes more than 15 time units, no matter how many elements were previously inserted. Sure, we need a bit more work per insert, but the total time for each insert remains constant. Of course the numbers 5 and 10 are only stand-ins for the maximum number of trees after each extract min, and the extra work per extract min on top of iterating over all roots. This is the actual time needed per extract min if we account for amortization. Let's see how we can implement extract min while keeping both these terms as low as possible. First we need to delete the minimum node, but if we did that, we would be left with some orphan child nodes. So let's first remove them one by one from the child list of the minimum node and insert them into the root list. This already requires some work, depending on how many children the minimum has. Remember when I said that we not only want to keep the number of trees low, but also the number of children per node. We start to see how this becomes important. From now on, let's call the number of children of a node its degree. In this example, all degrees are quite low, but we don't know yet how large node degrees might become as our heap grows. For now, let's assume there is a maximum degree, which gives us an upper bound for the work required in this phase. After removing the minimum node, we will start the cleanup phase. Here we reduce the number of trees by merging them. To merge two trees, we attach the one with the larger key to the one with the smaller key. We can't do it the other way around, because only this way we preserve the heap property. Also notice that the degree of the red tree increases by one, because we've added one child. But which trees do we merge? Here we want to keep node degrees as well as the total number of trees low. Let's just assume for now, that node degrees will in fact be low. Can you think of a merging scheme that guarantees that the number of trees will then also be low? The trick is to merge trees with the same degree until all trees have a different degree. So we allow at most one root with no children, one root with one child and so on up to the maximum degree. This ensures that including degree zero, the number of trees after cleaning up is at most the maximum degree plus one. We start by creating an intermediate array to keep track of root degrees. Again, we assume that we know the largest possible degree, so we create an array that contains one free spot for every possible degree. Then we iterate over the root list. Each tree is stored in one spot of the array depending on the degree of its root. If we already have a tree with the same degree, we will merge both trees just like we saw before. Remember that this increases the degree of the root by one. So we need to remove the tree from its spot in the array and move to the next one. Of course this spot might also be occupied, which requires us to do another merge. We'll keep merging trees until we eventually find a free spot, then we continue iterating over the root list. How much work do we need to do in this phase? We need to initialize the temporary array so it's large enough to fit every possible node degree. Iterating over the root list takes time linear in the number of trees. We also need to consider the time for merging the trees. Each merge requires constant time, and every merge combines two trees, decreasing the number of trees by one. This means the number of merges is at most as large as the number of trees, so the time for merging disappears in Big O notation. Now we can start rebuilding our heap. We iterate over the array and build up a new root list. It is during this phase that we also update the heap minimum. Because of the heap property, we only need to check tree roots. The total work for this phase depends on the size of the array, which again depends on the maximum node degree. So this is the total time we need for a single extract min. Remember, our amortized analysis showed that we can ignore the time spent for each tree, and that only the work on top of that actually matters. It looks like that extra work depends on the maximum node degree. Because our cleanup ensures that no two roots have the same degree, we can be sure that the number of trees after the extract min is also at most the maximum degree, so it all relies on the maximum node degree. If we can somehow show that node degrees generally stay quite small, extract min will be fast, and this is exactly what we're going to do. Let's see how large some tree with a given degree can be. A tree with degree 0, of course, contains just one node. Remember, we always merge trees that have the same degree. So let's merge two degree 0 trees to get a degree 1 tree. Merging two degree 1 trees gives us a degree 2 tree and so on. These kinds of trees have a special name. They're called binomial trees. Binomial trees have a few really cool properties. For example, children of binomial trees are just smaller binomial trees themselves, which makes perfect sense if you think about how we just constructed them. What did we do during extract min? We removed the minimum and added its children to the root list. Then we started merging trees with the same degree. If all trees before the extract min were binomial trees, all trees after the extract min will also be binomial trees. Maybe take a minute to convince yourself that this is true. Because we start off with a bunch of single nodes, this means that at any time, all trees and all subtrees in our Fibonacci heap will actually just be binomial trees. So what is the largest possible node degree? Maybe you've noticed already that binomial trees grow exponentially, as every tree contains twice as many nodes as the previous one. This means that every tree with degree d contains exactly 2 to the d nodes, or in other words, every tree with k nodes has degree log of k. What does this tell us about the maximum node degree? Let's say the entire heap contains 10 nodes. Then the largest possible degree is 3, because a degree 4 tree would already contain more nodes than the entire heap. The maximum node degree is therefore log of n, if n is the total number of values in the heap. Let's recap what we've done so far. We started off with a binary heap. We realized that in theory we could implement insert and decrease key in constant time by being lazy. So we built a new kind of data structure, where we implemented insert in the laziest way possible. Implementing extractMin needed a lot more effort, but we realized that by using the power of amortization, we could implement it only needing time linear in the maximum node degree. And we've just shown that the maximum node degree is log of n for a Fibonacci heap containing n elements. So we indeed managed to improve the insert running time while keeping extractMin the same. Now the only thing left is decrease key. Let's say we want to decrease the value 62. There are actually two possibilities. If the new value is still at least as big as its parent, we don't violate the heap property, so we don't need to do anything. But if the value is now smaller than its parent, we need to fix the heap property somehow. Of course, we could use the same bubble-up approach we used with binary heaps. This would actually work, taking at most logarithmic time. But remember, we want to be lazy. Can you think of an even simpler thing we could do instead? Well, we could just cut out the node and insert it into the root list. Root values can be arbitrarily low, so this does fix the heap property, and we can do it easily in constant time. We just need to remember to check if we have a new minimum. Now every decrease key call might add one more tree to the root list, which the next extract min needs to clean up. But this is basically just like insert, which means we can use the same amortization trick as before. You again can think of every decrease key call paying in advance for the additional but constant work during the next extract min. So we did it, right? We implemented decrease key in constant time. Actually, there is a pretty big problem with our implementation. Can you see it? Well, the running time of extract min relies on node degrees only growing logarithmically. But they only grow logarithmically, because all trees are binomial trees. If we allow decrease key to arbitrarily cut out nodes, then our trees are not binomial trees any longer. This is a big problem. Look at this heap. The tree has degree 4, and therefore contains 16 nodes. But let's say we cut out all these nodes. We can do this by decreasing all keys down to zero. After that, we can even remove them from the heap using extract min. Now the tree still has degree 4, but it only contains 5 nodes. While without decrease key, a tree with degree D always contained exactly 2 to the D nodes, now it could be as few as D plus 1. This means that the maximum node degree does not grow logarithmically anymore, but linearly, which of course completely destroys our extract min running time. So we are in somewhat of a dilemma. If we don't cut out nodes, decrease key will not be as fast as it could be. But if we do, there is a risk our node degrees grow too large, which slows down extract min. However, in practice, things don't actually look too bad. Situations like in the previous example, where a tree loses a lot of nodes, are probably quite unlikely. If we only cut out a few nodes here and there, our trees will still at least somewhat resemble binomial trees, and node degrees will remain low. But how can we be sure? This is where perhaps the most ingenious idea for banachia heaps comes in. A compromise. We only allow cutting out one child per node. Intuitively, this ensures we don't stray away too far from the original tree shapes, while still having enough flexibility to cut out nodes. Let's see how this looks like. Today we decrease this node's key to 19, which means we need to cut it out, just as before. We mark its parent 22, to indicate that it has lost one child. But what happens, when we also cut out this node? We know that 22 has lost two children now, because it's marked already. In this case, we cut out 22 as well. At first, this does not seem to make any sense. Shouldn't we cut out fewer nodes, not more? We think of it like this. We don't want trees with a large degree to contain few nodes. This tree with degree 4 starts to contain fewer nodes than it's supposed to. By cutting out 22 as well, we turn it into a degree 3 tree. This means it can now contain fewer nodes without posing a problem. Basically, cutting out children is fine, as long as we don't lose too many grandchildren. And cutting out nodes that have lost one child achieves exactly that. Once a node has been cut out, it doesn't pose a problem to its previous parent, so we can safely remove its marking. Notice that cutting out the parent node is recursive. This means there are situations like this one, where decreasing one key causes a bunch of nodes to be cut out. Believe it or not, this is how we can implement decreaseKey in constant time while extractMin remains fast. But maybe you're not convinced yet. Which makes sense, because there are a few things about this approach I haven't addressed. We sometimes cut out more than one node during a single decreaseKey. So how is it possible we still only need constant time for a decreaseKey? And if we cut out more nodes, which we need to clean up later, how can extractMin still be fast? And how can we be sure that node degrees only grow logarithmically? I've only made a few vague claims so far about not straying away too much from binomial trees. These are the questions I want to answer in the final section of this video. And yes, this is also the part where the Fibonacci numbers come in. We will start by dealing with the first question. Let's say we perform 10 decreaseKeys. What is the maximum number of nodes we need to cut out in total? Here are a few things to help answer this question. When we cut out a node, it's either because we decreased its key, or because it's marked. During each decreaseKey call, we mark at most one node. And once we cut out a marked node, we remove its marking. In the worst case, we need to cut out every node whose key we decreased. On top of that, we mark up to 10 nodes, which means we might need to cut out up to 10 additional nodes. So we need to cut out at most 20 nodes in total. In general, for k decreaseKeys, we need to cut out at most 2k nodes. What we just did is basically another amortized analysis. While a single decreaseKey might be slow, if we consider all decreaseKeys at once, on average every decreaseKey cuts out at most 2 nodes and therefore only takes constant time. This insight allows us to answer the second question as well. Every insert call added one tree to the root list, so we thought of every insert paying in advance for cleaning up this extra node during the next extractMin. We now know that every decreaseKey call on average cuts out at most 2 nodes, and therefore adds two trees to the root list. So every decreaseKey needs to pay for cleaning up 2 nodes. Sure, this is some extra work per decreaseKey, but it's only a constant amount of work. So decreaseKey's running time stays constant. Just like before, accounting for amortization, the running time of extractMin only depends on the maximum node degree. And this brings us to question 3. We need to somehow show that with our decreaseKey implementation, node degrees only grow logarithmically. If we can do that, extractMin will take logarithmic time. Let's recall how we previously showed that node degrees grow logarithmically. We managed to show that every tree with degree d contained exactly 2 to the d nodes, so trees grow exponentially with their degrees. This implied that the maximum degree can only grow logarithmically in the size of the heap, because any tree with a larger degree would already contain more values than the entire heap. So the actual question is, do trees grow exponentially? With decreaseKey, things are a bit more complicated, because our trees don't have a fixed shape anymore. What we need to do is to look at the smallest tree for any given degree and show that even that worst possible tree contains at least exponentially many nodes. With exponentially, I don't necessarily mean 2 to the n. Actually, any base larger than 1 would be fine. This is because in Big O notation, all logarithms behaved the same, no matter their base. Let's look at this tree. We will call it x. The size of x depends on the degrees of its children y1 to y5. Higher degrees would mean more grandchildren, great grandchildren and so on, therefore a larger tree. With our initial decreaseKey implementation, every child could have degree 0, but now things are different. Recall then when merging trees, we always added the new child at the right. This means that y1 is actually the oldest child of x, while y5 became a child of x most recently. Focus on y4. What is the smallest degree it could possibly have? At the time y4 became a child of x, x had at least 3 children, namely y1, y2 and y3. Maybe it had even more, which were cut out later, but it had at least these three. Remember, we always merge nodes with the same degree. So when y4 became a child of x, it too had at least 3 children. What could have happened to y4 since then? This is where our cutoutAtMostOneChild rule comes in. Since then, y4 could have lost at most one child, because if it had lost more than one, we would have cut it out, and it wouldn't be a child of x anymore. This means that y4 must have at least 2 children, or in general, yi, the ith child of some node, has at least degree i-2. I know this was a lot to grasp, so feel free to rewatch this part if you didn't get it right away. Now we can actually construct the smallest possible tree with a certain degree. Degrees 0 and 1 are straightforward, because there is an obvious smallest tree. Starting from degree 2, we can use our new knowledge about child degrees. For example, the fourth child of the smallest degree 4 tree must have at least degree 2. To make it as small as possible, we will pick the smallest possible degree 2 tree, and we already know how it looks like. Do these numbers look familiar? These are the Fibonacci numbers, the famous sequence that starts with 0 and 1, where every falling number is the sum of the previous two. The only difference is that our tree sizes start two terms into the sequence. But why do the Fibonacci numbers suddenly show up? Let's look at the rightmost tree. Its first 4 children look just like the previous tree, and its fifth child looks just like the tree before that, or in other words, its size 13 is the sum of the sizes of the previous two trees, so it really shouldn't be that surprising that the tree sizes are just the Fibonacci numbers. But how does this help us answer our question? Probably the most well-known fact about the Fibonacci sequence is that the ratio of two adjacent values in the sequence approaches this number, better known as the golden ratio of phi. If you want to know why, I can recommend this video from Numberphile. But this means that every Fibonacci number is about 5 times as large as the previous one, and this is just exponential growth. What if the Fibonacci numbers grow exponentially, and the sizes of the smallest possible trees are just the Fibonacci numbers offset by 2? This means that the tree sizes must also grow at least exponentially. In fact, you can show that any tree with degree d must contain at least phi to the d nodes using a simple proof by induction. So the answer to our final question is a definite yes. And there we have it. It may have been a long and perhaps a bit convoluted process, but with the final operation decrease key, we are finally done. We managed to create a priority queue, where every operation except extractMin only takes constant time. And it turns out that even extractMin is basically optimal, because of the so-called lower bound for comparison-based sorting, but this is a topic for another video. If Fibonacci heaps are so good, they're probably used quite a lot in practice, right? Well, not really. In theory, Fibonacci heaps do improve the running times of certain algorithms compared to binary heaps. But in practice, the inputs to these algorithms must be huge, before using such a complicated data structure like Fibonacci heaps is actually worth it. But if Fibonacci heaps aren't actually that useful, was all we did in this video for nothing? I don't think so. First, mathematicians think about things that have no practical purpose all the time, so why can't we do the same in computer science? And second, while many concepts in math or computer science are presented as these ingenious, most beautiful ideas, Fibonacci heaps show that more often than not, solutions to specific problems are messy, and require many iterations to get just right.",
        "output": "### Introduction to Fibonacci Heaps\nFibonacci heaps are an advanced data structure used in computer science to improve the running time of certain algorithms, such as Dijkstra's algorithm for finding shortest paths. They are based on binary heaps, but with a looser structure that allows for faster insert and decrease key operations.\n\n### Data Structure and Priority Queue\nFibonacci heaps are implemented as a collection of trees called the root list, with each tree satisfying the heap property—every value is at least as small as its children. The smallest value in the heap is always at a root node, and a pointer to that node is maintained for constant-time access to the minimum element.\n\n### Insert Operation\nThe insert operation for a Fibonacci heap is simple: create a new tree with a single node and add it to the root list. Update the minimum pointer if necessary. This operation runs in constant time due to its simplicity.\n\n### Extract Minimum Operation\nThe extract minimum operation requires finding and deleting the root node with the minimum value, reinserting its children into the root list, merging trees with the same degree, and updating the minimum pointer. Amortization analysis allows for a running time proportional to the maximum node degree, which is logarithmic in the size of the heap.\n\n### Decrease Key Operation\nThe decrease key operation involves cutting a node with a decreased key value out of the tree and inserting it into the root list, and fixing the heap property if necessary. It also allows for cutting out at most one child per node to ensure node degrees grow logarithmically.\n\n### Final Analysis\nFibonacci heaps have constant time operations except for extract minimum; however, they are not commonly used in practice due to their complexity. Despite this, they demonstrate an interesting exploration of advanced concepts in computer science and data structure optimization."
    },
    {
        "instruction": "<|SYSTEM|>You are a scholars that is taking markdown notes about some video, you would be giving some written scripts for the task.\nYou have finished writing the notes, and you want to extract the title from the notes.\nThe title should only be the name of the technology or algorithm, no extra words needed.\nPlease write the notes in markdown format. No need to write \"Title:\" for the Title. Make sure the title is within 10 words.\nHere is an example:\n## Hidden Markov Model\n<|ASSISTANT|>## ",
        "input": "<|USER|>### Introduction to Fibonacci Heaps\nFibonacci heaps are an advanced data structure used in computer science to improve the running time of certain algorithms, such as Dijkstra's algorithm for finding shortest paths. They are based on binary heaps, but with a looser structure that allows for faster insert and decrease key operations.\n\n### Data Structure and Priority Queue\nFibonacci heaps are implemented as a collection of trees called the root list, with each tree satisfying the heap property—every value is at least as small as its children. The smallest value in the heap is always at a root node, and a pointer to that node is maintained for constant-time access to the minimum element.\n\n### Insert Operation\nThe insert operation for a Fibonacci heap is simple: create a new tree with a single node and add it to the root list. Update the minimum pointer if necessary. This operation runs in constant time due to its simplicity.\n\n### Extract Minimum Operation\nThe extract minimum operation requires finding and deleting the root node with the minimum value, reinserting its children into the root list, merging trees with the same degree, and updating the minimum pointer. Amortization analysis allows for a running time proportional to the maximum node degree, which is logarithmic in the size of the heap.\n\n### Decrease Key Operation\nThe decrease key operation involves cutting a node with a decreased key value out of the tree and inserting it into the root list, and fixing the heap property if necessary. It also allows for cutting out at most one child per node to ensure node degrees grow logarithmically.\n\n### Final Analysis\nFibonacci heaps have constant time operations except for extract minimum; however, they are not commonly used in practice due to their complexity. Despite this, they demonstrate an interesting exploration of advanced concepts in computer science and data structure optimization.",
        "output": "Fibonacci Heaps"
    },
    {
        "instruction": "<|SYSTEM|>You are a scholars that is taking markdown notes about some video with Wiki style, you would be giving some written scripts for the task.\nAnd summarize each paragraph with a title.\nRemember to keep the technology or algorithm name, very important.\nPlease write the notes in markdown format.\nHere is an example:\n### Definiation\nA deadlock is a situation in which two computer programs sharing the same resource are effectively preventing each other from accessing the resource, resulting in both programs ceasing to function. \nThe earliest computer operating systems ran only one program at a time.\n\n### What it have to do with Relationsal Database ManagementSystem\nIn a database, a deadlock is an unwanted situation in which two or more transactions are waiting indefinitely for one another to give up locks. \nDeadlock is said to be one of the most feared complications in DBMS as it brings the whole system to a Halt. \nWhen you tries to insert or update data on database, sometime your record need to depend on the row record.  \n<|ASSISTANT|>#### ",
        "input": "<|USER|> In contrast to traditional shutter-based cameras, event cameras record asynchronous events which are per-pixel changes in brightness. This results in high dynamic range, lack of motion blur, low latency, improved data bandwidth and less power consumption. Event cameras have recently been applied to many computer vision problems. This includes optical flow estimation, video interpolation and deblurring, 3D pose estimation, geometry reconstruction and many more. However, existing event-based 3D reconstruction approaches can recover only sparse, colorless point clouds of the scene. Such sparsity is a limiting factor in many cases, especially in computer vision and graphics, and it has not been addressed satisfactorily so far. The research question we address in this work is whether just an event stream from an event camera moving around the scene is sufficient to reconstruct a dense volumetric 3D representation of that static scene in the absence of RGB images. Accordingly, we propose the first approach for 3D consistent, dense and photorealistic novel view synthesis using just a single short color event stream as input, which we call event nerve and describe next. Note that even though we use only the event stream as input, our results are rendered in the RGB space. On the left we have an input event stream. On the right we have a volumetric scene model. To this end we adopt neural radiance fields or NERF for short, which is a state-of-the-art coordinate based volumetric 3D scene representation. So how do we connect these two modalities? We propose using the following equation, where the left side can be computed completely from the events and the right side can be computed as the difference between two rendered views. Hence we can compute the accumulated difference by summing up the polarities from the events. And we can compute a similar difference from a volumetric model by subtracting two rendered views and applying a biofilter. We supervise our model by comparing these two differences, making them converge to each other as the model is trained. If we just sample rays randomly, as in the traditional NERF, the model will either converge slowly or not converge at all due to the sparsity of events. Hence we propose a corresponding ray sampling strategy which allows training event nerve efficiently and consistently. First we split the whole event stream into intervals. We pick a random one as the window end. And then we need to choose the window length. We notice that using constant short windows results in poor propagation of high-level lighting and using constant long windows often results in poor local detail. Hence we sample the window length randomly as well. Now in every event window rays are sampled from the positions where at least one event happened. However if sampling is limited to non-zero rays only, called positive sampling, information about pixels with no events will be ignored which would degrade the results. Hence we also apply negative sampling which means that we sample 10% of the rays from every other pixel in the frame. This makes our method more robust to noise events and leads to better reconstruction of the uniformly colored surfaces and thin structures. Now let us proceed to the experiments. As the baseline we propose the following simple approach. We start with the event stream and run it through a frame based reconstruction method such as e2vid to obtain RGB frames that we can use to supervise a traditional nerve. However there are many drawbacks compared to purely event based approaches such as ours. The asynchronous nature of events is ignored. Having to store all the reconstructed frames drives up disk and memory usage and there's a limit on the window size and the total number of views due to the frame reconstruction method. Here we show performance on synthetic data. We show that our method completely outperforms e2vid plus NERF and can reconstruct a variety of view dependent effects, thin structures and complex geometry. We recorded all real data using the following setup. We placed the object on a vinyl turntable, rotating at constant speed and recorded the rotating object with our event camera. Please note that the resolution of our event camera is only 346 by 260 pixels so the predictions here are greatly upscaled. All our real sequences were recorded under low lighting conditions using only a 5 watt USB light. Here we show the original simultaneous RGB frames of the event camera. For clarity we tone map these frames when they are used in the rest of the video. Note that event NERF does not use this data and only uses the events. In the fake plant and sewing sequences the model can reconstruct every stem and leaf of the plant and even the one pixel wide needle of the sewing machine. In the microphone sequence it reconstructs even the mesh of the microphone similarly to the synthetic case. In the goatling and controller sequences we also show handling of high dynamic range by reconstructing both the dark details in the controller and in the fur and highlights in the glasses. In the cube and bottle sequences we show that we can reconstruct sharp color details. In the chicken and dragon sequences we show that event NERF recovers details in fuzzy colorful materials as well. In the multimeter sequence we show that the model can reconstruct view dependent effects such as specular highlights. As with the synthetic sequences our event nerve significantly outperforms our baselines on real data. Replacing e2vid with SSL e2vid leads to even worse results as SSL e2vid it doesn't support color, mostly uses only the information inside the event window and its performance is usually bounded by e2vid. We also compare against the blur NERF which is an RGB based NERF method designed specifically to handle blurry RGB frames. We perform this comparison by applying the method on the simultaneous RGB frames of our event camera and simulated blurry frames of synthetic sequences. Results show that our approach significantly outperforms de-blur NERF while only using events as input. This is expected as de-blur NERF can only handle view inconsistent blur. In contrast our approach produces almost blur free results. We now proceed to an ablation study using fixed long windows instead of randomized window lengths results in loss of fine detail on the object and some minor artifacts and trails. Disabling negative sampling results in significant artifacts. Now we discuss data efficiency. By varying the event generation threshold we can control how much data there is without changing the duration of the event stream. Note that at thresholds high enough some details never trigger a single event and hence cannot be reconstructed. As we can see our model successfully reconstructs the scene even from 3% of the data which is equivalent to less than one regular RGB view. This is possible because even this small amount of data contains all the information from all of the camera movement. Hence we can still use multi view consistency to reconstruct the scene. As our model is based on NERF we can apply marching cubes to its density volume to extract the textured mesh. Thus our method can be used to reconstruct detailed textured 3d meshes of the objects. Also our event NERF approach is not bound to a particular volumetric representation. To show this we replace our original NERF++ backbone with Torch NGP. And as a result this model can be trained in just one minute and then rendered in real time on a single consumer GPU. We believe that this makes our method future-proof and opens up many exciting applications. For example fast object scanning and reconstruction which can be used on the factory floor. Object recording only takes a second and reconstruction takes just a minute. We are going to publicly release our code and data in the near future. Thanks for watching!",
        "output": "### Introduction to Event Cameras\n\nEvent cameras record asynchronous events, which are per-pixel changes in brightness. This results in high dynamic range, lack of motion blur, low latency, improved data bandwidth, and less power consumption. They have been applied to numerous computer vision problems, such as optical flow estimation, video interpolation, and 3D pose estimation.\n\n### 3D Reconstruction with Event Cameras\n\nCurrent event-based 3D reconstruction approaches can only recover sparse, colorless point clouds of the scene. The research question addressed in this work is if just an event stream from an event camera moving around a scene is sufficient to reconstruct a dense volumetric 3D representation of that static scene in the absence of RGB images. The proposed approach is called Event NERF.\n\n### Neural Radiance Fields (NERF) \n\nNeural Radiance Fields (NERF) is a state-of-the-art coordinate-based volumetric 3D scene representation. The researchers adopt NERF to connect the event stream and volumetric scene model.\n\n### Ray Sampling Strategy for Event NERF\n\nThe ray sampling strategy involves splitting the event stream into intervals, picking random window ends and lengths, and applying positive and negative sampling. This strategy allows for efficient and consistent training.\n\n### Experiments and Results\n\nEvent NERF shows improved performance over the baseline method (E2VID + NERF) on both synthetic and real data. This includes handling high dynamic range, reconstructing sharp color details, fuzzy materials, and view-dependent effects.\n\n### Ablation Study\n\nUsing fixed long windows instead of randomized window lengths results in loss of fine detail and minor artifacts. Disabling negative sampling results in significant artifacts. However, the model demonstrates data efficiency by reconstructing scenes using as little as 3% of the data.\n\n### Mesh extraction and Torch NGP integration\n\nEvent NERF can be used to reconstruct detailed textured 3D meshes using marching cubes. The method can also be integrated with Torch NGP for real-time rendering on a single consumer GPU.\n\n### Conclusion and Future Applications\n\nEvent NERF shows promise in various applications, such as fast object scanning and reconstruction on factory floors. The code and data will be publicly released in the future."
    },
    {
        "instruction": "<|SYSTEM|>You are a scholars that is taking markdown notes about some video, you would be giving some written scripts for the task.\nYou have finished writing the notes, and you want to extract the title from the notes.\nThe title should only be the name of the technology or algorithm, no extra words needed.\nPlease write the notes in markdown format. No need to write \"Title:\" for the Title. Make sure the title is within 10 words.\nHere is an example:\n## Hidden Markov Model\n<|ASSISTANT|>## ",
        "input": "<|USER|>### Introduction to Event Cameras\n\nEvent cameras record asynchronous events, which are per-pixel changes in brightness. This results in high dynamic range, lack of motion blur, low latency, improved data bandwidth, and less power consumption. They have been applied to numerous computer vision problems, such as optical flow estimation, video interpolation, and 3D pose estimation.\n\n### 3D Reconstruction with Event Cameras\n\nCurrent event-based 3D reconstruction approaches can only recover sparse, colorless point clouds of the scene. The research question addressed in this work is if just an event stream from an event camera moving around a scene is sufficient to reconstruct a dense volumetric 3D representation of that static scene in the absence of RGB images. The proposed approach is called Event NERF.\n\n### Neural Radiance Fields (NERF) \n\nNeural Radiance Fields (NERF) is a state-of-the-art coordinate-based volumetric 3D scene representation. The researchers adopt NERF to connect the event stream and volumetric scene model.\n\n### Ray Sampling Strategy for Event NERF\n\nThe ray sampling strategy involves splitting the event stream into intervals, picking random window ends and lengths, and applying positive and negative sampling. This strategy allows for efficient and consistent training.\n\n### Experiments and Results\n\nEvent NERF shows improved performance over the baseline method (E2VID + NERF) on both synthetic and real data. This includes handling high dynamic range, reconstructing sharp color details, fuzzy materials, and view-dependent effects.\n\n### Ablation Study\n\nUsing fixed long windows instead of randomized window lengths results in loss of fine detail and minor artifacts. Disabling negative sampling results in significant artifacts. However, the model demonstrates data efficiency by reconstructing scenes using as little as 3% of the data.\n\n### Mesh extraction and Torch NGP integration\n\nEvent NERF can be used to reconstruct detailed textured 3D meshes using marching cubes. The method can also be integrated with Torch NGP for real-time rendering on a single consumer GPU.\n\n### Conclusion and Future Applications\n\nEvent NERF shows promise in various applications, such as fast object scanning and reconstruction on factory floors. The code and data will be publicly released in the future.",
        "output": "## Event NERF"
    },
    {
        "instruction": "<|SYSTEM|>You are a scholars that is taking markdown notes about some video with Wiki style, you would be giving some written scripts for the task.\nAnd summarize each paragraph with a title.\nRemember to keep the technology or algorithm name, very important.\nPlease write the notes in markdown format.\nHere is an example:\n### Definiation\nA deadlock is a situation in which two computer programs sharing the same resource are effectively preventing each other from accessing the resource, resulting in both programs ceasing to function. \nThe earliest computer operating systems ran only one program at a time.\n\n### What it have to do with Relationsal Database ManagementSystem\nIn a database, a deadlock is an unwanted situation in which two or more transactions are waiting indefinitely for one another to give up locks. \nDeadlock is said to be one of the most feared complications in DBMS as it brings the whole system to a Halt. \nWhen you tries to insert or update data on database, sometime your record need to depend on the row record.  \n<|ASSISTANT|>#### ",
        "input": "<|USER|> Would you believe me if I told you that this video right here is AI generated? But it absolutely is. In this video, I am going to give a breakdown of the technology behind training models like these and some intuition on how they work. Without further ado, let's get into the first episode of Neural Breakdown. So from a 30ft view, this work uses a neural image rendering algorithm called Neural Radiance Fields, lovingly known as NERF. NERF takes in a handful of 2D images of a 3D scene taken from different angles and then trains deep neural networks to synthesize new unseen images from arbitrary camera positions and angles. This video is produced by a cool new variant of NERF called ZipNerf. Now there is some background needed to fully understand the nuances of this paper but I will try my best to explain it concisely and not dive into too much high math stuff but a basic understanding of how neural network works will definitely help you to understand this. You are finding me for the first time. I am a new YouTuber and I like to make videos on Machine Learning, AI and Game Development in Unity. I will be making more of these AI breakdown videos on this channel along with my usual devlogs. So, if any of these interests you, please leave a like and subscribe. You are magnificent. Now let's now zoom in a bit and understand how NERF works. Imagine you want to be able to store a 3D scene in a way that you can render 2D images of the scene from any arbitrary camera position and angle. A brute force way to do this would be to just keep a 3D voxel based grid representation of the scene and you can use an algorithm like ray marching to shoot out rays into the scene from each pixel and then calculate its color depending on which voxels it intersected. Sadly, this scales badly to large scenes and storing complex volumetric data like this requires a lot of memory, often in terms of gigabytes of memory. This is where neural networks come in. The original NERF paper from 2020 trained a fully connected dense neural network that inputs the X, Y, Z coordinates as well as the rays direction as theta and phi and outputs the RGB color and the density at that position. The density basically means how much content is present at that position. If it's empty, the density will be zero. If it's fully opaque, it will be one. If it's like semi-transparent like glass or something, it'll be between zero and one. It is worth noting that training a NERF is very different from traditional machine learning algorithms where we usually have a large data set which we train our models on and then we use it for inference on new data. With NERF, we start with the data set of 2D images of a 3D object and then we try to overfit that data on a single randomly initialized neural network. Well, it's not always single. Sometimes there are two neural networks involved, but you get the idea. So what you really end up doing with these neural networks is have a super compressed representation of this entire 3D scene and store it instead of in 5GB, you store it in 3MB or something. So now you have a way to query the density and color at a given coordinate. How do you now render an image? You shoot out rays from each pixel of your image and to determine the color of the ray, NERF does something called the hierarchical volume sampling. First, it samples multiple coarse points separated out widely along that ray and evaluate each point using a neural network they call the coarse network. For the values that the coarse networks say are high, NERF does a second round of important sampling around those locations and evaluate it using a second neural network called the fine network. This allows NERF to save up on computation because the first round of sampling that is coarse and more sparse will usually rule out areas that are empty and most of the computation can be focused on the important sampling regions with the fine network. Another important trick that NERF does is instead of directly passing in the input coordinates as float values into the network, NERF trains positional encodings that map those coordinates into high dimensional high frequency vectors. They derive these encodings using combinations of sine and cosine waves very similar to how positional encodings work in transformer papers if you're familiar with them. This work revolutionized the research field at the time, but it still had a bunch of limitations which people are working on fixing and improving ever since. There were issues like these jagged lines around the edges, also known as aliasing artifacts, and also issues with rendering multiple objects and at different scales in unbounded scenes, and above all really slow training times. So how did we go from this to that? In 2022, NVIDIA Labs published their amazing paper on Instant NGP that sped up NERF training from a few hours to a few seconds. They introduced three factors to achieve this ginormous leap in speed. Optimizing training with CUDA and GPU drivers, training very small MLP layers, and third and most interestingly, they introduced a novel idea of multi-resolution hash encoding. So they divide up this 3D volume into multiple spatial grids with each of a different resolution, and they maintain a hash table of vector encodings for each of the corner of these grids. They have some grids that are spaced apart that capture the global information about the scene, and there are some that are more finely spaced that capture very minute details in the scene. And then to find the embedding of any query point along a ray, they would look at all the grids that that point intersects with and use the hash table to look up the encodings of the corners of these grids. And then they will do a linear interpolation on those embeddings to produce an average embedding for each resolution. This method allows some amazing stuff that's even difficult to comprehend, like, you know, gigapixel image approximation. Like if you look at this image and you keep zooming in, it never stops giving and it's training and inferencing all of this in real time, which is just absolutely crazy. It's a truly amazing and a seminal piece of work. And I'll definitely link some presentations below and the paper if you want to check it for yourself. So the ZipNerf paper tries to improve on this hashing idea. They claim that the original INGP method still causes some aliasing artifacts because the network is only evaluated at a single point. Like you're creating a single point and you're creating the nearest neighbors of that point and then you're collapsing all of those embeddings, which is an infinitesimally small point. What the ZipNerf paper proposes instead is that it sends out a cone into the scene instead of a ray. For each conical ray, we will want to average out all the content within the visible section of the cone. This idea of casting cones instead of rays is actually from a previous paper called MIPNERF by the same guys, which I'll share in the description. So they sample a number of points around a spiral like this and then treat each of these points as a Gaussian. The final encoding is then derived using the hash encodings of the corners these Gaussian intersects with. The encoding contributed by each corner is also down weighted by how much that Gaussian actually fits within that cell. So this whole thing achieves two things. We are taking multiple samples to calculate the colors, which reduces the noise in the image because you're super sampling. And this acts as an anti aliasing strategy, but at the cost of longer inference time and running slower than INGP. Second, because the encodings now depend on the width of these Gaussians and the spread of the cone gets larger as it moves away from the camera, the network now has a sense of scale which was missing previously. Pretty smart. And you can see the comparison between the two methods right here. So the next issue the authors point out is that of Z aliasing. Z aliasing occurs when a ray misses an object in the scene depending on the camera's distance from the object, introducing these tearing artifacts. To address this, the authors introduce a concept called proposal supervision. In an earlier paper called the MIPNERV 360, the authors had replaced the system of maintaining a coarse network and a fine network with instead training a proposal network and a NERF network. Now this is going to get a little confusing, but hang on. The proposal network is a small neural network that is queried on a bunch of coarse points and outputs just the densities for each query point along the ray. The NERF network is a much larger neural network that would then sample multiple points along the high weighted regions identified by the proposal network. So the NERF network is trained with the supervised loss with respect to the ground truth pixel color, whereas the proposal network was trained to output distributions that are as close to the NERF network as possible. To design a loss like that is not trivial because it is not straightforward to apply a loss between two differently spaced histograms. Intuitively, the way they optimized this proposal network was so that each bin it outputs has a lower value than the sum of all bins that overlap in the NERF model's distribution. Thus the proposal loss is only optimized if the proposal network's values exceed the sum of the overlapping bins returned by the NERF network. This makes the densities output by the proposal network not smooth with respect to the scene's density and the authors believe that is what is causing the model to skip scene content. So the authors fix this by a neat signal processing trick. They convolve the NERF distribution to form this piecewise linear probability density function like this and then they compute the cumulative distribution function of this and basically slice up this cdf by the proposal network's bin endpoints. Finally, by taking the difference between the adjacent points, they reconstruct a new histogram that now have the same bin endpoints as the proposal network. Now that's some hunky-dory stuff but now these two distributions have the same coordinates and they can just employ a simple element-wise loss to train the proposal network to output distributions similar to the NERF network. And that's a hell of a trick. So with that, they have most of the ingredients for ZibNerf. It's truly an amazing piece of technology. I couldn't believe my eyes when I saw it for the first time. Yes, it is an order of magnitude slower than the instant and GPU method mainly because of the multi-sampling approach that ZibNerf takes but the results are so consistent and that just makes it so beautiful. There are a lot to talk about regarding this paper and this research area but I will leave you guys here with some paper links and some presentations I found helpful when I was trying to understand this. I hope you guys enjoyed this video. Don't forget to subscribe.",
        "output": "### Introduction to AI-generated Video and Neural Radiance Fields\nThis video explains the technology behind AI-generated videos using a neural image rendering algorithm called Neural Radiance Fields (NERF) and its variant, ZipNerf.\n\n### NERF: Rendering 3D Scenes from 2D Images\nNERF takes a set of 2D images of a 3D scene and trains deep neural networks to synthesize new images from various camera positions and angles. It uses a fully connected dense neural network to input XYZ coordinates and ray directions to output RGB colors and density at a given position. The end result is a highly compressed representation of the entire 3D scene.\n\n### Rendering Images with Hierarchical Volume Sampling\nNERF uses a hierarchical volume sampling method, which involves shooting out rays from each pixel, evaluating them using a coarse network, and then focusing on important sampling regions with a fine network. Positional encodings are used to train the neural networks, allowing for more efficient image rendering.\n\n### Limitations of Original NERF and Improvements with ZipNerf\nThe original NERF had limitations such as aliasing artifacts and slow training times. ZipNerf, a variant of NERF, addresses these issues and improves the overall image quality and performance of the neural networks.\n\n### NVIDIA Labs' Instant NGP: Speeding Up NERF Training\nIn 2022, NVIDIA Labs introduced Instant NGP to speed up NERF training by optimizing it with CUDA and GPU drivers, training small MLP layers, and using multi-resolution hash encoding. This method allows for real-time training and inference, as well as gigapixel image approximation.\n\n### ZipNerf: Casting Cones Instead of Rays and Addressing Z Aliasing\nZipNerf further improves on the hashing idea by casting cones instead of rays into the scene, allowing for super sampling to reduce noise and anti-aliasing. Additionally, it addresses Z aliasing by introducing proposal supervision, a technique that optimizes the proposal network to reduce tearing artifacts in the final rendered images.\n\n### Conclusion\nZipNerf is an incredible piece of technology that produces high-quality AI-generated videos with consistent results, albeit at a slower pace than instant NGP. This research area continues to advance, pushing the boundaries of AI-generated content and neural image rendering."
    },
    {
        "instruction": "<|SYSTEM|>You are a scholars that is taking markdown notes about some video, you would be giving some written scripts for the task.\nYou have finished writing the notes, and you want to extract the title from the notes.\nThe title should only be the name of the technology or algorithm, no extra words needed.\nPlease write the notes in markdown format. No need to write \"Title:\" for the Title. Make sure the title is within 10 words.\nHere is an example:\n## Hidden Markov Model\n<|ASSISTANT|>## ",
        "input": "<|USER|>### Introduction to AI-generated Video and Neural Radiance Fields\nThis video explains the technology behind AI-generated videos using a neural image rendering algorithm called Neural Radiance Fields (NERF) and its variant, ZipNerf.\n\n### NERF: Rendering 3D Scenes from 2D Images\nNERF takes a set of 2D images of a 3D scene and trains deep neural networks to synthesize new images from various camera positions and angles. It uses a fully connected dense neural network to input XYZ coordinates and ray directions to output RGB colors and density at a given position. The end result is a highly compressed representation of the entire 3D scene.\n\n### Rendering Images with Hierarchical Volume Sampling\nNERF uses a hierarchical volume sampling method, which involves shooting out rays from each pixel, evaluating them using a coarse network, and then focusing on important sampling regions with a fine network. Positional encodings are used to train the neural networks, allowing for more efficient image rendering.\n\n### Limitations of Original NERF and Improvements with ZipNerf\nThe original NERF had limitations such as aliasing artifacts and slow training times. ZipNerf, a variant of NERF, addresses these issues and improves the overall image quality and performance of the neural networks.\n\n### NVIDIA Labs' Instant NGP: Speeding Up NERF Training\nIn 2022, NVIDIA Labs introduced Instant NGP to speed up NERF training by optimizing it with CUDA and GPU drivers, training small MLP layers, and using multi-resolution hash encoding. This method allows for real-time training and inference, as well as gigapixel image approximation.\n\n### ZipNerf: Casting Cones Instead of Rays and Addressing Z Aliasing\nZipNerf further improves on the hashing idea by casting cones instead of rays into the scene, allowing for super sampling to reduce noise and anti-aliasing. Additionally, it addresses Z aliasing by introducing proposal supervision, a technique that optimizes the proposal network to reduce tearing artifacts in the final rendered images.\n\n### Conclusion\nZipNerf is an incredible piece of technology that produces high-quality AI-generated videos with consistent results, albeit at a slower pace than instant NGP. This research area continues to advance, pushing the boundaries of AI-generated content and neural image rendering.",
        "output": "Neural Radiance Fields and ZipNerf"
    },
    {
        "instruction": "<|SYSTEM|>You are a scholars that is taking markdown notes about some video with Wiki style, you would be giving some written scripts for the task.\nAnd summarize each paragraph with a title.\nRemember to keep the technology or algorithm name, very important.\nPlease write the notes in markdown format.\nHere is an example:\n### Definiation\nA deadlock is a situation in which two computer programs sharing the same resource are effectively preventing each other from accessing the resource, resulting in both programs ceasing to function. \nThe earliest computer operating systems ran only one program at a time.\n\n### What it have to do with Relationsal Database ManagementSystem\nIn a database, a deadlock is an unwanted situation in which two or more transactions are waiting indefinitely for one another to give up locks. \nDeadlock is said to be one of the most feared complications in DBMS as it brings the whole system to a Halt. \nWhen you tries to insert or update data on database, sometime your record need to depend on the row record.  \n<|ASSISTANT|>#### ",
        "input": "<|USER|> Hi there, today we're looking at implicit neural representations with periodic activation functions by Vincent Sitzman, Julian NP Martel, Alexander W Bergman, David B. Lindell and Gordon Wettstein. So this paper is a bit of a special paper. If you're like me coming from like classic machine learning or deep learning, things like this, this paper requires you to think around your, your notion of what it means to handle data and so on a bit and to think about data points, and so on. Essentially what they're doing is they are representing signals such as images or sound or generally waves or point clouds. They're representing these signals as functions mapping, for example, from their coordinates to their values, and we'll see what that entails. And they, they're not the first ones to do this, but they managed to do this very well using these new models called sirens, which are basically neural networks that so sirens spelled like this neural networks that have sine waves as their nonlinearities instead of like Rehlu's or hyperbolic tangents and so on. So it turns out that if you initialize these very carefully, those can be made to capture these signals very, very well. So that's the kind of high level overview. And we'll go through the paper in a bit of a fashion of someone that is not in this particular literature. So this is not going to be like as in depth or technical as usually because I myself am not super familiar with with this kind of literature with the neural representations and so on. So if you go at this paper from a machine learning perspective, the first like you're going to be ultimately super confused at the beginning. So I'm going to try to kind of clear up retrace my steps of my confusion. Okay, so I love that this paper starts out at we are interested in a class of functions Phi that satisfy equations of the form this right here, like, aren't we all like, we are interested in a class of functions. Okay, I've never, you know, particularly had many dreams about functions like this, but so how can you how can you look at this? So we're interested in the relation between inputs and outputs. This here is the function, as you can see, this maps input to output, okay. And we're also interested in its derivatives. So here you go, first, second, third derivative, and so on. So this function right here is what we're going to call a neural representation or an implicit representation. It's called a neural representation, I guess, if it's a neural network that this function is so, so far so good. You've seen this, right? You've seen this could be a data point, and then could map it to a label or something like this. Since we're going to represent images, you already know maybe a GANs, a generative adversarial network, where this here is the latent vector, and then you have a neural network mapping this latent vector to an image, right? So this is going to produce an image. This here is quite similar, but not quite. So in a GAN, I guess this here would count as the representation, the continuous representation of this picture. However, in this case right here, the function itself is the representation. So in a GAN, what we do, we learn this right here, this function phi, we learn this from data such that if I plug in one particular vector, I get one particular image. And if I plug in another vector, I get another image and the function always stays the same. Here it's going to be one function per image. So each image, the function is the image. So how is a function an image? You can simply, if I have an image, and it's made of pixels, right? So it's made of pixels. Each pixel has an x and the y coordinate. Okay, let's call that x one and x two, the coordinate of that. And each pixel also has a color value, right, which is three dimensional. So each pixel has a three dimensional RGB color value. So technically, an image is a function from coordinates to pixel values. Okay, so if if if this is my image is represented by a function, then if I input any coordinates, like three, four, that function should return, what are the RGB values at that maybe it's like 0.5, 0.7, and 0.1. Those are the RGB values at that. Okay, so now the goal is to have this right here, be a neural network where I have like a multi layer perceptron. And they, I think they always use like a five layer MLPs. So really simple neural networks. And you simply input. So here you have two input neurons where this here goes. So one gets the three, one gets the four, then this travels through the network. And at the end, the network should output three output nodes. And this should be like the 0.5, 0.7, 0.1. Okay. Now, again, this network here is, they now train this network to map input to output, okay, to map coordinates to values. And this, of course, it is one particular image. So you're going to have one neural network per image. Now, you might reasonably ask why do we do it like this, right? Why don't we just save the image as the pixel values? Why do we need like a function mapping the coordinates to the pixels? And that's a valid question, I guess. And the image is just one example of this. But one advantage that you immediately get is that now you have a continuous representation. So now you cannot not only do you know, because if you store an image like this, you only know its value at each of the pixel locations. However, if you store an image like this, you know its value at any continuous in between location, right? So you can ask the network, what's the pixel value at 3.2 and 4.1, right? It will give you an answer. And if the network is trained, well, it will give you sort of an answer that makes sense. That is, what's the exact color at this sub pixel location right here? Now, so far, so good, right? So essentially, this boils down to not really a machine learning problem in the classic sense, but an optimization problem, because all you have to do is you have to make the neural network match all input to all output, there's not really a training and a test set right here. Namely, your data set is going to be all the pixels in the image. So each pixel in the image is going to be one data point, because it's one. So each pixel is x, y, 2 RGB. Okay. The way they train these networks, and the examples of pixels, the way they train it, they simply sample a mini batch of pixels like this one, this one, this one, this one, this one. They use that mini batch to train the network to do one step to train the network, and then they sample another mini batch and so on. You might sample the same pixels multiple times. But ultimately, what you want is sort of a continuous representation of the image that there, this is not a new idea. And this has been around and they cite a lot of literature where this has been around before. So what their new thing is, is that they say these other representations, so if you use a neural network in a classic sense like this, and you do your training with the mini batches like this, what you'll end up with is a bad image. So if you then, if you then simply go, right, once you've trained the network, you can take it and take your network, and you can simply output each pixel location. So you say, okay, now I'm going to reproduce this image using my network, because if it's had trained well, it certainly give me back the positions at the pixels. So you ask it, what's 00? What's 01? What's 02? What's at 03? And you can fill in the picture and that usually gives you very bad outcomes or so they claim. I mean, I haven't checked it particularly. But you can see right here, this is the ground truth. And the here you have a network that is parameterized with ReLU functions like with ReLU nonlinearities. And as you can see, the ReLU network misses a lot of the sort of higher definition things in the image. And so it depends on the architecture that you use how well you can make a neural network represent those things. Again, you kind of need to forget what you what you know about machine learning in the classic sense because like I'd still see people who you're going like, just use again or something like this. So yes, valid point. But we are in the business right now of, of solving this particular problem. And as we'll go on to see, it's not just about images. But images are a nice example of a natural signal. So the tanh networks you also see they I think they fail even harder to have these artifacts back here even. And this here, it gets better when you do ReLU networks with what is called a positional encoding. So not only do you have your x and your y coordinates go through a ReLU network, but you also have them go through a positional encoding. And that's very much like in like you would have in a transformer. So if you watch my video about attention is all you need, I explain how the positional encodings work there. And basically what you do is you map these things to cosine and sine waves. So you're going to be like the sine of x times 10. And then the sine wave of x times 100, and so on. So which you'll end up and you do the same for y. And that ends you up with more features that sort of then the function can use to represent positions way better than just given the x and y coordinates. If you do that, you kind of recover some of the image. But you see here, they also analyze how so this is the ground truth. And this is the gradient of the ground truth, which is basically a Sobel filter, if you know that it's basically an edge detector, color gradient thing. And then this here is the second derivative, the Laplacian of the image. And ideally, if your implicit representation models the signal very well, it should also model the derivatives of the signal very well. So now we're kind of connecting it to what we saw at the beginning, right? These siren networks are specifically designed to not only match the signal right here, but also match its derivatives. And if you match maybe in an image, it's not so it's not that important to match the derivatives even though it is because there are small things. Like you can see right here, the grass isn't as well represented. And here you mostly you get some artifacts that you see here in the in the gradient might not be as important for images in terms of human vision. But for many signals, it's also important to match the derivatives. And here at the siren, even though it's trained on the image itself, you can see that its derivatives are very much in line with the original signal. So simply by matching the signal, you this architecture manages to also capture the derivatives of the signal and therefore have a more faithful representation. Okay, so that was positional RBF ReLUs are simply the ReLU network. And I think somewhere in here, there is an RBF kernel. If you young kids don't know what an RBF kernel is, then yeah, no, I guess I guess I don't want to I don't want to dunk on anyone. It's basically you how do I explain it? You map it to an infinite dimensional space using Gaussian kernels. Yeah, maybe Wikipedia is better at that than I am. So sirens, what what do they do in order to be able to capture a signal very well? What do how does it sing a siren different from like an RBF network? And the answer is pretty, pretty, pretty simple. So the architecture of a siren network is the end does it already stand for network? I'm not sure, honestly, maybe we'll find out. Yes, it's the sinusoidal representation networks. So the end is network. So we don't say siren network, we say siren. And a siren is simply made of what is that? Here, it's a multi layer perceptron, basically, right. So it is a, this here is the network, the network, this is the final layer of the network, which is a linear layer. Before that you have all these layers, just concatenate, but following each other. So it's a multi layer perceptron, pretty regular. And each of the layers in the multi layer perceptron is made up like this, you have an input, you multiply it by a weight matrix, you add a bias, and then you put it through a sine wave. So the sine wave here is really, that's, that's the only change from a from a MLP otherwise. So usually here you have something like a sigmoid or a relu function. Now you have a sine wave. And the I mean, it's a bit weird, right? Because a relu function is like this. So it has this center thing where it kind of switches, but here, it's linear and monotonic. And here, it's kind of constant, and even a, even a sigmoid. So the sigmoid is, I don't even remember like this. Yes, I guess. So the sigmoid is like this. So it's kind of constant here, constant here, monotonic, and so on. We're used to monotonic activation functions, whereas a sine wave is really different. The sine wave, of course, is something like this, right? It's not monotonic at all. Like if you, if you want to increase your function value at any point, and you're here, and you go up the hill, and you do a step that's too large, you end up down the hill again. But it turns out that these, these networks have particularly, have some good properties if you want to capture natural signals. And they have some bad properties, namely that the fact that they are periodic and go down again. And the reason why they get around the bad properties is because or so they claim they initialize the network in a very particular fashion. Because I think at least I when I when I started in deep learning, I had this idea. So a lot of other people must have had this idea too of like, hey, what if I just replaced a nonlinearity with like my sine function? Could I do something this and then tried it out and it didn't really work. So I scrapped that. Now this here, of course, isn't simply replacing the neural network, it's also using the neural network for something completely different than I would, namely, it's using the neural network to learn these implicit representations and not like I would to do simply for learning a data set. But still, it seems like you need to initialize them fairly with with very careful consideration. And we'll go on onto that right now. So actually, they just describe it. It's not like a, it's not very interesting, but you need to sample the weights uniformly from this uniform distribution, where I think, yeah, and they have a proof in the supplementary material where they sort of show why that is. So or not here, we propose to draw weights with C equals six, such that W is in this uniform distribution right here. Oh, no, it's different. Okay. This ensures that the input to each of the sign activation is normal distributed with a standard deviation of one. Since only a few weights have magnitude larger than pi, the frequency throughout the sine network grows only slowly. Finally, we propose to initialize the first layer of the sine network with weights so that the sine function spans multiple periods over negative one to one, we found W zero to equals 30 to work well, for all the applications in this work. The proposed initialization scheme yielded fast and robust convergence using the atom optimizer for all experiments in this work. So the initialization here takes a fairly prominent piece in that paper, which tells me maybe that they have spent a lot of time working on this. And this is, I mean, if this is the case, this is to their credit, because I guess most people like me would try out something like this, and then after a while realize it doesn't work. And to, you know, be so convinced to go and really figure out how do we need to initialize these to make it work. And of course, as you're doing this, there's still like a 99% chance that it's not going to work once you've done that is quite respectable. I find it might have been really different. This might have been the first thing they thought about and just worked it out. But yeah, okay. So what is the deal with all these derivatives? Now, since this network right here has the sine waves in it, right, so it's a neural network with sine waves as derivatives as nonlinearities. What now so we have a neural network, what now is the first derivative of that neural network, right with respect to its input, so we have an input. Now what's the first derivative with respect to its input? And the cool thing about this is what's the first derivative of a sine wave? It's of course, a sine wave that's shifted. So it's a cosine, which is a sine wave that's simply phase shifted. And then the next derivative again, is a shifted sine wave, and so on. So the derivative of a siren is a siren. And that does not hold for any of these other nonlinearities. So in ReLU's, it's the derivative of a ReLU network is like a con. So if I take the derivative of this, it's like a constant zero right here, and then a constant one right here. And if I then take the derivative again, it's simply a constant zero function, right. And all these other nonlinearities, their derivatives are different from themselves. And here, since we want to not only match a signal, but also the signals derivatives, these property of this siren become in very, very, very handy. So how do you train a siren? We've already alluded to how you would do that in the in the kind of idea of matching an image where you simply train the pixel values to the RGB values, but there's more that you can do with the sirens given that they basically given that their derivatives are also sirens, what you can do. So with the image part, we've basically neglected all of this, we simply said, we want to find a relationship between the input x and the output. Like this, what we can also do is we can say no, no, no, no, no, we want to find a relationship between the input and its first derivative, and not even have this as part of the, let's say of the loss function. And then we can see what comes out. So that's what they do. Can I find it? Can I find it? That's what they do right here. Okay. So here you see the the ground truth image. And this is its gradients. And this is its laplacian. Okay. Now, we've already seen that we can fit the image itself. But what if we just fit the first derivative, so we simply input this thing right here, we input this into the siren, we do the same thing, right? The siren is now it maps x and y to RGB. But our loss function isn't going to be mapping x and y to RGB, our loss function is going to depend on the gradient of that. So our loss function is going to be something like the gradient of the image, let's call the image i minus the gradient of that function that maps x of this function right here. Okay, because we have these auto differentiation tools right now, we can easily make this into a loss function. So here we are looking for the function whose gradient matches the gradient of the image, right? Now, again, you can say why is this? Why can't we just match the image itself and I valid point, but it's not about why can't we just it's about demonstrating the power of these networks. So if you only match the gradients, right, what you'll find is, if you then look at the function, right, you still find the function you don't, you don't find the gradient, you still train the function, you still train the weights of the function itself. But the loss function depends on the gradient of that function. If you do that, you'll find that if you then look at the function, again, you can ask the function to produce the image by simply cycling over each of the coordinates. You'll find that look at that, just by matching the gradient, you'll match the image itself pretty, pretty well. Right. And that's pretty cool. Now of course, you're not going to match the RGB values, this is a grayscale image. And you know, there's a there's kind of a reason for that. Because since the gradient loses, like constant bias information, so what if you'd match an RGB image, I'm going to guess you're going to have like color, very much color distortions. But and here, what you're going to have in this case is just distortions in luminosity. If you know that if you have a function, if you have the derivative of a function, and you will want to find the function itself, and you integrate, then the solution is always an entire space of functions because you will integrate the function this thing right here. So with the whatever its input is, and you will have to add a constant and you don't know what the constant was in the original function, because when you derive the function, the constant drops away. So similarly, here, what we'd expect is that the image that we're getting back will be faithful with respect to like its its borders, right, since we're matching the gradient, and the gradient is basically an edge detector will match the sort of edge information of the picture, which you can clearly see, but what we would expect is some difference in overall luminosity. And I don't even know how they exactly did this, because they now have to choose a constant to add. Maybe they just chose it in some way, or maybe they just let the network do. But this is, you know, still pretty, pretty impressive. You can see there's some detail missing, but not much. And the same exact same thing you can do for matching the second derivative. And now you match the Laplacian of the image. And remember, in the ReLU networks, they don't even have a Laplacian, it's a constant. So this is something you could never do. And you can see that the outcome image is still pretty good, right? This is now missing the constant luminosity in the first and second derivative, sorry, in the in the zeroeth and first derivative, and still the information is, the reconstruction is pretty good. Alright, so this demonstrates kind of the power of these networks. Again, we're not having our data set, our entire data set is just this image. So if we fit something, then this thing right here is our entire data set. There's no, there's no big data set. And this is a test sample like this is the data set and the test sample at the same, I guess you can consider the Laplacian here, the data set, and then the actual image is the test sample, like the label or something like this. So what does that buy you? Here is a thing you can do. If you want to mix two images, what do you do? So if you want to mix this and this, what you could do is linearly interpolate. But that would be not very cool. Because right here, you have a lot of like, very bright pixels, which probably have like values of one. And here you'd have the dark pixels, which probably have values like more close to zero. And the if you simply mix them, if you simply add them together and divide by two, then you'd get kind of get a wash of the two. And similarly, here, you kind of wash out the bear, because you'd have some pixel values here that would come over. And generally not not a good idea to mix images like this. Now, you know, with GANs, we can do this. But we have to have like a training data set and so on. Here, what we'll say is we'll simply say, we'll take the gradient of this, and we'll take the gradient of this. And then we'll add the two gradient maps. Now what this does is that, as you can see right here on the left is the composite gradients. And what this does is, right here in the sky, there is no gradient information in this image because it's just a flat patch of sky, right? So and down maybe down here, there's not that much gradient information, there is a bit right but not here. So that's where this bear head is. And if you want to mix images, like it can be a good idea to mix their gradients, because generally the information in an image is where the gradients are. So what we would expect the gradient to represent the gradient would carry over this portion, maybe carry over a bit of this portion, it would carry over this portion and this portion. So everything where the signal is not flat. So here you can see the composite gradient. And if we fit, again, we fit our function such that the gradient of the function that we fit matches this mixed gradient right here, then this is the gradient of the function that we match. And this is the actual function. And you can see, pretty pretty good, right? It basically mixed everywhere where there was gradient. And this is now just reconstructed from this gradient, there is no, I think there is no, as least as I understand it, there is no pixel information carried over from either of those images, they're simply added to this gradient, the gradient is fit. And then the function is asked to output pixel value at each location. And that's that. Okay. So this is just a simple, you know, thing that you can play around with. But they do they do other more interesting things right here. For example, this representing shapes with signed distance functions. So if you go over the formulation, the actual formulation of their loss function, we haven't actually done this right quite yet. It's here. It's very complicatedly stated. But ultimately, what this means is, so a component right here is are these Cm, which are constraints. So this loss function operates on these constraints. And the constraints are across a of x, which basically is just x, it's kind of a the anything depending on the input itself, then the output of the function, the gradient of the output of the function, the second derivative, third derivative, and so on. So this, these sirens can fit anything that you can formulate as a set of constraints that relate the input of the function, right here, to its output or any of its derivatives. And we've already seen that at once we if we fit an image, our only constraint is that these things match right here with the original image that the coordinates are mapped to the RGB values, then when we match the gradients, we don't care about this, we only care about the relation between this and so on. So the loss function is literally just over the entire signal space, which in our case was was over the entire image. We want these constraints to hold or to be as small as possible, right, the constraints are always formulate, such that if they are fulfilled, they equal zero. And so the for example, the L2 loss between the RGB values of the true image and the RGB values that you fit the RGB loss, sorry, the L2 loss would be a constraint like this. And of course, the more differentiable you make it, the more the easier this network has at fitting it, right. So that's why there is this norm right here. But it's not that complicated. It simply says, whatever you can formulate as a constraint on relating the inputs to the outputs or any of the derivatives of this implicit representation, that is the loss function. All right, so the in the next interesting thing we can do, as I said, is representing shapes with signed distance functions. So we're going to go slowly. And this is, yeah, it's not that hard. Inspired by recent work on shape representation with differentiable signed distance functions, SDFs, we fit SDFs directly on oriented point clouds, using both ReLU based implicit neural representations and sirens. Okay. So what is an SDF a signed distance function? That's pretty easy. Signed distance function is simply a distance function with a sign, like wow. So a, if you have a and it's usually done if you have like a boundary somewhere between things, then of course, any point here has a distance to the boundary. But you if you have a signed distance function simply means that each point also has a sign in front of it. And that means all the things on one side of the boundary maybe have a plus and all the things on the other side maybe have a minus. So even though two points could be the same distance from the boundary, one is like plus five away and one is negative five away. And you can do this. This is useful, for example, when you fit point clouds as they do in this example. So when they have point clouds, and that's usually in 3d space, but if you have point clouds, you basically have points right here. And you know that the points should represent some kind of shape, maybe a wall or so they have these room interiors, as you can see, right here. So this is a 3d scene, but you only have a point cloud of the 3d scene. And what that means is that maybe you were in this room, and you put up a laser scanner right here, laser scanner, I don't know how a laser scanner looks. And the laser scanner kind of shoots lasers at random locations and always measures the distance, right. And that's that's how you end up with a point cloud. So you'll end up with like a point cloud, where in 3d space, you know where the laser hit something. And the reasonable assumption to make if you have like a dense sampling of this is that you should be able to like connect those point clouds in some way to obtain the actual continuous shape of the thing that you measured. And this is what we're going to try to do with these sirens, right, to go from point clouds to shape by training an implicit representation. So we're going to train a neural network that represents this shape right here, basically by mapping coordinates to signed distance values. So whenever we ask the neural network, what at this location here, what's the signed distance? And it's going to tell us, oh, it's plus five, or at this location here, what's the sign distance, it's going to tell us it's zero, right. So we're going to, we're going to train a neural network to do that. And hello, yes, no. Okay. So this is a bit more complicated. And since we have these awesome power of the sirens, we can also do two more constraints. So we know, and this goes on, this amounts to solving a particular iconal boundary value problem that constrains the norm of spatial gradients to be one almost everywhere. So this iconal boundary value problem, this is a property of signed distance function, that the norm of the gradients with respect to the input is one almost everywhere, almost everywhere means everywhere, I guess, except at the boundary itself where the distance is zero. So I could be wrong. Note that ReLU networks are seemingly ideal for representing SDFs, as their gradients are locally constant, and their second derivatives are zero. Adequate training procedure for working directly with point clouds were described in prior work, we fit a siren to an oriented point cloud using a loss of the form. And now we'll look at the loss. So the first thing you observe in the loss is that it is made of three different integrals. And that simply means they now partition the space right here, they partition it into two different, they partition it into two different regions, so to say. So maybe go here. No, can I zoom here. So the first region is going to be whatever is on the boundary itself, right. And that's basically wherever a point, wherever a point hit, right, whenever you have a point, or on the boundary itself, that's going to be your omega zero is going to be that. And then all the other points right here are going to be part of your omega without the omega zero. So you're going to have different constraints for all of these things right here, for example, and I'm have to pay attention that I don't say anything wrong. You will have this this constraint of this gradient, my tablet, I mean, maybe I'll start monetizing just so I can get a new tablet. Okay, so no, okay, the this this condition right here says that the gradient should be one and that's actually everywhere, right. So I was wrong that the gradient is only one outside the boundary. Then you can see right here. The last part is all the points that are not on the boundary, since our network maps any point in 3d space to assign distance function. So most of these points aren't going to be on the boundary itself, even though in the mini batch where we train where they train, they sample points on and off the on and off the boundary at the at equal rates just to to have the network train more stably. So this is a condition on all the points off of the boundary. And they say here, this function is this exponential function with alpha larger than one, it penalizes off surface points for creating SDF values close to zero. So this is simply a regularizer that says, whenever I input coordinates that are far away from the boundary from the surface, then there should be a large sign distance function like it should not be close to zero because it's away from a boundary. Okay. And in practice, how you're going to train this is if you have a point cloud, if your coordinates are far away from the next point, then this this is going to be a high this should be a high value otherwise the network is penalized. So we have this condition right here on the gradients, which we know sign distance function should fulfill, we have this thing right here, which is a regularizer basically telling points far away from our data that they should have a high distance function. And then we have this last thing right here, which is for all the points on the surface itself. Here's what we'll what we require. First of all, we require their value to be zero or close to zero, right? This is the loss function. We want to minimize this. And this is simply the output value. So the sign distance function of points on the surface, you know, the things we actually measure they should be zero, right? Because the sign distance function measures how far away from the surface you are. So this is pretty intuitive. But then also, this right here, it says that the gradient of the sign distance function and the normal vector of that point should align. And that basically means and this is now I think this is because we have an oriented point cloud or no. Yes. So what we can do is we can kind of connect points next to each other. And then calculate the normal vectors of that, right? And the signed the network, if we ask the network, hey, what do you think about this position right here, the network should tell us first of all, the sign distance function should be zero, because it's on the boundary. Second of all, the norm of the gradient of the sign distance function at that point should be one because that's a property of sign distance function. And third, and that's the thing right now, the gradient of the sign distance function should align with this normal vector. And that's, you know, pretty intuitive, because you want, you want the sign distance function to increase in value, the gradient basically tells you where the highest increase in value of the function is, you want it to increase along the normal direction and not along any other direction. So that's a pretty good, pretty good constraint to have. So you can see right here, I mean, you don't really have to understand exactly about sign distance functions and so on. But these sirens are pretty good at capturing all of these different constraints. And this was a point, you know, on the surface points off the surface, you additionally say, hey, you should have a pretty high value, and actually not zero value, but a pretty high value. So and again, we only fit one particular scene, we only ever fit one scene with an entire network. So the entire neural network, this whole structure right here, everything is captured by this neural network that we train on the point cloud. And you can see that if you use a relu, what you'll get is super, super wobbly. Because if even if you train the relu with the same loss function, these constraints on the gradients, they're just not going to work out with the relu because the gradients are like constant and discontinuous, right? Whereas the siren can basically fulfill all of these constraints on the different parts, like on the values and on the gradients of the of the loss function. And they have another example right here where they fit this shape. Yeah, so you see all the details are preserved way better where the relus, they'll simply kind of flatten over everything and make it wobbly. Alright, so I hope this sort of made sense. And we'll go to the last thing right now. That is restarting, I wanted to show you the website right here they have for this. It's a pretty cool website to go along with it. And as you can see right here, they have all these samples that they have in the paper, but also in animated format. And as you can see right here, this is the fitting process, the learning process of how you represent these images. So as I said, there, you want to fit these functions to the ground truth. And that happens in steps. So this is very much like you would learn a deep learning functions, I think they use the Adam optimizer. It's just that the data set now come all comes from this one ground truth image. And you can see that the siren network on the right pretty quickly zeros in on the on the image and then gets the details subsequently, right. They also represent audio with this and you can watch that they represent video, compare that to relu representations. And here solving the Poisson equation is where you only fit the gradients or the Laplacian of an image and still get out the good image. That's pretty cool. And here you can see that you can actually play around with these things. So you can click on them and look at this. Look at this learned thing. So on the left, you can see what the siren network learned. And let's scroll down here a bit. And on the right is a relu representation of the same thing. So this is the same network with the same objective. It just has relus instead of sine waves as activation functions. So you can see how much of a difference that makes right here. And the middle is a really with the positional encodings still not good, right? The only thing right here that you have to think of if you look at how big these sirens are, how many parameters they have, they're about at the order of magnitude of how many pixels there are in the image. So I'm, yeah, it's certainly a cool method. But to like these, it's not like your the implicit representation here is very, very well at generalizing, though it would be very cool to see what happens outside, right? If you because now you have you can input any x y coordinates. So technically, you could continue the picture to the bottom and just see what the siren thinks should be here at the bottom. So all of these things would be pretty, pretty cool to actually experiment with and they have the code available to do that. And you can see the fitting process of the Helmholtz equation right here and related projects. Pretty cool website, I definitely invite you to check it out. And let's go back to the paper. And we're back and my tablet crashed. And let's continue. So they're now going on to use sirens in order to solve PDEs. And so in physics, often you have these problems where you are given an equation, but the equation doesn't necessarily involve a function itself, but only involves derivatives of that function like, or relates derivatives to the function and so on. So one example here is this Helmholtz equation that's given as this, where the I think the, the F is a known function, but this is the wave field, we want to you want to get you want to figure out which is unknown. And then this H, M is including, for example, this right here, which is the Laplace operator. So you're given the relation between the function and a Laplace operator of the wave that you want to find out. And your task is to recover the wave. I don't want to go very much into this right here. What you can do is basically you can measure, you can have a room and you can have measurements of the wave or of its derivatives and so on. And then you kind of calculate backwards from the measurements to what the actual wave was. And these sirens turn out to be very, very good at things like this. I guess that's in this solving for the wave field things. But essentially, what this amounts to is a numerical solution of these partial differential equations in physics using these sirens. And that's pretty cool. And the last thing they do is and this gets back to a more of the machine learning context, where they say learning a space of implicit functions. So now they go ahead and say, yeah, so we can represent images in terms of these functions, right? But each image is basically its own function. So each image is basically an optimization of fitting problem. Can we somehow learn functions of functions? So this goes, this comes now back to more of a machine learning context, where you say, so I, I have a network right here, that I have a network that gives me the parameters of the siren. So this right here is okay, let's, let's go to an example. In this example, what you'll have is you'll have an image like this one, where a few pixels are masked, actually, most of the pixels are masked. And you want to put this into a CNN. And the CNN should output the parameters of the siren network. So the parameters because the siren network, given its parameters, is the image itself. So that's the siren, I said siren network, the siren is the image, if you know its parameters, right. So here, you train a CNN to give you the parameters of the siren. That's almost the same as training a CNN to give you the image directly. But again, we don't want to have the explicit representation of an image, we want to have the implicit representation such that it's continuous, and we can manipulate it and so on. So the CNN is now trained on a data set. So you take C for 10, and you construct a whole bunch of, of images with only kind of 100 pixels remaining. And then you train a CNN to give you the parameters of the siren that would reconstruct the ground truth, right. And then you can test that on the test image. And you can see right here, the results are pretty good. So these are test samples, these are now these are now images that were not seen during training of this CNN. And therefore, the outcoming siren also hasn't seen that image, it's the siren is simply parameterized by the CNN. You can see this works pretty well. So if you only have 10 pixels, you already get something out of it, right. And if you have 100 pixel, you already get fairly close to the to the ground truth right here. Now, this is not GAN quality images, of course, but it's pretty impressive to see that an implicit parameterization and implicit representation of the images can be so powerful, right? Yeah, so this, this is a pretty cool thing. And again, it's better than it's kind of more back to the machine learning framework that you're used to because there's a train and a test data set. And now the only thing is that the output is a function given by its parameters, and not the actual pixel values. Okay, so let's actually let's look at the broader impact statement. The proposed siren representation enables accurate representations of natural signals, such as images, audio and video in a deep learning framework. This may be an enabler for downstream tasks involving such signals such as classification for images or speech to text systems for audio. Such applications may be leveraged for both positive and negative ends. Siren may in the future further enable novel approaches to the generation of such signals. This has potential for misuse in impersonating actors without their consent. For an in depth discussion of so called deep fakes, we refer the reader to a recent review article in neural rendering. This has this has like no perplexity, like no perplexity at all. Like is anyone benefited by this? Seriously? Okay. But at least we made the authors think of the consequences of their research. Yeah, so I invite you to check out this paper maybe with this right now, you can follow a bit better what happens here. This is a different paradigm of research. It's a cool paradigm. It's away from your usual machine learning framework. And yeah, so I'm excited what happens next. And this I also invite you to check out the websites. They have lots of videos and goodies and so on. And with that, bye bye.",
        "output": "### Implicit Neural Representations with Periodic Activation Functions\n#### Introduction and Overview\nSIREN (Sinusoidal Representation Networks) is a new model that can represent signals such as images, sound, waves or point clouds as functions by mapping their coordinates to their values. The authors introduce siren neural networks that use sine waves as their nonlinearities instead of ReLU or hyperbolic tangent functions. Initializing these networks carefully allows them to capture signals very well.\n\n#### SIREN Architecture and Initialization\nSIREN is an MLP (Multi-Layer Perceptron) model. The only difference from classic MLPs is that they use sine wave as activation functions instead of ReLU or sigmoid functions. The initialization of the weights and biases in the siren networks plays an essential role as improper initialization may lead to poor performance. The authors propose an initialization scheme that bends the input to each of the sine activation functions to be normally distributed.\n\n#### Fitting Image Gradients and Laplacian\nSIREN can match not only an image but also its derivatives. They showcase this property by training siren networks to match the image's first and second derivatives (gradients and Laplacian). By matching the gradients, the network is able to reproduce the image with high fidelity while only using the gradient information. Similarly, when matching the Laplacian, the reconstructed image is still of good quality, despite missing the constant luminosity in both the first and second derivative.\n\n#### Practical Applications and Conclusion\nSIREN demonstrates that neural representations with periodic activation functions allow more faithful representation of images and signals than traditional neural networks. The ability to match not only the image but also its derivatives opens up new possibilities in image processing and signal understanding. The initialization is crucial for SIREN networks' performance, and researchers should keep this in mind when experimenting with sine wave activation functions in deep learning models."
    },
    {
        "instruction": "<|SYSTEM|>You are a scholars that is taking markdown notes about some video, you would be giving some written scripts for the task.\nYou have finished writing the notes, and you want to extract the title from the notes.\nThe title should only be the name of the technology or algorithm, no extra words needed.\nPlease write the notes in markdown format. No need to write \"Title:\" for the Title. Make sure the title is within 10 words.\nHere is an example:\n## Hidden Markov Model\n<|ASSISTANT|>## ",
        "input": "<|USER|>### Implicit Neural Representations with Periodic Activation Functions\n#### Introduction and Overview\nSIREN (Sinusoidal Representation Networks) is a new model that can represent signals such as images, sound, waves or point clouds as functions by mapping their coordinates to their values. The authors introduce siren neural networks that use sine waves as their nonlinearities instead of ReLU or hyperbolic tangent functions. Initializing these networks carefully allows them to capture signals very well.\n\n#### SIREN Architecture and Initialization\nSIREN is an MLP (Multi-Layer Perceptron) model. The only difference from classic MLPs is that they use sine wave as activation functions instead of ReLU or sigmoid functions. The initialization of the weights and biases in the siren networks plays an essential role as improper initialization may lead to poor performance. The authors propose an initialization scheme that bends the input to each of the sine activation functions to be normally distributed.\n\n#### Fitting Image Gradients and Laplacian\nSIREN can match not only an image but also its derivatives. They showcase this property by training siren networks to match the image's first and second derivatives (gradients and Laplacian). By matching the gradients, the network is able to reproduce the image with high fidelity while only using the gradient information. Similarly, when matching the Laplacian, the reconstructed image is still of good quality, despite missing the constant luminosity in both the first and second derivative.\n\n#### Practical Applications and Conclusion\nSIREN demonstrates that neural representations with periodic activation functions allow more faithful representation of images and signals than traditional neural networks. The ability to match not only the image but also its derivatives opens up new possibilities in image processing and signal understanding. The initialization is crucial for SIREN networks' performance, and researchers should keep this in mind when experimenting with sine wave activation functions in deep learning models.",
        "output": "Implicit Neural Representations with Periodic Activation Functions"
    },
    {
        "instruction": "<|SYSTEM|>You are a scholars that is taking markdown notes about some video with Wiki style, you would be giving some written scripts for the task.\nAnd summarize each paragraph with a title.\nRemember to keep the technology or algorithm name, very important.\nPlease write the notes in markdown format.\nHere is an example:\n### Definiation\nA deadlock is a situation in which two computer programs sharing the same resource are effectively preventing each other from accessing the resource, resulting in both programs ceasing to function. \nThe earliest computer operating systems ran only one program at a time.\n\n### What it have to do with Relationsal Database ManagementSystem\nIn a database, a deadlock is an unwanted situation in which two or more transactions are waiting indefinitely for one another to give up locks. \nDeadlock is said to be one of the most feared complications in DBMS as it brings the whole system to a Halt. \nWhen you tries to insert or update data on database, sometime your record need to depend on the row record.  \n<|ASSISTANT|>#### ",
        "input": "<|USER|> In this video, we'll discuss a topic that's essential to every software developer, data structures. We use them every day, and they play a critical role in building efficient systems. So let's dive right in and take a closer look at some common examples. Let's start by discussing lists. Lists are a versatile and essential data structure in software development. They are great for storing and manipulating ordered data. They are useful in various applications like task management, social media feeds, and shopping carts. In a task management application, a list can be used to store and organize tasks for each user. Tasks can be added, removed, or reordered easily, and users can mark them as complete as needed. Lists are also useful in social media applications like Twitter, where they can store and display a user's feed in real time, ensuring the latest content is shown in the correct order. Arrays are another fundamental data structure. They provide a fixed-size order collection of elements. They are particularly well suited for situations where the size of the collection is known or doesn't change frequently. Arrays are commonly used in mathematical operations, storing large datasets, or when there is a need for random access to elements. For example, in a weather application, an array could be used to store temperature readings for a specific location over a defined period. This allows for easy calculations like averages and trends. Arrays are also widely used in image processing, where each pixel's color data can be represented in a two-dimensional array. It enables efficient manipulation and transformation of the image. Next, we have stacks. Stacks follow the last-in, first-out principle. They are perfect for supporting undo and redo operations in text editors or maintaining browsing history in web browsers. In a text editor, a stack can be used to store each change made to the text, making it simple to revert to a previous state when the user triggers an undo operation. Queues operate on a first-in, first-out basis. They are good for managing printer jobs, sending user actions in games, or handling messages in chat applications. In chat applications, a queue can be used to store incoming messages in the order they are received. It ensures that they are displayed to the recipient in the correct sequence. Heaps, on the other hand, are used for task scheduling and memory management. They are especially helpful in implementing priority queues, where we need to access the highest or lowest priority item efficiently. Trees organize data hierarchically. They are useful for representing data with natural hierarchies or relationships. They can be used in various applications like database indexing, AI decision making, or file systems. In AI decision making, trees like decision trees are used in machine learning for classification tasks. Trees are also used in database indexing, where they can help speed up search, insert, or delete operations. For example, B-trees and B-plus trees are commonly used in relational databases to efficiently manage and index large amounts of data. Hash tables allow for efficient data look-up, insertion, and deletion. They use a hash function to map keys to their corresponding storage locations. It enables constant time access to the store values. Hash tables are widely used in various applications, such as search engines, caching systems, and programming language interpreters or compilers. In search engines, hash tables can be used to store and quickly retrieve index data based on keywords. This provides fast and relevant search results. Caching systems may use hash tables to store and manage cached data. Caching allows for rapid access to frequently requested resources and improves overall system performance. Another example is the implementation of symbol tables in programming language interpreters or compilers. Hash tables can be used to efficiently manage and look up variables, functions, and other symbols defined in the source code. Suffix trees are specialized for searching strings in documents. This makes them perfect for text editors and search algorithms. In a search engine, a suffix tree can be used to efficiently locate all occurrences of a search term within a large corpus of text. Graphs are all about tracking relationships and finding paths. This makes them invaluable in social networks, recommendation engines, and pathfinding algorithms. In a social network, a graph can be used to represent the connections between users. It enables features like friend suggestions or analyzing network trends. R-trees are good at finding nearest neighbors. They are crucial for mapping apps and geolocation services. In a mapping application, R-trees can be used to store spatial data, such as points of interest. This enables efficient queries to find the nearest locations based on the user's current positions. Now let's discuss cache-friendly lists and how it relates to various data structures, including lists, arrays, and other mentioned earlier in the video. The CPU cache is a small, fast memory between the main memory and the CPU. It stores recently accessed data and instructions, so the CPU can access them quickly without fetching them from the slower main memory. Different data structures have varying levels of cache-friendly lists based on how their elements are stored in memory. Contiguous memory storage, like that in arrays, allows for better cache locality and fewer cache misses, resulting in improved performance. When an array element is accessed, the cache can prefetch and store nearby elements, anticipating that they might be accessed soon. On the other hand, data structures with non-contiguous memory storage, like linked lists, can experience more cache misses and reduce performance. In a linked list, elements are stored in nodes scattered throughout the memory, and each node contains a pointer to the next node in the sequence. This makes it difficult for the CPU to predict and load the next node before it's needed. The other data structures, such as trees, hash tables, and graphs, also have varying degrees of cache-friendly lists based on the implementation and use case. This disparity in access times can lead to performance issues in modern computing, particularly in situations where cache misses occur frequently. We should be mindful of this when working with performance-critical applications, and choose the appropriate data structure based on the specific requirements and constraints of the projects. And there you have it. These are just some of the many data structures we use every day as software developers. Understanding and mastering these data structures will help us build more efficient systems, making us better at our craft. If you like our videos, you may like our system design newsletter as well. It covers topics and trends in large-scale system design, trusted by 300,000 readers. Subscribe at blog.buybuygo.com",
        "output": "### Introduction to Common Data Structures\nData structures are essential for software developers, playing a vital role in building efficient systems. This video covers several common examples, including lists, arrays, stacks, queues, heaps, trees, hash tables, suffix trees, graphs, R-trees, and cache-friendly lists.\n\n### Lists\nLists are versatile data structures used for storing and manipulating ordered data. Applications include task management, social media feeds, and shopping carts.\n\n### Arrays\nArrays provide a fixed-size ordered collection of elements, suited for situations with a known or infrequent change in the collection size. They are used in mathematical operations, large datasets, and random access to elements.\n\n### Stacks\nStacks follow the last-in, first-out (LIFO) principle and are ideal for undo and redo operations in text editors or maintaining browsing history in web browsers.\n\n### Queues\nQueues operate on a first-in, first-out (FIFO) basis and are suitable for managing printer jobs, sending user actions in games, or handling messages in chat applications.\n\n### Heaps\nHeaps are primarily used for task scheduling and memory management, particularly implementing priority queues for efficient access to high or low priority items.\n\n### Trees\nTrees organize data hierarchically and represent data with natural hierarchies or relationships. Applications include database indexing, AI decision making, and file systems.\n\n### Hash Tables\nHash tables enable efficient data look-up, insertion, and deletion by mapping keys to storage locations with a hash function. Applications include search engines, caching systems, and programming language interpreters or compilers.\n\n### Suffix Trees\nSuffix trees are specialized for searching strings in documents, making them ideal for text editors and search algorithms.\n\n### Graphs\nGraphs are used for tracking relationships and finding paths, making them invaluable in social networks, recommendation engines, and pathfinding algorithms.\n\n### R-Trees\nR-Trees are suitable for finding nearest neighbors, making them crucial for mapping apps and geolocation services.\n\n### Cache-friendly Lists\nCache-friendly lists pertain to various data structures, with contiguous memory storage providing better cache locality and fewer cache misses. This is particularly relevant for performance-critical applications.\n\n### Conclusion\nUnderstanding and mastering these common data structures will help developers build more efficient systems, improving their skills in the field."
    },
    {
        "instruction": "<|SYSTEM|>You are a scholars that is taking markdown notes about some video, you would be giving some written scripts for the task.\nYou have finished writing the notes, and you want to extract the title from the notes.\nThe title should only be the name of the technology or algorithm, no extra words needed.\nPlease write the notes in markdown format. No need to write \"Title:\" for the Title. Make sure the title is within 10 words.\nHere is an example:\n## Hidden Markov Model\n<|ASSISTANT|>## ",
        "input": "<|USER|>### Introduction to Common Data Structures\nData structures are essential for software developers, playing a vital role in building efficient systems. This video covers several common examples, including lists, arrays, stacks, queues, heaps, trees, hash tables, suffix trees, graphs, R-trees, and cache-friendly lists.\n\n### Lists\nLists are versatile data structures used for storing and manipulating ordered data. Applications include task management, social media feeds, and shopping carts.\n\n### Arrays\nArrays provide a fixed-size ordered collection of elements, suited for situations with a known or infrequent change in the collection size. They are used in mathematical operations, large datasets, and random access to elements.\n\n### Stacks\nStacks follow the last-in, first-out (LIFO) principle and are ideal for undo and redo operations in text editors or maintaining browsing history in web browsers.\n\n### Queues\nQueues operate on a first-in, first-out (FIFO) basis and are suitable for managing printer jobs, sending user actions in games, or handling messages in chat applications.\n\n### Heaps\nHeaps are primarily used for task scheduling and memory management, particularly implementing priority queues for efficient access to high or low priority items.\n\n### Trees\nTrees organize data hierarchically and represent data with natural hierarchies or relationships. Applications include database indexing, AI decision making, and file systems.\n\n### Hash Tables\nHash tables enable efficient data look-up, insertion, and deletion by mapping keys to storage locations with a hash function. Applications include search engines, caching systems, and programming language interpreters or compilers.\n\n### Suffix Trees\nSuffix trees are specialized for searching strings in documents, making them ideal for text editors and search algorithms.\n\n### Graphs\nGraphs are used for tracking relationships and finding paths, making them invaluable in social networks, recommendation engines, and pathfinding algorithms.\n\n### R-Trees\nR-Trees are suitable for finding nearest neighbors, making them crucial for mapping apps and geolocation services.\n\n### Cache-friendly Lists\nCache-friendly lists pertain to various data structures, with contiguous memory storage providing better cache locality and fewer cache misses. This is particularly relevant for performance-critical applications.\n\n### Conclusion\nUnderstanding and mastering these common data structures will help developers build more efficient systems, improving their skills in the field.",
        "output": "## Common Data Structures"
    },
    {
        "instruction": "<|SYSTEM|>You are a scholars that is taking markdown notes about some video with Wiki style, you would be giving some written scripts for the task.\nAnd summarize each paragraph with a title.\nRemember to keep the technology or algorithm name, very important.\nPlease write the notes in markdown format.\nHere is an example:\n### Definiation\nA deadlock is a situation in which two computer programs sharing the same resource are effectively preventing each other from accessing the resource, resulting in both programs ceasing to function. \nThe earliest computer operating systems ran only one program at a time.\n\n### What it have to do with Relationsal Database ManagementSystem\nIn a database, a deadlock is an unwanted situation in which two or more transactions are waiting indefinitely for one another to give up locks. \nDeadlock is said to be one of the most feared complications in DBMS as it brings the whole system to a Halt. \nWhen you tries to insert or update data on database, sometime your record need to depend on the row record.  \n<|ASSISTANT|>#### ",
        "input": "<|USER|> Recently vector databases got a lot of fame with companies raising hundreds of millions of dollars to build them and people Calling it a new kind of database for the ai era on the other hand for many projects It might be an overkill solution and using a traditional database or even just a numpy nd array might work just fine But there is no doubt that vector databases are extremely fascinating and allow many great applications Especially when you want to give large language models like GPT-4 long-term memory So in this video, I will explain in a very beginner friendly way what vector databases are and how they work We will go over some use cases for them and then I will briefly show you some options you can use So let's get started So let's start with the why over 80% of the data out there is unstructured such as social media posts Images videos or audio data, you cannot easily fit them into a relational database Let's take an image as an example if you want to put this into a relational database in order to search for similar images What ends up happening is that often we manually assign keywords or tags to it because from the pixel values alone We cannot really search for similar images and the same holds true for unstructured text blobs or audio and video data So we either have to assign tags or attributes to it often manually or we can find a different representation to store the data and this brings us to vector embeddings and vector databases in short a vector database indexes and stores vector embeddings for fast retrieval and similarity search So let's take a step back and look at those two important components first It uses clever algorithms to calculate the so-called vector embeddings This is done by machine learning models a vector embedding is just a list of numbers that represents the data in a different way for example You can calculate an embedding for a single word a whole sentence or an image and now we have numerical data that the computer Can understand one easy possibility we get with vectors is to find similar vectors by calculating the distances and doing a nearest neighbor Search so we can easily find similar items for simplicity I display a 2d case here, but in reality, of course those vectors can have hundreds of dimensions But just storing the data as embeddings is not enough Performing a query across thousands of vectors based on its distance metric would be extremely slow And this is why those vectors also need to be indexed So the indexing process is the second key element of a vector database an index is a data structure that Facilitates the search process So the indexing step maps the vectors to a new data structure that will enable faster searching This is a whole research field on its own and different ways to calculate indexes exist So I won't go into details here. Just know that indexes are needed for efficient search. So let's go over some use cases I already mentioned that we can use vector databases to equip large language models with long-term memory This is for example what you can easily implement with langchain We can use it for semantic search when we need to search not for exact string matches But rather based on the meaning or context of our question We can also use it for similarity search for images audio or video data So we can say hey find me a similar image to this one and we don't need to use some keywords or text to describe The image and we can use a vector database as a ranking and recommendation engine for example for online retailers It can be used to suggest items similar to past purchases of a customer since we can simply Identify the nearest neighbors of an item in our database. So now that you know some use cases Let's go over some options. You can use as a vector database. There are a number of vector databases available For example, we have pinecone viviate Chroma redis also has a vector database Kudrant milvis or vespa AI so I won't go into details here But if you want to see a separate video with an in-depth comparison, then let me know in the comments below All right I hope you now have a good understanding of what vector databases are and what you can do with them if you want to see More explainer videos and AI tutorials then make sure to subscribe to our channel and then I hope to see you in the next video Bye",
        "output": "### Introduction to Vector Databases\nVector databases have gained popularity recently, with some companies raising significant funds to develop them for AI applications. In some cases, traditional databases or numpy nd arrays might be sufficient, but vector databases offer unique advantages, particularly for large language models like GPT-4.\n\n### Unstructured Data and Vector Embeddings\nMore than 80% of existing data is unstructured, such as social media posts, images, videos, or audio data. These types of data are difficult to fit into a relational database. Using vector embeddings and vector databases, we can represent and store this data in a different way for fast retrieval and similarity search.\n\n### Calculating Vector Embeddings\nVector embeddings are calculated using machine learning models, and they represent data as a list of numbers. These embeddings can be calculated for words, sentences, or images, allowing computers to easily understand and process them. Calculating distances between vectors can identify similar items.\n\n### Indexing Vectors in a Vector Database\nTo search efficiently across thousands of vectors, they need to be indexed. The indexing process maps vectors to a new data structure that enables faster searching. Different methods exist for calculating indexes, but the important thing is that they are needed for efficient search.\n\n### Use Cases for Vector Databases\n- Long-term memory for large language models, like langchain\n- Semantic search based on meaning or context, rather than string matches\n- Similarity search for images, audio, or video data\n- Ranking and recommendation engine for online retailers\n\n### Vector Database Options\nSeveral vector databases are available, such as Pinecone, Vivi, Chroma, Redis, Kudraint, Milvus, and Vespa AI. Comparisons of these options can be provided in a separate video. \n\n### Conclusion\nVector databases are a fascinating and powerful tool, particularly for AI applications handling unstructured data. Understanding their function and potential use cases can be helpful in determining whether they are the right choice for a project."
    },
    {
        "instruction": "<|SYSTEM|>You are a scholars that is taking markdown notes about some video, you would be giving some written scripts for the task.\nYou have finished writing the notes, and you want to extract the title from the notes.\nThe title should only be the name of the technology or algorithm, no extra words needed.\nPlease write the notes in markdown format. No need to write \"Title:\" for the Title. Make sure the title is within 10 words.\nHere is an example:\n## Hidden Markov Model\n<|ASSISTANT|>## ",
        "input": "<|USER|>### Introduction to Vector Databases\nVector databases have gained popularity recently, with some companies raising significant funds to develop them for AI applications. In some cases, traditional databases or numpy nd arrays might be sufficient, but vector databases offer unique advantages, particularly for large language models like GPT-4.\n\n### Unstructured Data and Vector Embeddings\nMore than 80% of existing data is unstructured, such as social media posts, images, videos, or audio data. These types of data are difficult to fit into a relational database. Using vector embeddings and vector databases, we can represent and store this data in a different way for fast retrieval and similarity search.\n\n### Calculating Vector Embeddings\nVector embeddings are calculated using machine learning models, and they represent data as a list of numbers. These embeddings can be calculated for words, sentences, or images, allowing computers to easily understand and process them. Calculating distances between vectors can identify similar items.\n\n### Indexing Vectors in a Vector Database\nTo search efficiently across thousands of vectors, they need to be indexed. The indexing process maps vectors to a new data structure that enables faster searching. Different methods exist for calculating indexes, but the important thing is that they are needed for efficient search.\n\n### Use Cases for Vector Databases\n- Long-term memory for large language models, like langchain\n- Semantic search based on meaning or context, rather than string matches\n- Similarity search for images, audio, or video data\n- Ranking and recommendation engine for online retailers\n\n### Vector Database Options\nSeveral vector databases are available, such as Pinecone, Vivi, Chroma, Redis, Kudraint, Milvus, and Vespa AI. Comparisons of these options can be provided in a separate video. \n\n### Conclusion\nVector databases are a fascinating and powerful tool, particularly for AI applications handling unstructured data. Understanding their function and potential use cases can be helpful in determining whether they are the right choice for a project.",
        "output": "## Vector Databases"
    },
    {
        "instruction": "<|SYSTEM|>You are a scholars that is taking markdown notes about some video with Wiki style, you would be giving some written scripts for the task.\nAnd summarize each paragraph with a title.\nRemember to keep the technology or algorithm name, very important.\nPlease write the notes in markdown format.\nHere is an example:\n### Definiation\nA deadlock is a situation in which two computer programs sharing the same resource are effectively preventing each other from accessing the resource, resulting in both programs ceasing to function. \nThe earliest computer operating systems ran only one program at a time.\n\n### What it have to do with Relationsal Database ManagementSystem\nIn a database, a deadlock is an unwanted situation in which two or more transactions are waiting indefinitely for one another to give up locks. \nDeadlock is said to be one of the most feared complications in DBMS as it brings the whole system to a Halt. \nWhen you tries to insert or update data on database, sometime your record need to depend on the row record.  \n<|ASSISTANT|>#### ",
        "input": "<|USER|> Hugging Face have just announced something that I think is probably going to be a very major thing in the future of large language models and NLP. And that is their spin on agents for large language models and transformers in general. Now there's quite a few reasons as to why I think Hugging Face are in a very good position to offer possibly one of the best agents and tool frameworks out there. And I'm going to discuss those. First for those of you that haven't heard of these things before or just kind of not sure what they are, let me quickly explain what an agent and what a tool is. So we know what large language models are. They are big transformer models that can basically answer questions in natural language for us based on some natural language input. A agent kind of takes this and takes it a little bit further and expands these LLMs out to basically allow them to have multiple sets of reasoning and thoughts so they can think to themselves. And this is ideal for when we want to integrate what are called tools. So what we can do is we can tell an LLM, hey we want you to answer a question. If you can't answer by yourself, you can actually refer to some other tools that we have given to you. And you might say something like if you don't know about a particular topic, you can form a Google search in order to find out about that particular topic. And you would also explain how we can do that. And because the LLM has this multi-step thought process, it can say, OK, I've got this question. I need to use a Google search tool. And then it will say, how do I use that Google search tool? It's going to provide some input to Google search. We would then go do a Google search for it, return some answers and pass that back into the LLM. And now all of a sudden it can answer a question. And you can do this for a ton of tools, SQL databases, knowledge base, vector databases and so on. Python interpreters, you can do basically anything you can program. You can create a tool out of it. Now, obviously, this is a very powerful thing to be able to do. Now, at the moment, by far the biggest library for using agents is LangChain. There's also Haysack who have introduced agents recently. And there's also actually chatgpt plugins, which are a form of agents as well, or at least it's a form of tools added to the agents, which is actually chatgpt itself. Now, Hugging Face have also kind of jumped on the bandwagon. And I think their implementation is actually very interesting and particularly powerful for a lot of different reasons. A big component of that is what Hugging Face actually is. So Hugging Face is essentially almost like a huge community and hub of all of these different transformer models, diffusion models for generating images, datasets, and just a ton of anything you can think of in machine learning. Hugging Face actually cover a lot of it. And their version of agents and tools are very interesting. And, you know, I haven't been all the way through yet. It's kind of my first look, but the agent itself is very simple to use. It also can be used as a conversational agent. So as a chatbot, where you have multi-seps in the process, and it also gives us access to all of these models on Hugging Face, which I think is one of the coolest things about it. But, and there's other things as well, but actually let me jump into it and show you those rather than just talking through them. We'll just have a quick look at their example here. So they have basically, they don't really show you anything. I'm going to go through code in a moment, but you run agent.run, caption the following image, right? And then they pass in this image, right? Through image here. And then the output is a beaver swimming in the water. Then they also have this. So agent.run read the following text out loud, and it will use a text-to-speech model to actually do that. Can also do this. So we have like some OCR reading this document or this image of a document. And then we say in this following document, we ask a question and the output is this ballroom foil, which is down here at the bottom right. Now, the point here is that these agents are using like a ton of different models from the Transformers library. And I think this takes pretty clear inspiration from this paper called Hugging GPT, which essentially use chat GPT with a ton of Hugging Face models to do a load of cool things. So it uses the same sort of approach where you have an agent, which was the chat GPT model. And if we come down to the first image here, this basically shows us how it works. We have a large language model chat GPT in this case, our RGP3.5 turbo I assume. And this is like your controller. And then we have all these what we would call specialist models that can do particular things that chat GPT is not able to do, like understand what is within an image or caption an image. And chat GPT or your large language model is able to basically figure out, okay, given the question, which models do we need to use? What input do I need to pass to them? And then uses the output from those models to inform the next step of trying to figure out what it needs to do and then provide a very cool answer that a normal large language model would not be able to do by itself. Now let's take a look at a code example of how we can use this. Now initially I'm just actually using one of the examples from Hugging Face and then we'll just go through a few cells and then we'll do something a little more interesting. So first thing we need to do here is install a few things. So transformers, which is a library that contains the agents. Because we're going to be using image generation and models here, so the diffusion models, we need to use Hugging Face diffusers. And we're also going to use accelerate, which I believe allows us to run things faster. Now in reality, and I'm not sure if this is the case, I think we should hopefully be running on a GPU here. Okay, so here I've gone into my runtime settings and just change my hardware accelerator to GPU. And then what we do is we're just going to use OpenAI here, obviously, you know, for your large language model. Hugging Face makes it very easy to use other open source options. So you can do that. I'm just using this because I know it's going to work. It's quick. So here we are. And I'm going to use text-adventure-003. Basically what I found generally speaking is that text-adventure-003 is usually better at following instructions for tools within agents than GPT 3.5 turbo. And that's also the model that they use, that Hugging Face are using in their examples. So I'm not 100% sure if they support GPT 3.5 turbo yet. I'll need to try at some point. So yeah, all we do, transformers, tools, import OpenAI agent. And there are other agents as well. I think there's the Hugging Face agent is like HF agent, maybe, but obviously we're just going to use OpenAI one here. You will need to add in your OpenAI API key, which you can get from platform.openai.com. Okay. So we're going to run this. This is going to initialize our agent and actually that's all we need to do. And now it works, which is very easy and quick to sell. So I think all we're doing here is, okay, so we're downloading the tool configuration. And this is a very interesting component of Hugging Face's tools implementation, which is that we can download community contributed tools and obviously Hugging Face's own tools. So in the next, probably very soon, next few weeks, we're probably going to see some pretty insane tools appear from the community, which will be fascinating to see. That's one of the big components as to why I think this is going to be pretty major. Okay. And another reason I think this is going to be pretty major is that we can do multimodal agents super easily. So I haven't done anything here, right? I just initialized my agent and I said, okay, I want to generate an image of a boat in the water. Right. And because Hugging Face has, they have a big diffusers library, which contains loads of text to image diffusion models. And they obviously have all the transform models as well. They've kind of integrated at least a few of those into the default agent. So if I just say generate an image of a boat in the water, what's in here is this isn't how long it takes to process. This is actually downloading the model. Okay. So the image generation model. This will only happen once. Okay. And I'll prove that by running it again in a moment. So that's going to download. It is, okay, here we go. So we've got an explanation from the agent. I'm going to use the following tool image generator to generate an image according to the prompt. Then it generates some code here and here we go. Here's here is our image of a boat in the water. Okay. So yeah, that was super easy to do. And let me just run that again. Okay, you'll see that it doesn't take quite as long this time. Okay, so it's generating the image. And there we go. Right, that was eight seconds, which considering it also three seconds to generate the image, that's pretty good for an agent. So that is really cool. And okay, what we can also do is, okay, I have boat image here. And we can come down and do agent run again. And I can pass in this variable. Okay, so we'll use this backtick here. And we can pass in a variable which we then enter its own actual variable here, right? So we could also just do like image, okay? And then that just means that we need to replace image here. Okay, so let's run that. I'm just going to ask you to write a caption for this image. Again, it's going to need to download the captioning model as you can see here. Okay, and then we get a boat floating in the water with clouds in the background. All right, let's run it again so we can see how long that actually takes. Okay, so four seconds. Again, very quick. Okay, and then, so here I was just looking, okay, what does that prompt template look like when we're doing that run method? You can kind of see a little bit of the logic that is going on in here. Okay, so I'm going to ask you to perform tasks. It includes all the tools here. And then it gives a few examples and then asks it to figure out what it needs to do next, right? This is just, yeah, we don't need that. It just contains all the code for the agent, which is another nice thing that I like about the Hugging Face implementation is that the code is pretty readable. So if something isn't quite working the way you'd expect it to work, you can go into code and kind of figure out why almost straightaway, which is not as easy to do with other libraries at the moment. So that's very nice. And yeah, I mean, it's super cool. Now let's have a look at a conversational agent. So basically a chatbot, right? So I'm going to say, hey, how are you? Okay, we just get this, right? Hi there, I'm doing well. Thank you for asking. Cool. I'm going to ask it to create an image of a giraffe riding a skateboard. And I just made this up very quickly before running this. And I mean, the results are not perfect, but they're funny, right? So we're not running any special diffusion models here. So we'll get like this weird two-headed giraffe, but you know, let's stick with that. And like I said, the results are entertaining. And they're not particularly impressive from an image generation point of view, but it's just interesting to see. So here we're using an image generator model. And then you come down here and it's not going to use an image generator model. It's going to use an image transform model to modify the existing image. And this is something that is really cool as well. So, okay, first it needs to download that model. So let me explain what is so cool here, right? So look, you can see that it's generating some code, right? And this code is actually referring to the image, okay? And the image is generated by this code beforehand. So the Python interpreter that all of this is using is maintained between chat interactions. It's going to write some code and then you can say, oh, actually can you do something else? And it can still interact with that code. It can still see that code and it's going to write some more code based on what it's already done. Which is not something that I have seen done by default in other libraries that use agents and tools. So, I mean, that's just a really cool thing that I like and it's just insane how easy it is to get that working. So, cool. Yeah, so now we get this, right? So it's an elephant. So this image transform model, I haven't used an image transform model before. I didn't actually know they were a thing, but I think what it does is identifies where in the image the draft is, which it's done, and then just tries to modify that part of the image. So we get this kind of weird, I mean, yeah, I can see what it's trying to do, but it's interesting, right? So, okay, cool. And then this didn't work for me before, I want to try it again. So could you give the elephant shiny laser eyes? Last time I tried this, it made the elephant like made of gold. Let's see what it does this time. Maybe it just read, could you make the elephant shiny? I'm not sure. Okay, so it went through that again. So now we have like a, I don't know what it is. It's like a gold draft, I think. And then, okay, we can caption the image. So I'm very curious as to what it says about this image. Okay, this caption is a GIF, GIF, signing on a skateboard. Before I'm pretty sure it gave me a very similar, very similar output. So I wonder if, is it, okay, so it's a modified image, right? So can you caption the modified image? Or what I'm going to say is, I'm going to copy this, and I'm going to say, sorry, I meant the modified image. Okay, okay, A-G, GAF. Okay, so the code is right. So the caption is image captioner, modified image. And then this is weird. I'm not sure why, like maybe there's some weird stuff going on with the tokenizer here. But yeah, here we get a GI GIF on a skateboard. Okay, fair enough. And then I wanted to test this a little more. Can you search the internet for some more of these types of images? So a search tool is a pretty typical tool that is included within agents. And I just wanted to see if they include that by default. So let's try and we'll see. Okay, so unfortunately, no, they don't seem to. So it refers to a text downloader tool. Okay, and that is apparently a thing. So downloads the text downloader model or tool. I'm not sure what it is exactly. And yeah, it just downloads some text. So doesn't work for everything yet, but that I think is already pretty cool. The fact that we're just referring to all these models that we have this Python interpreter just built in. And just so it's so easy to use, I think is really interesting. And yeah, for sure, we're definitely going to do a lot more on transformer agents in the future. But for now, I just want to introduce the library to you or the new features to you. And also just explore them myself. Again, like I said, there's a massive community aspect to this. So that is probably one of the biggest things that I think Hugging Face agents has going for it. The fact that they will have and I haven't seen if there are, if we can actually find them on the Hugging Face website. But let me show you what it looks like with just models. So we can come over here. We have models, right? And there's just tons of models on Hugging Face. Right now, imagine that they're planning to do or are doing the same thing with tools. And it's not here yet. I don't see any tools. But clearly the code or the interface is already there because we were downloading tools here. I believe we're downloading tools here. So that is super interesting. And I'm sure people are going to build some insane tools very quickly. So yeah, that will be pretty huge in my opinion. Now, I haven't seen how customizable these agents are yet. That's something we'll be exploring very soon. But I would imagine, you know, Hugging Face would make things pretty simple. So my expectation is that it will be pretty easy to work through and figure all that out. So yeah, overall, I'm very excited to see what they do with this. I think this will be a really cool feature. But for now, I'm going to leave it there. So I hope this has been interesting and useful. Thank you very much for watching and I will see you again in the next one. Bye. you",
        "output": "### Hugging Face Agents Introduction\n\n#### The Importance of Hugging Face Agents\nHugging Face has announced their spin on agents for large language models and transformers. \nTheir implementation is interesting and powerful due to:\n- The huge community and hub that Hugging Face has for transformer models, diffusion models, datasets, and more.\n- The ease of using multi-modal agents, integrations with the community-contributed tools.\n- The readability of the code.\n\n#### What are Agents and Tools?\nLarge language models (LLMs) answer questions in natural language based on input. Agents extend LLMs by allowing them to have multiple sets of reasoning and thoughts. Tools can be integrated, letting the LLM ask for help from different tools like Google search or SQL databases.\n\n#### Code Examples\nThe example provided uses OpenAI's Text API and Hugging Face's Agents Library. The code shows different tasks performed by the agents, like generating images or image captioning.\n\n#### Conversational Agent Example\nThe example showcases a conversation where an agent is asked to create an image, modify it, and caption it. The multitasking capabilities are impressive and are made simple to integrate into the code.\n\n#### Community Aspect\nThere is a massive community aspect to Hugging Face agents, and it is expected that the community will contribute many tools. Being able to easily access these shared tools will bring a lot of potential to the platform."
    },
    {
        "instruction": "<|SYSTEM|>You are a scholars that is taking markdown notes about some video, you would be giving some written scripts for the task.\nYou have finished writing the notes, and you want to extract the title from the notes.\nThe title should only be the name of the technology or algorithm, no extra words needed.\nPlease write the notes in markdown format. No need to write \"Title:\" for the Title. Make sure the title is within 10 words.\nHere is an example:\n## Hidden Markov Model\n<|ASSISTANT|>## ",
        "input": "<|USER|>### Hugging Face Agents Introduction\n\n#### The Importance of Hugging Face Agents\nHugging Face has announced their spin on agents for large language models and transformers. \nTheir implementation is interesting and powerful due to:\n- The huge community and hub that Hugging Face has for transformer models, diffusion models, datasets, and more.\n- The ease of using multi-modal agents, integrations with the community-contributed tools.\n- The readability of the code.\n\n#### What are Agents and Tools?\nLarge language models (LLMs) answer questions in natural language based on input. Agents extend LLMs by allowing them to have multiple sets of reasoning and thoughts. Tools can be integrated, letting the LLM ask for help from different tools like Google search or SQL databases.\n\n#### Code Examples\nThe example provided uses OpenAI's Text API and Hugging Face's Agents Library. The code shows different tasks performed by the agents, like generating images or image captioning.\n\n#### Conversational Agent Example\nThe example showcases a conversation where an agent is asked to create an image, modify it, and caption it. The multitasking capabilities are impressive and are made simple to integrate into the code.\n\n#### Community Aspect\nThere is a massive community aspect to Hugging Face agents, and it is expected that the community will contribute many tools. Being able to easily access these shared tools will bring a lot of potential to the platform.",
        "output": "## Hugging Face Agents"
    },
    {
        "instruction": "<|SYSTEM|>You are a scholars that is taking markdown notes about some video with Wiki style, you would be giving some written scripts for the task.\nAnd summarize each paragraph with a title.\nRemember to keep the technology or algorithm name, very important.\nPlease write the notes in markdown format.\nHere is an example:\n### Definiation\nA deadlock is a situation in which two computer programs sharing the same resource are effectively preventing each other from accessing the resource, resulting in both programs ceasing to function. \nThe earliest computer operating systems ran only one program at a time.\n\n### What it have to do with Relationsal Database ManagementSystem\nIn a database, a deadlock is an unwanted situation in which two or more transactions are waiting indefinitely for one another to give up locks. \nDeadlock is said to be one of the most feared complications in DBMS as it brings the whole system to a Halt. \nWhen you tries to insert or update data on database, sometime your record need to depend on the row record.  \n<|ASSISTANT|>#### ",
        "input": "<|USER|> To encode you unroll. To decode you unroll. StatQuest. Hello, I'm Josh Starmer and welcome to StatQuest. Today we're going to talk about Seek to Seek and Encoder-Decoder neural networks and they're going to be clearly explained. Lightning, AI, it's the easiest way to scale your work up in the cloud. Lightning. This StatQuest is also brought to you by the letters A, B, and C. A always. B, B. C, curious. Always be curious. Note, this StatQuest assumes that you are already familiar with long short-term memory neural networks and word embedding. If not, check out the quests. Also, I want to give a special triple bam thanks to Ben Trevitt. His awesome tutorials on GitHub, the links are in the description below, made it possible for me to create this quest. Hey look, it's Statsquatch and the Normal-saurus. Hey Josh, I want to tell my friend in Spain, let's go, but I don't know Spanish. Can you help? Sure thing, Squatch. I've got a sequence of amino acids that I want translated into 3D structures like alpha helices. What? I've got a sequence of amino acid that I want translated. Don't worry Norm, I can help. Both of you have sequences of one type of thing that need to be translated into sequences of another type of thing. Both of these problems and many others are called sequence to sequence or seek to seek problems. One way to solve seek to seek problems is with something called an encoder decoder model, which we'll talk about in this StatQuest. BAM. Note, the basic seek to seek encoder decoder models that we're going to talk about today are totally awesome, but they are also a stepping stone to learning about transformers, which we'll talk about in future StatQuests. In other words, today we're taking another step in our quest to understand transformers, which form the basis of big fancy large language models like ChatGBT. Anyway, to illustrate how to solve a sequence to sequence problem using a seek to seek encoder decoder model, let's create one that translates English sentences into Spanish. Specifically, Squatch wants to tell his Spanish friend, let's go. So we'll create an encoder decoder model that can do this. First, however, let's talk a little bit more about the problem we want to solve. The first thing, which is pretty obvious, is that not all sentences in the English language are the same length. For example, someone might say, let's go. Or they might say, my name is StatSquatch. So we need something that can take different length sentences as input. Likewise, not all Spanish sentences are the same length, so we need something that can generate different length sentences as output. Lastly, the Spanish translation of an English sentence can have a different length than the original. For example, the two word English sentence, let's go, translates to the one word Spanish sentence, vamos. So we need our seek to seek encoder decoder model to be able to handle variable input and variable output lengths. The good news is that we already know how to use long short-term memory units to deal with variable length inputs and outputs. For example, if the input sentence is let's go, then we put let's into the input for the LSTM, and then unroll the LSTM, and then plug go into the second input. However, we're getting ahead of ourselves. If you remember from the word embedding StatQuest, we can't just jam words into a neural network. Instead, we use an embedding layer to convert the words into numbers. Note, to keep the example relatively simple, the English vocabulary for our encoder decoder only has three words, let's, to, and go, and this symbol, EOS, which stands for end of sentence. Oh no, it's the dreaded terminology alert. Because the vocabulary contains a mix of words and symbols, we refer to the individual elements in the vocabulary as tokens. Also note, in this example, we're just creating two embedding values per token instead of hundreds or thousands. Okay, now that we have an embedding layer for our input vocabulary, we can put it in front of the input for the LSTM. Now, when we have the input sentence, let's go, we put a one in the input for let's and a zero for everything else, and then we unroll the LSTM and the embedding layer, and put a one in the input for go and a zero for everything else. Note, to be clear, when we unroll the LSTM and the embedding layer, we reuse the exact same weights and biases no matter how many times we unroll them. In other words, the weights and biases in the LSTM cell and embedding layer that we use for the word let's are the exact same weights and biases that we use for the word go. Now, in theory, this is all we need to do to encode the input sentence, let's go, bam. However, in practice, in order to have more weights and biases to fit the model to our data, people often add additional LSTM cells to the input. To keep things simple, we'll just add one additional LSTM cell to this stage. This means that the two embedding values for the word let's are used as the input values for two different LSTM cells, and these two LSTM cells have their own separate sets of weights and biases. And when we unroll them for the word go, the original LSTM cell reuses its set of weights and biases, and the new LSTM cell reuses its separate set of weights and biases. Now, to add even more weights and biases to fit the model to our data, people often add additional layers of LSTMs. To illustrate how this works, we'll add one more LSTM layer to the encoder. What that means is that the output values, the short-term memories, or the hidden states from the unrolled LSTM units in the first layer are used as the inputs to the unrolled LSTM units in the second layer. Note, just like how both embedding values are used as inputs to both LSTM cells in the first layer, both outputs, long-term memories, or hidden states from each cell in the first layer are used as inputs to both LSTM cells in the second layer. Lastly, the only thing left to do is to initialize the long and short-term memories, the cell and hidden states. And now we're done creating the encoder part of the encoder-decoder model. In this example, we have two layers of LSTMs with two LSTM cells per layer. In essence, the encoder encodes the input sentence, Let's Go, into a collection of long and short-term memories, also known as cell and hidden states. Bam! Oh no, it's the dreaded terminology alert again. The last long and short-term memories, the cell and hidden states, from both layers of the LSTM cells in the encoder are called the context vector. Thus, the encoder encodes the input sentence, Let's Go, into the context vector. Now we need to decode the context vector. So, the first thing we do is connect the long and short-term memories, the cell and hidden states, that form the context vector to a new set of LSTMs. That, just like the encoder, have two layers and each layer has two cells. Note, to be clear, the LSTMs in the decoder are different from the ones in the encoder and have their own separate weights and biases. Anyway, the context vector is used to initialize the long and short-term memories, the cell and hidden states, in the LSTMs in the decoder. And the ultimate goal of the decoder is to decode the context vector into the output sentence. So, just like in the encoder, the input to the LSTM cells in the first layer comes from an embedding layer. However, now the embedding layer creates embedding values for the Spanish words, ir, vamos, and e, and the eos, end of sentence, symbol. In other words, this is the embedding layer that we use in the encoder and this is the embedding layer that we use in the decoder. As you can see, they have different input words and symbols, or tokens, and different weights, which result in different embedding values for each token. Now, because we just finished encoding the English sentence, let's go, the decoder starts with the embedding values for the eos, end of sentence, token. In this case, we're using the eos token to start the decoding because that is what they used in the original manuscript. However, sometimes you'll see people use sos for start of sentence. Anyway, the decoder does the math with the two layers of LSTMs, each with two LSTM cells. And the output values from the top layer of LSTM cells are transformed by additional weights and biases in what is called a fully connected layer. A fully connected layer is just another name for a basic vanilla neural network. This fully connected layer has two inputs for the two values that come from the LSTM cells in the top layer and four outputs, one for each token in the Spanish vocabulary. And in between, we have connections between each input and output with weights and biases. Then we run the output of the fully connected layer through a softmax function to pick out the output word. Now, going back to the full encoder decoder model, we see that the output from the softmax function is vamos, the Spanish translation for let's go. Double bam? Not yet. So far, the translation is correct, but the decoder doesn't stop until it outputs an EOS token. So we plug the word vamos into the decoder's unrolled embedding layer and unroll the two LSTM cells in each layer and then run the output values into the same fully connected layer. And the next predicted token is EOS. And that means we translated the English sentence let's go into the correct Spanish sentence. Vamos. Double bam? To summarize the decoder stage, the context vector created by both layers of the encoder's unrolled LSTM cells are used to initialize the LSTMs in the decoder. And the input to the LSTMs comes from the output word embedding layer that starts with EOS, but subsequently uses whatever word was predicted by the output layer. In practice, the decoder will keep predicting words until it predicts the EOS token or it hits some maximum output length. Note, by decoupling the encoder from the decoder, the input text and the translated output text can be different lengths. In this case, we translated the two word English sentence let's go to a one word Spanish sentence vamos. Bam. Note, just like for all neural networks, all of these weights and biases are trained using backpropagation. And if you're not already familiar with backpropagation, check out the quests. That said, encoder decoder models have two special things that happen during training. In this example, we use the predicted token vamos as the input to the unrolled LSTMs. In contrast, when training an encoder decoder, instead of using the predicted token as input to the decoder LSTMs, we use the known correct token. In other words, if the first predicted token was the Spanish word y, which translates to and in English, and thus is the wrong word, then during training, we'll still use vamos, the correct Spanish word, as input to the unrolled LSTMs. Also, during training, instead of just predicting tokens until the decoder predicts the EOS token, each output phrase ends where the known phrase ends. In other words, even if the second predicted token was the Spanish word ir instead of the correct token EOS, then during training, we'll still stop predicting additional tokens. Plugging in the known words and stopping at the known phrase length rather than using the predicted tokens for everything is called teacher forcing, which sounds vaguely like what happened when I took introduction to statistics in school. Lastly, let's talk about the differences between this super simple encoder decoder model and the model used in the original sequence to sequence manuscript. First, instead of just using three words and one symbol or a total of four tokens in each input and output vocabulary, the original manuscript had an input vocabulary with 160,000 tokens and an output vocabulary with 80,000 tokens. Also, instead of just two embedding values per token, the original manuscript created 1,000 and instead of two layers of LSTMs with two LSTM cells in each layer, the original manuscript used four layers with 1,000 LSTM cells per layer. Also, the output layer had 1,000 inputs from the 1,000 LSTM cells in the fourth layer and 80,000 outputs to match the size of the output vocabulary. Lastly, my simple model, which I coded in PyTorch Lightning, only has 220 weights and biases to train, but the model in the original manuscript had 384 million weights and biases to train. And that gives you a sense of the scale that these models can have. Triple BAM! Now it's time for some shameless self-promotion. If you want to review statistics and machine learning offline, check out the StatQuest PDF study guides and my book, the StatQuest Illustrated Guide to Machine Learning at statquest.org. There's something for everyone. Hooray! We've made it to the end of another exciting StatQuest. If you like this StatQuest and want to see more, please subscribe. And if you want to support StatQuest, consider contributing to my Patreon campaign, becoming a channel member, buying one or two of my original songs or a t-shirt or a hoodie, or just donate. The links are in the description below. All right, until next time, Quest On!",
        "output": "### Introduction: Sequence to Sequence (Seek to Seek) Problems\nBoth translation and 3D structure prediction are examples of sequence to sequence or seek to seek problems that could be solved using an Encoder-Decoder neural network model.\n\n### Encoder-Decoder Model\nThis model is used to handle variable input and output lengths. The main components of the model are:\n\n1. Long short-term memory (LSTM) units for dealing with variable length inputs and outputs.\n2. An embedding layer for converting words into numbers.\n\n### Encoding\nThe encoder encodes the input sentence into a context vector, consisting of long and short-term memories (cell and hidden states) from LSTM cells.\n\n### Decoding\nThe decoder initializes the long and short-term memories (cell and hidden states) in the LSTMs with the context vector, transforming it into the output sentence using additional weights and biases in a fully connected layer. The decoder repeats this process until an end-of-sentence token (EOS) is predicted or a maximum output length is reached.\n\n### Training and Teacher Forcing\nDuring training, instead of using the predicted token as input, the known correct token is used. This method is called teacher forcing. The process stops at the known phrase length rather than predicting additional tokens.\n\n### Differences between the Basic Model and Original Manuscript\nThe basic model differs from the original manuscript in terms of vocabulary size, embedding values, LSTM layers and cells, and output layer inputs and outputs. The original manuscript used larger vocabularies and more layers and cells, resulting in more weights and biases to train."
    },
    {
        "instruction": "<|SYSTEM|>You are a scholars that is taking markdown notes about some video, you would be giving some written scripts for the task.\nYou have finished writing the notes, and you want to extract the title from the notes.\nThe title should only be the name of the technology or algorithm, no extra words needed.\nPlease write the notes in markdown format. No need to write \"Title:\" for the Title. Make sure the title is within 10 words.\nHere is an example:\n## Hidden Markov Model\n<|ASSISTANT|>## ",
        "input": "<|USER|>### Introduction: Sequence to Sequence (Seek to Seek) Problems\nBoth translation and 3D structure prediction are examples of sequence to sequence or seek to seek problems that could be solved using an Encoder-Decoder neural network model.\n\n### Encoder-Decoder Model\nThis model is used to handle variable input and output lengths. The main components of the model are:\n\n1. Long short-term memory (LSTM) units for dealing with variable length inputs and outputs.\n2. An embedding layer for converting words into numbers.\n\n### Encoding\nThe encoder encodes the input sentence into a context vector, consisting of long and short-term memories (cell and hidden states) from LSTM cells.\n\n### Decoding\nThe decoder initializes the long and short-term memories (cell and hidden states) in the LSTMs with the context vector, transforming it into the output sentence using additional weights and biases in a fully connected layer. The decoder repeats this process until an end-of-sentence token (EOS) is predicted or a maximum output length is reached.\n\n### Training and Teacher Forcing\nDuring training, instead of using the predicted token as input, the known correct token is used. This method is called teacher forcing. The process stops at the known phrase length rather than predicting additional tokens.\n\n### Differences between the Basic Model and Original Manuscript\nThe basic model differs from the original manuscript in terms of vocabulary size, embedding values, LSTM layers and cells, and output layer inputs and outputs. The original manuscript used larger vocabularies and more layers and cells, resulting in more weights and biases to train.",
        "output": "## Encoder-Decoder Model"
    },
    {
        "instruction": "<|SYSTEM|>You are a scholars that is taking markdown notes about some video with Wiki style, you would be giving some written scripts for the task.\nAnd summarize each paragraph with a title.\nRemember to keep the technology or algorithm name, very important.\nPlease write the notes in markdown format.\nHere is an example:\n### Definiation\nA deadlock is a situation in which two computer programs sharing the same resource are effectively preventing each other from accessing the resource, resulting in both programs ceasing to function. \nThe earliest computer operating systems ran only one program at a time.\n\n### What it have to do with Relationsal Database ManagementSystem\nIn a database, a deadlock is an unwanted situation in which two or more transactions are waiting indefinitely for one another to give up locks. \nDeadlock is said to be one of the most feared complications in DBMS as it brings the whole system to a Halt. \nWhen you tries to insert or update data on database, sometime your record need to depend on the row record.  \n<|ASSISTANT|>#### ",
        "input": "<|USER|> At the heart of the language model revolution and the Langchain framework lies the concept of a text embedding. A text embedding is a learned representation of text that takes the form of a vector of numbers. This vector allows us to efficiently prompt and retrieve context from vector storage, to extract relevant pieces of information, enhance the language model's memory and capabilities, and ultimately take the action we want to take to generate value. In this video we're going to have a closer look at this process by means of a real-world practical application. We're going to use Langchain to extract information and value from Amazon Revu data. One of the most slam-dunk applications of Langchain is customer experience analytics. I'm going to show you how you can take the unstructured Revu data and map the Revus into themes and a structure that allows you to act on the data. I'm also going to demonstrate how the Revu embeddings can form the basis as inputs to other machine learning models and just how packed these Revus are with signal that can be used to further enhance the capabilities of the language models. An avenue we're going to explore in more detail going forward on this channel. By the end of this video you'll have an idea of how you can generate value for businesses with Langchain, how you combine vector stores with large language models, and you'll have a small POC that you can further build on and put to good use. To get started we'll pip install the needed libraries and we'll drop the needed API keys and variables into an environment file. Links to the code and the data will be available below the video. Let's go. So here we have the Amazon data that we're going to use. These are reviews for products in the fashion category. What's important to note here, there's an overall rating. There is a review text with the actual review from the customer and there's an ID that allows us to tie this to a specific product. And then we have a complimentary data set called Meta Amazon Fashion with all the product information and an ID that allows us to join this with the review data set. In our notebook the first thing we're going to do is we're going to import the utilities needed for loading the API keys from the environment file and also extract the Amazon data from the files. And we're actually not going to extract the data from the JSON files as they are quite big but directly from the SIPT files. Then I'm going to load both data sets into Pandas data frames and I'm removing reviews without a review text as we need the text for the work in this video. And here we have the review data in a data frame. And this is the metadata also in a data frame. Next I'm going to truncate reviews so that we don't process reviews that are too long and you can play around with the number of characters you want to use. And then I'm going to find a product that has a good number of reviews for the sake of this video. And I had a look at the data and it looks like the second one from the bottom is a good fit. And if we extract that product you can see the name. It's called PowerStep Pinnacle Orthotic Shoe Insole. I guess that's fashion. But this is good for our purpose. So let's create the embedding vectors from these reviews. So we're going to work on just a slice of the data frame with the PowerStep Pinnacle Insole. And as for embeddings I'm going to use Hocking Face Embeddings. And this is just to show that we don't have to use OpenAI for this. I'm going to create a new column in the data frame with the embedding vectors. So I'm going to use the apply function on the data frame. And a word of caution here. Another reason I'm not using the OpenAI embeddings is because of the OpenAI pricing model. When you're running apply with an embedding function on a large data frame you risk incurring significant costs using OpenAI. Here we're only working on a slice of the original full data frame. But the full data frame has more than 800,000 rows. So please don't run the apply function with an OpenAI embedding unless you know what you're doing. Alright, here we have the embedding vectors as a new column in the data frame. And what I'm going to do now is I'm going to show you the richness of this review data. And I'll do that by training a simple random forest machine learning model. With the embedding vectors as features and the overall rating as a target. I'll use scikit-learn to divide the data into a training sample and a test sample. And then I'm going to import a random forest regressor. So I'm treating this as a regression problem. Meaning that the prediction is on a continuum. Even though that we know that the rating is an integer. But this is fine for demonstration. And then I'm training the model on the training part of the data set. And here you can also play around with the number of estimators you're using. I found that 150 was fine to make this point. And once the fitting is done, we can evaluate by using the mean absolute error on the test part of the data set. And we see that we achieve a mean absolute error of 0.53. Which means that on average the prediction is off by around half a point. And this is with a simple non-optimized machine learning model that takes five minutes to run. If you spend some time optimizing this or building a more advanced model in PyTorch. You could probably get to 0.3 or below. So there is significant signal in this data. But why would you want to predict the rating? Well you wouldn't unless you're missing the rating for some of the reviews. What you want to do is to switch out the target variable. And use the signal in the review data to build the model. And use the signal in the review data to build machine learning models that will actually help you generate value. And these are product recommendation models, churn or retention models, propensity models, uplift models and so on. I'm now going to load the review embeddings into the vector database. And show you how we can have GPT-4 access the data and give us a summary of the reviews. I'm using GPT-4 as it's currently the most powerful language model we have to work with. But you can switch out the language model as you see fit. To upload the review embeddings we import and initiate Pinecone. Then transform the truncated review text column into a list of reviews. Then we upload the reviews with the built-in from texts method. Using hocking face embeddings. Once the upload is done we can head over to Pinecone and check that the vectors have been uploaded. And here we have all the review vectors uploaded into Pinecone. And then we can do a basic vector similarity search to check that everything works. What I want to do now is I want to have the language model access the data in the vector store. I'm going to import retrieval-qa that is used to retrieve the most relevant reviews given a prompt. And feed those to the language model. And then I'm going to import chat-openapi that I'm going to use as a wrapper around GPT-4. Then we're going to define a chain, let's call it review chain. Using retrieval-qa that takes the language model, the vector store and then the chain type as an argument. And here we use chain type stuff which means that we stuff all the related data into the prompt. And we use that as a context and pass it to the language model. Then we simply write the query as we usually do when working with chat-gpt. And we run the chain with this query. And here I'm asking GPT-4 to give us an overall impression of the reviews. And give us the most prevalent examples and bullet points. And also give us suggestions for improvement. And remember when you're working with chat models like GPT-4 you can send system messages that will allow you to calibrate the system of the model. Which could significantly improve the quality of the output. Alright so here we are with GPT-4's impression of the reviews. Some examples of good reviews, mixed reviews, negative reviews. And then based on those reviews there are some potential areas to focus on improving. So pretty nice and I'm sure you can improve that by calibrating the system. What's really powerful about this is that you can turn this into a weekly digest and send it out to an internal email list within a company. And this can also be done with Langchain. Alright so the last thing we're going to have a look at is filtered vector similarity search. And what I'd like to do is to get all the reviews of a given rating that match a specific theme. So we need to filter on metadata to do that. I'm going to use Pinecone directly as Langchain doesn't seem to support this out of the box yet. To upload the data with the Pinecone Python client I'm going to add a metadata field to the data frame and create two versions of the data frame. One for uploading and a local version for extracting the actual reviews. And then I'm going to create the Pinecone index directly with the Pinecone Python client. And while the index is initializing we can head over to the Pinecone documentation and have a look at what the upsert should look like. So we can see that we need an ID field and a value field with the vector values. And then we're going to add the metadata field used for filtering. Once the index has been initialized we can upload the data in batches. And here I'm using batches of 50. Alright now that the upload is done let's try to run a filtered query. We do that by using a query string like we do without the filter. Then add a filter that has a MongoDB like syntax. This query will give us the top 100 matching reviews with a rating of 4. And the result is a list of IDs with a score indicating the match rate. To get the actual reviews we'll need to use the local version of the data. And what I'm going to do is I'm going to wrap the query in a Python function that executes a filtered vector similarity search. Followed by looping through the local version of the data and extracting all the reviews from the local data. Now if I call this function with the query willPurchase again filtering out the ratings with 5 star reviews. Then we get all the happy customers that should be put in a repeat purchase flow. And we can do something similar to get all the disappointed customers that gave a 1 star review. Which works even though I misspelled disappointed. And we can use this in a winback flow. Being able to filter out reviews like this is very powerful as it allows you to divide reviews into themes and get the angles you need in your remarketing campaigns. In the example we just saw in the notebook we have two different themes. We have the disappointed customers and we have the customers that want to purchase again. And if you have a customer ID and an email tied to the customer review. You can use Langchains integration with Sepia to create two lists. A winback list and a repurchase list. Then use your email service provider to run two different campaigns. And that's a topic for another video. So that's it for now. If you enjoyed this video give it a like and subscribe. Thanks for watching.",
        "output": "### Introduction to Langchain and Text Embeddings\nLangchain is a framework that utilizes a text embedding, which is a learned representation of text in the form of a vector of numbers. This allows for efficient context retrieval and enhances a language model's capabilities. This video demonstrates using Langchain with Amazon review data for customer experience analytics.\n\n### Importing and Preparing Data\nThe Amazon review dataset is loaded into Pandas dataframes, removing reviews without text and truncating long reviews. Hugging Face embeddings are used to create a new column in the dataframe with embedding vectors. Caution must be exercised when using OpenAI embeddings to avoid incurring high costs with large dataframes.\n\n### Finding Signal in Review Data\nThe embedding vectors are used as features in a simple Random Forest regression model predicting overall ratings. With a mean absolute error of 0.53, the model shows promise if further optimized. The goal is not to predict the ratings but to use the signal in review data for value-generating machine learning models.\n\n### Loading Review Embeddings into Vector Database\nAfter loading the review embeddings into Pinecone, GPT-4 is used to access the data in the vector store, providing an overall impression of the reviews, examples of good and bad reviews, and suggestions for improvement. This information can be turned into a weekly digest sent to company email lists.\n\n### Filtered Vector Similarity Search\nFiltered vector similarity searches can be performed using Pinecone directly, as Langchain does not currently support this functionality. By creating an index with Pinecone, uploading data with metadata fields for filtering, and running filtered queries, top matching reviews for specific ratings can be retrieved. This allows users to divide reviews into themes for remarketing campaigns.\n\n### Conclusion\nUsing Langchain with text embeddings, review data can be analyzed for customer experience and valuable insights. Langchain's integration with Sepia allows for the creation of targeted campaigns and extraction of value from operational data."
    },
    {
        "instruction": "<|SYSTEM|>You are a scholars that is taking markdown notes about some video, you would be giving some written scripts for the task.\nYou have finished writing the notes, and you want to extract the title from the notes.\nThe title should only be the name of the technology or algorithm, no extra words needed.\nPlease write the notes in markdown format. No need to write \"Title:\" for the Title. Make sure the title is within 10 words.\nHere is an example:\n## Hidden Markov Model\n<|ASSISTANT|>## ",
        "input": "<|USER|>### Introduction to Langchain and Text Embeddings\nLangchain is a framework that utilizes a text embedding, which is a learned representation of text in the form of a vector of numbers. This allows for efficient context retrieval and enhances a language model's capabilities. This video demonstrates using Langchain with Amazon review data for customer experience analytics.\n\n### Importing and Preparing Data\nThe Amazon review dataset is loaded into Pandas dataframes, removing reviews without text and truncating long reviews. Hugging Face embeddings are used to create a new column in the dataframe with embedding vectors. Caution must be exercised when using OpenAI embeddings to avoid incurring high costs with large dataframes.\n\n### Finding Signal in Review Data\nThe embedding vectors are used as features in a simple Random Forest regression model predicting overall ratings. With a mean absolute error of 0.53, the model shows promise if further optimized. The goal is not to predict the ratings but to use the signal in review data for value-generating machine learning models.\n\n### Loading Review Embeddings into Vector Database\nAfter loading the review embeddings into Pinecone, GPT-4 is used to access the data in the vector store, providing an overall impression of the reviews, examples of good and bad reviews, and suggestions for improvement. This information can be turned into a weekly digest sent to company email lists.\n\n### Filtered Vector Similarity Search\nFiltered vector similarity searches can be performed using Pinecone directly, as Langchain does not currently support this functionality. By creating an index with Pinecone, uploading data with metadata fields for filtering, and running filtered queries, top matching reviews for specific ratings can be retrieved. This allows users to divide reviews into themes for remarketing campaigns.\n\n### Conclusion\nUsing Langchain with text embeddings, review data can be analyzed for customer experience and valuable insights. Langchain's integration with Sepia allows for the creation of targeted campaigns and extraction of value from operational data.",
        "output": "## Langchain and Text Embeddings"
    },
    {
        "instruction": "<|SYSTEM|>You are a scholars that is taking markdown notes about some video with Wiki style, you would be giving some written scripts for the task.\nAnd summarize each paragraph with a title.\nRemember to keep the technology or algorithm name, very important.\nPlease write the notes in markdown format.\nHere is an example:\n### Definiation\nA deadlock is a situation in which two computer programs sharing the same resource are effectively preventing each other from accessing the resource, resulting in both programs ceasing to function. \nThe earliest computer operating systems ran only one program at a time.\n\n### What it have to do with Relationsal Database ManagementSystem\nIn a database, a deadlock is an unwanted situation in which two or more transactions are waiting indefinitely for one another to give up locks. \nDeadlock is said to be one of the most feared complications in DBMS as it brings the whole system to a Halt. \nWhen you tries to insert or update data on database, sometime your record need to depend on the row record.  \n<|ASSISTANT|>#### ",
        "input": "<|USER|> Hugging Face just released the Transformers agent this week, basically to use large language models to connect and execute transformer models hosted on Hugging Face. If you're familiar with linechain agents, Transformers agent looks really familiar. It has a lot of similarities with linechain agent. So in this video, we're going to take a look at how Transformers agent works and how it compares with linechain agent. So this figure describes how Transformers agent works. You have a set of tools like image generator model, image captioner model, text to speech model. You can see all the model details here. For example, text to speech is using speech T5 model that's hosted on Hugging Face. Now when you give the transformer agent an instruction, read out loud the content of the image, you will feed into a prompt template where your job is to come up with a series of Python commands that will perform the task and you can use the following tools. The agent, which is a large language model, will try to understand the prompt and now determine, okay, now I will use the image captioner to caption the image and use the text to speech to read out loud. And the output of this large language model is the Python syntax because in the prompt, we ask the language model to come up with a series commands in Python. That's why the output is Python code. This Python code get feeding to Python interpreter to execute and generate the output. Let's take a look at how exactly it works in code. Let's just open up this Colab. First up, we need to install needed packages, which is Hugging Face Hub because a lot of the models are hosted on Hugging Face Hub. And then we need to log in using our Hugging Face Hub API, which is free by the way, but you will need to set up API key. We have three options for agents. You can choose star coder, Open Assistant or OpenAI. You can see the models here. The Open Assistant model is using Pythea model right now, Pythea 12b. And then for OpenAI models, you will need a OpenAI API key. And you will need to set up billing, by the way. This is not free, but if you want to choose the other two options, those are completely free. But OpenAI will give you better results. So we're going to go with OpenAI. There are two ways to use the agent. One is Agent Ron, one is Agent Chad. Agent Ron doesn't keep any of the Chad histories. Agent Chad keeps Chad histories. So here's an example where we ask the agent to generate an image of a boat in the water. And then here you can see the steps. The first step is to understand the ask and decide, okay, I will use the following tool, image generator to generate an image according to the prompt. And now the next step is to generate the code. As you can see here, we're downloading model files to actually execute this model. As a result, we can see we have a boat generated. The next example, can you caption the boat image where the boat image is the boat image we just generated with the variable boat? Because previously we defined this image as boat. Now our language model decided to use another tool, image captioner, to generate a caption for the image. And the Python code is this thing. And the answer is a boat floating in the water. Another example, can you generate an image of a boat? Please read out loud the content of the image afterwards. So this is like chaining different models together for this one. We use the image generator to generate the image. And then we use image captioner to make a caption for the image. And finally, we use the text reader to generate audio based on the caption of the image. Even though, as you can see here, the explanation from the agent only mentioned image generator and text reader, it didn't mention the image captioner, but the code it actually generated included image captioner. That's kind of interesting that it was able to generate the code even without the reasoning of needing this code. A wooden boat floating in the water. A wooden boat floating on the water, which is pretty good. This next example is to read out loud the summary of the Hugging Face page. We can see it's using the text downloader to download the text of the web page. And then we use the summarizer function to create a summary of the text. And then we use the text reader to generate the audio of the text. Hugging Face is an eye community building the future. As I mentioned earlier, the first way to use the agent is to do agent.run. The second way is to do agent.chat. Agent.chat keeps memory across different runs. So here's an example. We get an image of a capybara using the image generator function. And then we can ask the agent to transform the image so that it snows. Now we're using the image transformer tool to transform the image, which is the image transformer function here. Yeah, you can see the image was changed. We can ask another question. Show me a mask of the snowy capybaras. It was able to figure out that it needed the image segmenter to create a segmentation mask for the image. And it's using the image segmenter function. If you want to clear all the history and start from scratch, just do agent.prepare for new chat and it will clear out everything. So as a reminder, transformer agents was able to use all of the tools listed here. For example, when we create a caption of the image, we're using the blink model. When we created the audio from the text, we're using speech T5 model and so on. So those are the tools that's currently supported. As you can see, a lot of the models here are designed for specific use cases, which makes the model more accurate and make the process easy to do. For example, you can do text classification using large language models with certain prompt engineering. But with this approach, we can simply use Bart to do the text classification. It's easier. You don't need to do any prompt engineering for a specific task anymore. You just call the model that does a specific task. And also those tools give more capabilities to the large language model that they can do them alone. For example, speech T5 allows us to convert text to speech and large language model can't do that. So the last section about this document is creating new tools, which is pretty straightforward. They are using the cat as a service API, which I didn't know before. To get random cats, we created a new class cat image fetcher, which depends on the super class tool. And then in this class, we just need to make the donder call method to open the image from the API. We call the tool is actually giving us a random cat image to use the tool in agent. We define the tool in the list of the additional tools parameter. And then we run the agent. The agent will be able to use the tool cat fetcher to fetch the image and then use the image captioner to caption it. So as you can see here, we get a cat equals cat fetch, fetch her image captioner to get that caption. And now the result is a cat sitting on the top of a table. In the next section, I want to talk about how is it different or similar from line chain agents. First of all, let's talk about tools. Here are the tools available for transformer agents. You can see majority of them are models, and there are some community based tools for specific use cases. If we look at line chain tools, however, and most of them are external APIs, it's not models. So if you want to do Google search, right, you can use large language models to connect with your Google search API and do Google search. There are some other options. You can also execute Python in the Python wrapper tool. Oh, yeah, I want to talk about this one. So line chain already has Hugging Face tools, which means it can also load Hugging Face models. So I think line chain can already do a lot of things that transformer agents can do. Take a look at the code. This is a directory of tools for the transformer agents. Each tool is a standalone Python file. And if we take a look at what's inside, it's just a class of the tool. The speech to text is actually using the OpenAI Whisper model. That's interesting. You can easily contribute to this directory and add a tool. Similarly, in line chain, the tools are in this utilities directory. For example, this is the Python wrapper tool. You can see just a few lines of code for you to execute Python code. I think it should be pretty straightforward for either of them to use each other's tools. So that's tools. Secondly, I want to talk about agent or maybe prompt to be more accurate. It's how do agents choose which model to use for this specific task? Here is the prompt that transformer agents are using. Your job is to come up with a series of commands in Python that will perform that to help you. I will give you access to a set of tools that you can use. Each tool is a Python function and has description explaining the task it performs, the input it expects, and the output it returns. You should first explain which tool you will use to perform the task and for what reason and write the code in Python. And then the prompt gives it a set of tools that we looked at. And then it's like providing few-shot learning. It gives a few examples for the large language model to learn from. So we have a task, the explanation of what tool to use is the translator tool, and the answer is to use the translator function and also the image QA function and so on. So it gives a few of the answers. And finally, the user-defined prompt is added here. This is pretty straightforward. Just give you a list of tools and give you some examples to follow. For Lanching, however, it's a little bit different. There are four different agent types. Each use a different kinds of framework and tools. The default is to use a React framework to determine which tool to use. And this is based on the React paper. React is short for reasoning and acting, meaning that it does not only give an action for what to do. It also gives some thoughts. I guess it's similar to the explanation that transformer agents are using. Yeah, I guess it's similar in that sense. And finally, one of the most different things between transform agency and Lanching agents are the purposes different. Transform agents seems to have a natural scope right now, because it's only for come up with Python commands and executing Python code. Whereas for Lanching, it's not just limited to execute Python. And of course, it can execute Python, but it can also do a lot of other things. Lanching has a much bigger scope for doing pretty much everything. Yeah, so again, transformer agents are still experimental. I am looking forward to see what's going to happen next. Thank you for watching. Bye.",
        "output": "### Hugging Face Transformers Agent Introduction\nHugging Face released the Transformers agent to connect and execute transformer models hosted on Hugging Face. It is similar to Linechain agents and offers three options: Star Coder, Open Assistant, and OpenAI. The Transformer agents use various tools, such as image generator model, image captioner model, text-to-speech model, etc., and the Python syntax to generate the output.\n\n### Transformers Agent vs. Linechain Agent\nTransformers agent mainly uses models for specific tasks, whereas Linechain agent uses external APIs. Linechain tools are mostly external APIs and not models. Both offer tools that can contribute to each other's use. The prompt selection between the two agents is different. Transformers agent's scope is narrower and executes Python codes only, while Linechain agent aims for a larger scope and use.\n\n### Examples and Benefits of Transformers Agent\nTransformers agent provides capabilities that large language models cannot do alone, such as converting text to speech. The combination of tools makes the process more accurate and easier for specific tasks. Transformers agent tools can be extended or contributed to with ease by using Python code. It handles tasks like image generation, image captioning, speech to text, and text classification effectively.\n\n### Creating New Tools\nNew tools can be created for Transformers agent by creating a new class that depends on the superclass 'tool'. Examples show how a Cat Image Fetcher tool can be created and used by the agent. This demonstrates the flexibility and extensibility of creating new tools in the Transformers agent framework."
    },
    {
        "instruction": "<|SYSTEM|>You are a scholars that is taking markdown notes about some video, you would be giving some written scripts for the task.\nYou have finished writing the notes, and you want to extract the title from the notes.\nThe title should only be the name of the technology or algorithm, no extra words needed.\nPlease write the notes in markdown format. No need to write \"Title:\" for the Title. Make sure the title is within 10 words.\nHere is an example:\n## Hidden Markov Model\n<|ASSISTANT|>## ",
        "input": "<|USER|>### Hugging Face Transformers Agent Introduction\nHugging Face released the Transformers agent to connect and execute transformer models hosted on Hugging Face. It is similar to Linechain agents and offers three options: Star Coder, Open Assistant, and OpenAI. The Transformer agents use various tools, such as image generator model, image captioner model, text-to-speech model, etc., and the Python syntax to generate the output.\n\n### Transformers Agent vs. Linechain Agent\nTransformers agent mainly uses models for specific tasks, whereas Linechain agent uses external APIs. Linechain tools are mostly external APIs and not models. Both offer tools that can contribute to each other's use. The prompt selection between the two agents is different. Transformers agent's scope is narrower and executes Python codes only, while Linechain agent aims for a larger scope and use.\n\n### Examples and Benefits of Transformers Agent\nTransformers agent provides capabilities that large language models cannot do alone, such as converting text to speech. The combination of tools makes the process more accurate and easier for specific tasks. Transformers agent tools can be extended or contributed to with ease by using Python code. It handles tasks like image generation, image captioning, speech to text, and text classification effectively.\n\n### Creating New Tools\nNew tools can be created for Transformers agent by creating a new class that depends on the superclass 'tool'. Examples show how a Cat Image Fetcher tool can be created and used by the agent. This demonstrates the flexibility and extensibility of creating new tools in the Transformers agent framework.",
        "output": "## Hugging Face Transformers Agent"
    },
    {
        "instruction": "<|SYSTEM|>You are a scholars that is taking markdown notes about some video with Wiki style, you would be giving some written scripts for the task.\nAnd summarize each paragraph with a title.\nRemember to keep the technology or algorithm name, very important.\nPlease write the notes in markdown format.\nHere is an example:\n### Definiation\nA deadlock is a situation in which two computer programs sharing the same resource are effectively preventing each other from accessing the resource, resulting in both programs ceasing to function. \nThe earliest computer operating systems ran only one program at a time.\n\n### What it have to do with Relationsal Database ManagementSystem\nIn a database, a deadlock is an unwanted situation in which two or more transactions are waiting indefinitely for one another to give up locks. \nDeadlock is said to be one of the most feared complications in DBMS as it brings the whole system to a Halt. \nWhen you tries to insert or update data on database, sometime your record need to depend on the row record.  \n<|ASSISTANT|>#### ",
        "input": "<|USER|> Hello people from the future, welcome to Normalize Nerd. Today I will explain the concept behind decision trees. To be more precise, classification using decision trees. As you know me, obviously I will discuss the intuition and the underlying math behind training a decision tree and this video will contain a lot of visualizations. So I hope you are super excited. If you want to see more videos like this and stay connected with me, please subscribe to this channel and join our discord server. The link will be in the description. So let's get started. First of all, we need some data, because no algorithm can learn without any previous experience. This dataset contains two features x0 and x1. I have plotted x0 along the horizontal axis and x1 along the vertical axis. There exist two classes, green and red. If you notice carefully, the classes are not linearly separable, that is you can't just draw a line to separate them. This is intentional, because I wanna show you the true power of decision trees. So what is a decision tree? Generally speaking, it's a binary tree that recursively splits the dataset until we are left with pure leaf nodes, that is the data with only one type of class. Don't worry if you didn't get what I just said. By the end of this video, you will understand every bit of it. The best way to start is to look at a trained decision tree. So here is the decision tree classifier for this dataset. Notice that there are two kinds of nodes, decision nodes and leaf nodes. The former one contains a condition to split the data and the latter one helps us to decide the class of a new data point. How? We will see later. I wanna walk you through the tree first. Let's have the whole data at our root node. Now based on the condition of the root node, we are gonna split it. The condition is whether x0 feature is less than or equal to minus 12. In the plot, the splitting condition looks like this vertical line. Every point that lies either left or on the line satisfies this condition. We are gonna place those points in the left children of the root node. And the points that don't meet the condition will go to the right. We are gonna follow this rule for the remaining nodes also. Please notice that the left child contains only red points. So it's a pure node. We don't need to split it further. Let's come to the right child. The condition is if x0 is less than or equal to 9. Here is the splitting line and resulting child nodes. In this case, the right child is pure. So we are gonna further split the left one. Following the left child, this time the condition is x1 less than or equal to 9. Obviously here we will have a horizontal splitting line. See that in this case both the child nodes are pure. So we don't need to split further. Now the natural question is how can we use this tree to classify a new data point. Suppose the new data point has x0 and x1 as 15 and 7 respectively. First we check the condition at the root node. 15 is greater than minus 12. So it doesn't satisfy that. So we come to the right child and here it fails again. So finally we arrive at a leaf node. Luckily all the points in this node are red. So we can easily classify our new point as red. But this might not be the case for more complex data. We might not have 100% pure leaf nodes. In those cases, we perform a majority voting and assign the majority class to the test point. Let me show you how our model has divided the feature space into two regions based on the splitting criteria. Okay so that is how we use a binary decision tree for classification. At this point some of you might be thinking, so this is just a bunch of nested if-else statements. So why do we consider this as machine learning. Well yes it is just a bunch of if-else statements. But we need to know the correct conditions right. There are many many possible splitting conditions. Here I am showing a couple of them. So our model needs to learn which features to take and the corresponding correct threshold values to optimally split the data. And this is why it is machine learning. But how our model decides the optimal splits. For that let's start at the root. At this stage we have the whole data with us. We are gonna compare two splits. The first one is x1 less than or equal to 4 and the corresponding child nodes look like this. The second split condition is x0 less than or equal to minus 12. Now which split would you choose. Well if you remember our goal that is to get pure leaf nodes. We must go for the second split right. Because in this case we have successfully produced the left child with red points only. But how can our computer do the same. Well the answer lies in information theory. More precisely the model will choose the split that maximizes the information gain. To calculate information gain we first need to understand the information contained in a state. Let's look at the root state. Here the number of red points and green points is the same. Imagine we are predicting the class of a randomly picked point. Only half of the time we will be correct. Which means this state has the highest impurity or uncertainty. The way to quantify this is to use entropy. It is the measure of information contained in a state. If entropy is high then we are very uncertain about the randomly picked point. And we need more bits to describe this state. By the way, PI denotes the probability of the ith class. Here both red and green have equal probability i.e. 0.5. If we just plug in the values here, we will find the entropy corresponding to the root state is 1. Which is by the way the highest possible value of entropy. Please note that I am using log base 2 here. Using this formula, we will calculate the entropy of the remaining 4 states. You can also notice that the state with 4 red points gives the minimum entropy. That's why we call it a pure node. Now to find the information gain corresponding to a split, we need to subtract the combined entropy of the child nodes from the entropy of the parent node. Also notice that I have written weights for the child nodes. That's nothing but the relative size of a child with respect to the parent. So let's calculate the information gain. Clearly, the second split gives a greater information gain. And that's why we would choose this one. Now I have just compared two splits to show you the working. In reality, the model compares every possible split and takes the one that maximizes information gain. So the model traverses through every possible feature and feature value to search for the best feature and the corresponding threshold. It will be more clear to you when I will show you the code. Yes, in the next video, we will see how to code a decision tree from scratch. Here I wanna tell you a very important point. Decision tree is a greedy algorithm. Well, it selects the current best split that maximizes information gain. It does not backtrack and change a previous split. So all the following splits will depend on the current one. And this does not guarantee that we will get the most optimal set of splits. But yes, greedy search makes our training a lot faster and it works really good despite of its simplicity. Coming back to our tree, now it's gonna find the best split on the right child and so on. After this process is done, we will see our final model. Notice that how at every level the impurity of the states are decreasing. Okay, so by now you should have a great understanding about decision tree classifiers. In the next video, I will show you how to code such a tree from scratch that you can actually use on a real world dataset. And I will also show you another thing called Gini Index. So please subscribe and turn on the bell notification. I will see you in the next video. Thanks for watching.",
        "output": "### Decision Trees: Introduction and Classification\n\n#### Dataset and Problem\n\nWe have a dataset with two features x0 and x1 and two classes (green and red). The dataset is not linearly separable. The goal is to use decision trees to classify data points based on these features.\n\n#### Decision Tree\n\nA decision tree is a binary tree that recursively splits the dataset into pure leaf nodes, that contain only one type of class. Decision tree has two kinds of nodes - decision nodes and leaf nodes. Decision nodes contain a condition to split the data, while leaf nodes help decide the class of a new data point.\n\n#### Splitting Criteria\n\nThe model needs to learn the correct conditions for optimally splitting the data. This is done by information gain, which depends on entropy - a measure of information contained in a state. The model chooses the split that maximizes information gain by comparing every possible split.\n\n#### Greedy Algorithm and Final Model\n\nDecision tree is a greedy algorithm that selects the current best split that maximizes information gain without backtracking to change previous splits. This doesn't guarantee the most optimal set of splits but makes training faster and performs well despite its simplicity.\n\nIn the next video, we will see how to code a decision tree from scratch and discuss Gini Index."
    },
    {
        "instruction": "<|SYSTEM|>You are a scholars that is taking markdown notes about some video, you would be giving some written scripts for the task.\nYou have finished writing the notes, and you want to extract the title from the notes.\nThe title should only be the name of the technology or algorithm, no extra words needed.\nPlease write the notes in markdown format. No need to write \"Title:\" for the Title. Make sure the title is within 10 words.\nHere is an example:\n## Hidden Markov Model\n<|ASSISTANT|>## ",
        "input": "<|USER|>### Decision Trees: Introduction and Classification\n\n#### Dataset and Problem\n\nWe have a dataset with two features x0 and x1 and two classes (green and red). The dataset is not linearly separable. The goal is to use decision trees to classify data points based on these features.\n\n#### Decision Tree\n\nA decision tree is a binary tree that recursively splits the dataset into pure leaf nodes, that contain only one type of class. Decision tree has two kinds of nodes - decision nodes and leaf nodes. Decision nodes contain a condition to split the data, while leaf nodes help decide the class of a new data point.\n\n#### Splitting Criteria\n\nThe model needs to learn the correct conditions for optimally splitting the data. This is done by information gain, which depends on entropy - a measure of information contained in a state. The model chooses the split that maximizes information gain by comparing every possible split.\n\n#### Greedy Algorithm and Final Model\n\nDecision tree is a greedy algorithm that selects the current best split that maximizes information gain without backtracking to change previous splits. This doesn't guarantee the most optimal set of splits but makes training faster and performs well despite its simplicity.\n\nIn the next video, we will see how to code a decision tree from scratch and discuss Gini Index.",
        "output": "## Decision Trees"
    },
    {
        "instruction": "<|SYSTEM|>You are a scholars that is taking markdown notes about some video with Wiki style, you would be giving some written scripts for the task.\nAnd summarize each paragraph with a title.\nRemember to keep the technology or algorithm name, very important.\nPlease write the notes in markdown format.\nHere is an example:\n### Definiation\nA deadlock is a situation in which two computer programs sharing the same resource are effectively preventing each other from accessing the resource, resulting in both programs ceasing to function. \nThe earliest computer operating systems ran only one program at a time.\n\n### What it have to do with Relationsal Database ManagementSystem\nIn a database, a deadlock is an unwanted situation in which two or more transactions are waiting indefinitely for one another to give up locks. \nDeadlock is said to be one of the most feared complications in DBMS as it brings the whole system to a Halt. \nWhen you tries to insert or update data on database, sometime your record need to depend on the row record.  \n<|ASSISTANT|>#### ",
        "input": "<|USER|> Welcome back. In this video, I will discuss how to find the different performance metrics like accuracy, precision, recall and so on given the confusion matrix. In this case, we have been given a confusion matrix with 150 predictions. Out of 150 predictions, there are 100 examples where classified or predicted as yes in this case and 50 examples were predicted as no in this particular case. And out of 100, 150 examples, there are totally 50 examples are of type no and 100 examples are of type yes over here. Now, given this particular data, we need to find the different performance metrics of this particular classifier. So, first we will try to give some names to each of these particular numbers. Now, if you look at this particular 45, these 45 examples are of type no actually when they were predicted as no here. So, that is the reason you can say that it is a true negative in this case. And coming back to this particular 5, these 5 examples are actually of type no, but predicted as yes over here. So, you can say that these are false positive actually. Now, coming back to this one, this particular 5 are actually of S type, but predicted as no here. The meaning is it is false negative. And this particular 95 examples are of type yes and they were predicted as yes over here. So, you can say that it is a true positive in this case. So, given this particular data, we will try to find out the first performance metrics that is the accuracy. Accuracy is the overall correctness of this particular model over here. So, that is equivalent to the total correct predictions that is the true negative plus true positive divided by total number of predictions over here. So, that is true positive and true negative is equivalent to 45 plus 95 over here divided by total number of examples are 150. And if you calculate this particular equation, you will get a 93.33 percent over here that is the accuracy of this particular model in this case. Coming back to the next performance metrics that is the misclassification rate, the how often the classification was done wrong in this particular case. That is the total misclassification divided by total number of examples. That is this five examples are of type no and they were predicted as yes here. That is the misclassification. And this particular five examples are of type yes, but they were predicted as no here. That is nothing but the f n plus f p divided by total number of examples here. That is 10 divided by 150. That is equivalent to 6.67 percent over here. The next performance metrics is true positive rate. That is nothing but when a particular example is yes and how often that example is predicted as yes over here. So, in this case, the total yes examples are 100 and the total number of times it is predicted as yes is 95 over here. So, this will be TP divided by actual yes examples over here. That is 95 divided by 100 in this particular case. So, it is also called as sensitivity or recall. If I put those particular values in this particular case, I will get 95 percent of true positive rate or sensitivity or recall in this particular case. Coming back to the next one, that is false positive rate. That is when a particular example is no, how often it is predicted as yes over here. So, in this particular case, five times a negative example is classified as yes over here. That is nothing but five divided by 50 in this particular case. That is f p. That's a false positive divided by actual no in this case, which is equivalent to 10 percent over here. Coming back to the next one, that is true negative. That is when a particular example is negative, that is negative. How often it is predicted as no here. So, in this case, it is predicted as no in 45 examples out of 50. So, that's the reason you can say that true negative is 45 divided by 50. That is equivalent to 90. And it is also called as specificity in this case. The next performance matrix is called as precision. This is nothing but when it predicted as yes, how often it is correct. For example, in this case, 95 percent of the time or 95 times examples were classified as yes. And how often it is correct over here. So, that is nothing but the total number of yes predictions divided by that is number of times the example is predicted as yes divided by total yes predictions over here. That is t p divided by predicted yes over here. That is 95 divided by 100. Again, it is 95 percent over here. And the last one is something called as prevalence. That is nothing but how often does yes condition actually occur in our sample. That is nothing but actual yes divided by total number of examples. Actual yes is actually 95 and actual yes is actually 100 divided by total number of examples are 150 over here. That is nothing but 66.67 percent over here. Different performance matrix we usually use in machine learning model to know the performance of that particular model. I have explained how to calculate these particular values with a simple example. I hope this particular concept is clear. If you like the video, do like and share with your friends. Press the subscribe button for more videos. Press the bell icon for regular updates. Thank you for watching.",
        "output": "### Performance Metrics for Confusion Matrix\n\n#### Given Confusion Matrix\nA given confusion matrix has 150 predictions with 100 examples predicted as 'yes' and 50 examples predicted as 'no'. There are 50 'no' type examples and 100 'yes' type examples.\n\n#### Definitions\n- True Negative: 45, actual type is 'no' and predicted as 'no'\n- False Positive: 5, actual type is 'no' but predicted as 'yes'\n- False Negative: 5, actual type is 'yes' but predicted as 'no'\n- True Positive: 95, actual type is 'yes' and predicted as 'yes'\n\n#### Performance Metrics Calculation\n1. Accuracy: (TN + TP) / Total predictions = (45 + 95) / 150 = 93.33%\n2. Misclassification Rate: (FN + FP) / Total predictions = 10 / 150 = 6.67%\n3. True Positive Rate (Sensitivity or Recall): TP / Actual Yes = 95 / 100 = 95%\n4. False Positive Rate: FP / Actual No = 5 / 50 = 10%\n5. True Negative Rate (Specificity): TN / Actual No = 45 / 50 = 90%\n6. Precision: TP / Predicted Yes = 95 / 100 = 95%\n7. Prevalence: Actual Yes / Total predictions = 100 / 150 = 66.67%\n\nThese different performance metrics are used to evaluate machine learning model performance."
    },
    {
        "instruction": "<|SYSTEM|>You are a scholars that is taking markdown notes about some video, you would be giving some written scripts for the task.\nYou have finished writing the notes, and you want to extract the title from the notes.\nThe title should only be the name of the technology or algorithm, no extra words needed.\nPlease write the notes in markdown format. No need to write \"Title:\" for the Title. Make sure the title is within 10 words.\nHere is an example:\n## Hidden Markov Model\n<|ASSISTANT|>## ",
        "input": "<|USER|>### Performance Metrics for Confusion Matrix\n\n#### Given Confusion Matrix\nA given confusion matrix has 150 predictions with 100 examples predicted as 'yes' and 50 examples predicted as 'no'. There are 50 'no' type examples and 100 'yes' type examples.\n\n#### Definitions\n- True Negative: 45, actual type is 'no' and predicted as 'no'\n- False Positive: 5, actual type is 'no' but predicted as 'yes'\n- False Negative: 5, actual type is 'yes' but predicted as 'no'\n- True Positive: 95, actual type is 'yes' and predicted as 'yes'\n\n#### Performance Metrics Calculation\n1. Accuracy: (TN + TP) / Total predictions = (45 + 95) / 150 = 93.33%\n2. Misclassification Rate: (FN + FP) / Total predictions = 10 / 150 = 6.67%\n3. True Positive Rate (Sensitivity or Recall): TP / Actual Yes = 95 / 100 = 95%\n4. False Positive Rate: FP / Actual No = 5 / 50 = 10%\n5. True Negative Rate (Specificity): TN / Actual No = 45 / 50 = 90%\n6. Precision: TP / Predicted Yes = 95 / 100 = 95%\n7. Prevalence: Actual Yes / Total predictions = 100 / 150 = 66.67%\n\nThese different performance metrics are used to evaluate machine learning model performance.",
        "output": "## Confusion Matrix Performance Metrics"
    },
    {
        "instruction": "<|SYSTEM|>You are a scholars that is taking markdown notes about some video with Wiki style, you would be giving some written scripts for the task.\nAnd summarize each paragraph with a title.\nRemember to keep the technology or algorithm name, very important.\nPlease write the notes in markdown format.\nHere is an example:\n### Definiation\nA deadlock is a situation in which two computer programs sharing the same resource are effectively preventing each other from accessing the resource, resulting in both programs ceasing to function. \nThe earliest computer operating systems ran only one program at a time.\n\n### What it have to do with Relationsal Database ManagementSystem\nIn a database, a deadlock is an unwanted situation in which two or more transactions are waiting indefinitely for one another to give up locks. \nDeadlock is said to be one of the most feared complications in DBMS as it brings the whole system to a Halt. \nWhen you tries to insert or update data on database, sometime your record need to depend on the row record.  \n<|ASSISTANT|>#### ",
        "input": "<|USER|> Recurrent neural nets. They are feedforward neural networks rolled out over time. As such, they deal with sequence data, where the input has some defined ordering. This gives rise to several types of architectures. The first is vector-to-sequence models. These neural nets take in a fixed-size vector as input, and it outputs a sequence of any length. In image captioning, for example, the input can be a vector representation of an image, and the output sequence is a sentence that describes the image. The second type is a sequence-to-vector model. These neural networks take in a sequence as input and spits out a fixed-length vector. In sentiment analysis, the movie review is an input, and a fixed-size vector is the output indicating how good or bad this person thought the movie was. Sequence-to-sequence models is the more popular variant, and these neural networks take in a sequence as input and outputs another sequence. So, for example, language translation. The input could be a sentence in Spanish, and the output is the translation in English. Do you have some time series data to model? Well, RNNs would be the go-to. However, RNNs have some problems. RNNs are slow. So slow that we use a truncated version of backpropagation to train it. And even that's too hardware-intense. And also, they can't deal with long sequences very well. We get gradients that vanish and explode if the network is too long. In comes LSTM Networks in 1991 that introduced a long short-term memory cell in place of dumb neurons. This cell has a branch that allows past information to skip a lot of the processing of the current cell and move on to the next. This allows the memory to be retained for longer sequences. Now, to that second point, we seem to be able to deal with longer sequences well. Or are we? Well, kind of. Probably if the order of hundreds of words instead of a thousand words. However, to the first point, normal RNNs are slow. But LSTMs are even slower. They're more complex. For these RNN and LSTM networks, input data needs to be passed sequentially or serially, one after the other. We need inputs of the previous state to make any operations on the current state. Such sequential flow does not make use of today's GPUs very well, which are designed for parallel computation. So question. How can we use parallelization for sequential data? In 2017, the transformer neural network architecture was introduced. The network employs an encoder-decoder architecture, much like recurrent neural nets. The difference is that the input sequence can be passed in parallel. Consider translating a sentence from English to French. I'll use this as a running example throughout the video. With an RNN encoder, we pass an input English sentence one word after the other. The current word's hidden state has dependencies in the previous word's hidden state. The word embeddings are generated one time step at a time. With a transformer encoder, on the other hand, there is no concept of time step for the input. We pass in all the words of the sentence simultaneously and determine the word embeddings simultaneously. So how is it doing this? Let's pick apart the transformer architecture. I'll make multiple passes on the explanation. The first pass will be like a high overview and the next rounds will get into more details. Let's start off with input embeddings. Computers don't understand words. They get numbers, they get vectors, and matrices. The idea is to map every word to a point in space where similar words and meaning are physically closer to each other. The space in which they are present is called an embedding space. We could pre-train this embedding space to save time, or even just use an already pre-trained embedding space. This embedding space maps a word to a vector. But the same word in different sentences may have different meanings. This is where positional encoders come in. It's a vector that has information on distances between words and the sentence. The original paper uses a sine and cosine function to generate this vector, but it could be any reasonable function. After passing the English sentence through the input embedding and applying the positional encoding, we get word vectors that have positional information. That is, context. Nice. We pass this into the encoder block where it goes through a multi-headed attention layer and a feed-forward layer. OK, one at a time. Attention. It involves answering, what part of the input should I focus on? If we are translating from English to French, and we are doing self-attention, that is attention with respect to oneself, the question we want to answer is, how relevant is the i-th word in the English sentence relevant to other words in the same English sentence? This is represented in the i-th attention vector, and it is computed in the attention block. For every word, we can have an attention vector generated which captures contextual relationships between words in the sentence. So that's great. The other important unit is a feed-forward net. This is just a simple feed-forward neural network that is applied to every one of the attention vectors. These feed-forward nets are used in practice to transform the attention vectors into a form that is digestible by the next encoder block or decoder block. Now, that's the high-level overview of the encoder components. Let's talk about the decoder now. During the training phase for English to French, we feed the output French sentence to the decoder. But remember, computers don't get language. They get numbers, vectors, and matrices. So we process it using the input embedding to get the vector form of the word. And then we add a positional vector to get the notion of context of the word in a sentence. We pass this vector, finally, into a decoder block that has three main components, two of which are similar to the encoder block. The self-attention block generates attention vectors for every word in the French sentence to represent how much each word is related to every word in the same sentence. These attention vectors and vectors from the encoder are passed into another attention block. Let's call this the encoder-decoder attention block since we have one vector from every word in the English and French sentences. This attention block will determine how related each word vector is with respect to each other. And this is where the main English to French word mapping happens. The output of this block is attention vectors for every word in English and French sentence, each vector representing the relationships with other words in both the languages. Next, we pass each attention vector to a feed-forward unit. This makes the output vector more digestible by either the next decoder block or a linear layer. Now, the linear layer is, surprise, surprise, another feed-forward connected layer. It's used to expand the dimensions into the number of words in the French language. The softmax layer transforms it into a probability distribution, which is now human-interpretable. And the final word is the word corresponding to the highest probability. Overall, this decoder predicts the next word, and we execute this over multiple time steps until the end-of-sentence token is generated. That's our first pass over the explanation of the entire network architecture for transformers. But let's go over it again, but this time introduce even more details, going deeper. An input English sentence is converted into an embedding to represent meaning. We add a positional vector to get the context of the word in the sentence. Our attention block computes the attention vectors for each word. Only problem here is that the attention vector may not be too strong. For every word, the attention vector may weight its relation with itself much higher. It's true, but it's useless. We are more interested in interactions with different words. And so we determine like eight such attention vectors per word, and take a weighted average to compute the final attention vector for every word. Since we use multiple attention vectors, we call it the multi-head attention block. The attention vectors are passed in through a feed-forward net, one vector at a time. The cool thing is that each of the attention nets are independent of each other, so we can use some beautiful parallelization here. Because of this, we can pass all our words at the same time into the encoder block, and the output will be a set of encoded vectors for every word. Now the decoder. We first obtain the embedding of French words to encode meaning. Then add the positional value to retain context. They are then passed to the first attention block. The paper calls this the masked attention block. Why is this the case though? It's because while generating the next French word, we can use all the words from the English sentence, but only the previous words of the French sentence. If we are going to use all the words in the French sentence, then there would be no learning. It would just spit out the next word. So while performing parallelization with matrix operations, we make sure that the matrix will mask the words appearing later by transforming it into zeros, so the attention network can't use them. The next attention block, which is the encoder-decoder-attention block, generates similar attention vectors for every English and French word. These are passed into the feedforward layer, linear layer, and the softmax layer to predict the next word. That's the pass 2 over the architecture explain. I hope you are understanding more and more details here. Now for the next pass, where we go even deeper. How exactly do these multi-headed attention networks look? Now, the single-headed attention looks like this. Q, K, and V are abstract vectors that extract different components of an input word. We have Q, K, and V vectors for every single word. We use these to compute the attention vectors for every word using this kind of formula. For multi-headed attention, we have multiple weight matrices, WQ, WK, and WV, so we will have multiple attention vectors, z, for every word. However, our neural net is only expecting one attention vector per word, so we use another weighted matrix, WZ, to make sure that the output is still an attention vector per word. Additionally, after every layer, we apply some form of normalization. Typically, we would apply batch normalization. This smoothens out the loss surface, making it easier to optimize while using larger learning rates. This is the TLDR, but that's what it does. But we can actually use something called layer normalization, making the normalization across each feature instead of each sample. It's better for stabilization. If you are interested in dabbling in transformer code, TensorFlow has a step-by-step tutorial that can get you up to speed. Transformer neural nets have largely replaced LSTM nets for sequence-to-vector, sequence-to-sequence, and vector-to-sequence problems. Google, for example, created BERT, which uses transformers to pre-train models for common NLP tasks. Read that blog. It's good. However, there was another paper called Pervasive Attention that could be even better than transformers for sequence-to-sequence models. Although transformers can be better suited for a wider variety of problems, it's still a very interesting read. I'll link it in the description below with other resources. So check that out. Hope this helped you get you up to speed with transformer neural nets. If you liked the video, hit that like button. Subscribe to stay up to date with some deep learning and machine learning knowledge, and I will see you guys in the next one. Bye-bye.",
        "output": "### Recurrent Neural Networks (RNNs) and Long Short-Term Memory (LSTM)\nRNNs are feedforward neural networks rolled out over time, allowing for handling sequence data. There are three types of architectures: vector-to-sequence, sequence-to-vector, and sequence-to-sequence models. However, RNNs can be slow and struggle with long sequences. LSTM networks were introduced in 1991 to improve memory retention and deal with longer sequences.\n\n### Transformer Architecture: High-Level Overview\nIntroduced in 2017, the transformer architecture employs an encoder-decoder design, allowing input sequences to be passed in parallel. It utilizes input embeddings, positional encoders, multiple encoder blocks, an attention mechanism, and feed-forward layers.\n\n### Encoder and Decoder Components\nThe encoder maps words to vectors and positional encoders add context information. Encoders use multi-headed attention layers and feed-forward layers. The decoder focuses on predicting the next word using self-attention and encoder-decoder attention blocks.\n\n### Multi-Headed Attention Networks\nThese networks have multiple weight matrices, resulting in multiple attention vectors per word. Attention vectors are generated, normalized, and passed through the encoder and decoder blocks.\n\n### Transformers in Real-World Applications\nTransformers have largely replaced LSTM networks for sequence data processing tasks. Google's BERT uses transformers for pre-training common NLP tasks. Other research, such as Pervasive Attention, may provide improvements over transformer models, but they remain a powerful tool in various contexts."
    },
    {
        "instruction": "<|SYSTEM|>You are a scholars that is taking markdown notes about some video, you would be giving some written scripts for the task.\nYou have finished writing the notes, and you want to extract the title from the notes.\nThe title should only be the name of the technology or algorithm, no extra words needed.\nPlease write the notes in markdown format. No need to write \"Title:\" for the Title. Make sure the title is within 10 words.\nHere is an example:\n## Hidden Markov Model\n<|ASSISTANT|>## ",
        "input": "<|USER|>### Recurrent Neural Networks (RNNs) and Long Short-Term Memory (LSTM)\nRNNs are feedforward neural networks rolled out over time, allowing for handling sequence data. There are three types of architectures: vector-to-sequence, sequence-to-vector, and sequence-to-sequence models. However, RNNs can be slow and struggle with long sequences. LSTM networks were introduced in 1991 to improve memory retention and deal with longer sequences.\n\n### Transformer Architecture: High-Level Overview\nIntroduced in 2017, the transformer architecture employs an encoder-decoder design, allowing input sequences to be passed in parallel. It utilizes input embeddings, positional encoders, multiple encoder blocks, an attention mechanism, and feed-forward layers.\n\n### Encoder and Decoder Components\nThe encoder maps words to vectors and positional encoders add context information. Encoders use multi-headed attention layers and feed-forward layers. The decoder focuses on predicting the next word using self-attention and encoder-decoder attention blocks.\n\n### Multi-Headed Attention Networks\nThese networks have multiple weight matrices, resulting in multiple attention vectors per word. Attention vectors are generated, normalized, and passed through the encoder and decoder blocks.\n\n### Transformers in Real-World Applications\nTransformers have largely replaced LSTM networks for sequence data processing tasks. Google's BERT uses transformers for pre-training common NLP tasks. Other research, such as Pervasive Attention, may provide improvements over transformer models, but they remain a powerful tool in various contexts.",
        "output": "Recurrent Neural Networks and Long Short-Term Memory"
    },
    {
        "instruction": "<|SYSTEM|>You are a scholars that is taking markdown notes about some video with Wiki style, you would be giving some written scripts for the task.\nAnd summarize each paragraph with a title.\nRemember to keep the technology or algorithm name, very important.\nPlease write the notes in markdown format.\nHere is an example:\n### Definiation\nA deadlock is a situation in which two computer programs sharing the same resource are effectively preventing each other from accessing the resource, resulting in both programs ceasing to function. \nThe earliest computer operating systems ran only one program at a time.\n\n### What it have to do with Relationsal Database ManagementSystem\nIn a database, a deadlock is an unwanted situation in which two or more transactions are waiting indefinitely for one another to give up locks. \nDeadlock is said to be one of the most feared complications in DBMS as it brings the whole system to a Halt. \nWhen you tries to insert or update data on database, sometime your record need to depend on the row record.  \n<|ASSISTANT|>#### ",
        "input": "<|USER|> Recurrent neural networks, you just unroll them Bam StatQuest Hello, I'm Josh Starmer and welcome to StatQuest. Today, we're gonna talk about recurrent neural networks and they're gonna be clearly explained Lightning and grid are totally cool. Check them out when you've got some time Note this StatQuest assumes that you are already familiar with the main ideas behind neural networks backpropagation and the ReLU activation function. If not, check out the quests Also note, although basic or vanilla Recurrent neural networks are awesome They are usually thought of as a stepping stone to understanding fancier things like long short-term memory networks and transformers which we will talk about in future StatQuests In other words, every quest worth taking takes steps and this is the first step So with that said let's say hi to StatSquatch. Hi And StatSquatch says hello the other day I bought stock in a company called get rich quick, but the next day their stock price went down and I lost money wah wah So I was thinking maybe we could create a neural network to predict stock prices wouldn't that be cool That sure would be cool Squatch Unfortunately, the actual stock market is crazy complicated And we'd probably both get in a lot of trouble if we offered advice on how to make money with it But if we go to that mystical place called Statland things are much simpler and there are far fewer lawyers So let's build a neural network that predicts stock prices in Statland However, first let's just talk about stock market data in general When we look at stock prices, they tend to change over time For example, the price of this stock went up for four days hooray before going down wah wah Also, the longer a company has been traded on the stock market the more data will have for it For example, we have more time points for the company represented by the blue line Then we have for the company represented by the red line What that means is if we want to use a neural network to predict stock prices Then we need a neural network that works with different amounts of sequential data In other words if we want to predict the stock price for the blue line company on day 10 Then we might want to use the data from all nine of the preceding days In contrast if we wanted to predict the stock price for the red line company on day 10 Then we would only have data for the preceding five days So we need the neural network to be flexible in terms of how much sequential data we use to make a prediction This is a big difference compared to the other neural networks we've looked at in this series For example in neural networks clearly explained we examined a neural network that made predictions using one input value no more and no less and If you saw the stat quest on neural networks with multiple inputs and outputs you saw this neural network that made predictions using two input values no more and no less and In the stat quest on deep learning image Classification you saw a neural network that made a prediction using an image that was six pixels by six pixels No bigger and no smaller However, and now we need a neural network that can make a prediction using the nine values We have for the blue company and make a prediction using the five values we have for the red company The good news is that one way to deal with the problem of having different amounts of input values is to use a recurrent neural network Just like the other neural networks that we've seen before recurrent neural networks have weights biases layers and activation functions the big difference is that recurrent neural networks also have feedback loops and Although this neural network may look like it only takes a single input value The feedback loop makes it possible to use sequential input values like stock market prices collected over time to make predictions to understand how Exactly this recurrent neural network can make predictions with sequential input values Let's run some of Statland stock market data through it In Statland if the price of a stock is low for two days in a row Then more often than not the price remains low on the next day In other words if yesterday and today's stock price is low Then tomorrow's price should also be low in contrast if yesterday's price was low and today's price is medium then tomorrow's price should be even higher and When the price decreases from high to medium then tomorrow's price will be even lower Lastly if the price stays high for two days in a row then the price will be high tomorrow Now that we see the general trends in stock prices in Statland We can talk about how to run yesterday and today's data through a recurrent neural network to predict tomorrow's price The first thing we'll do is scale the prices so that low equals zero medium equals 0.5 and high equals 1 Now let's run the values for yesterday and today Through this recurrent neural network and see if it can correctly predict tomorrow's value Now because the recurrent neural network has a feedback loop We can enter yesterday and today's values into the input sequentially We'll start by plugging yesterday's value into the input Now we can do the math just like we would for any other neural network At this point the output from the activation function the y-axis coordinate that we'll call y sub 1 Can go two places First y sub 1 can go towards the output and if we go that way and do the math Then the output is the predicted value for today However, we're not interested in the predicted value for today because we already have the actual value for today Instead we want to use both yesterday and today's value to predict tomorrow's value So for now, we'll ignore this output and Instead focus on what happens with this feedback loop The key to understanding how the feedback loop works is this summation The summation allows us to add y sub 1 times W sub 2 which is based on yesterday's value To the value from today times W sub 1 in Other words the feedback loop allows both yesterday and today's values to influence the prediction Hey, this feedback loop has got me all turned around. Is there an easier way to see how this works? Yes, there's an easier way to see what's going on Instead of having to remember which value is in the loop and which value is in the input We can unroll the feedback loop by making a copy of the neural network for each input value Now instead of pointing the feedback loop to the sum in the first copy We can point it to the sum in the second copy by unrolling the recurrent neural network we end up with a new network that has two inputs and two outputs the first input is for yesterday's value and If we do the math straight through to the first output like we did earlier we get the predicted value for today However, as we saw earlier we can ignore this output the second input is for today's value and the connection between the first activation function and the second summation Allows both yesterday and today's values to influence the final output Which gives us the predicted value for tomorrow? Now when we put yesterday's value into the first input and we do the math just like before Then we follow the connection from the first activation function to the summation in the second copy of the neural network Now we put today's value into the second input and keep doing the math And that gives us the predicted value for tomorrow zero Which is consistent with the original observation in other words the recurrent neural network correctly predicted tomorrow's value BAM Likewise when we run yesterday and today's values for the other scenarios through the recurrent neural network We predict the correct values for tomorrow Double BAM This recurrent neural network performs great with two days worth of data, but what if we have three days of data? When we want to use three days of data to make a prediction about tomorrow's price like this Then we just keep unrolling the recurrent neural network until we have an input for each day of data Then we plug the values into the inputs always from the oldest to the newest in this case That means we start by plugging in the value for the day before yesterday then we plug in yesterday's value and then we plug in today's value and When we do the math the last output gives us the prediction for tomorrow BAM note Regardless of how many times we unroll a recurrent neural network the weights and biases are shared across every input In other words even though this unrolled network has three inputs the weight w sub 1 is the same for all three inputs and The bias B sub 1 is also the same for all three inputs Likewise all of the other weights and biases are shared So no matter how many times we unroll a recurrent neural network We never increase the number of weights and biases that we have to train triple BAM Okay now that we've talked about what makes basic recurrent networks so cool Let's briefly talk about why they are not used very often One big problem is that the more we unroll a recurrent neural network the harder it is to train This problem is called the vanishing exploding gradient problem Which is also known as the hey wait where'd the gradient go kaboom? problem in our example the vanishing Exploding gradient problem has to do with the weight along the squiggle that we copy each time we unroll the network Note to make it easier to understand the vanishing Exploding gradient problem. We're going to ignore the other weights and biases in this network and just focus on w sub 2 Also just to remind you when we optimize neural networks with back propagation We first find the derivatives or gradients for each parameter We then plug those gradients into the gradient descent algorithm to find the parameter values that minimize a loss function Like the sum of the squared residuals BAM Now even though the vanishing slash exploding gradient problem starts with vanishing We're going to start by showing how a gradient can explode kaboom In our example the gradient will explode when we set w sub 2 to any value larger than 1 So let's set w sub 2 equal to 2 now the first input value input sub 1 will be multiplied by 2 on the first squiggle and then multiplied by 2 on the next squiggle and again on the next squiggle and again on the last squiggle In other words since we unrolled the recurrent neural network four times We multiply the input value by w sub 2 which is 2 raised to the number of times we unrolled which is 4 and That means the first input value is amplified 16 times before it gets to the final copy of the network Now if we have 50 sequential days of stock market data, which to be honest really isn't that much data then we would unroll the network 50 times and 2 raised to the 50 power is a huge number and This huge number is why they call this an exploding gradient problem kaboom If we tried to train this recurrent neural network with back propagation this huge number would find its way into some of the gradients and That would make it hard to take small steps to find the optimal weights and biases In other words in order to find the parameter values that give us the lowest value for the loss function We usually want to take relatively small steps BAM However, when the gradient contains a huge number then we'll end up taking relatively large steps And instead of finding the optimal parameter, we'll just bounce around a lot wah wah One way to prevent the exploding gradient problem would be to limit w sub 2 to values less than 1 However, this results in the vanishing gradient problem. Hey, wait, where'd the gradient go? To illustrate the vanishing gradient problem. Let's set w sub 2 to 0.5 Now just like before we multiply the first input by w sub 2 raised to the number of times we unroll the network So if we have 50 sequential input values that means multiplying input sub 1 by 0.5 raised to the 50th power and 0.5 raised to the 50th power is a number super close to 0 Because this number is super close to 0 this is called the vanishing gradient problem Now when optimizing a parameter instead of taking steps that are too large we end up taking steps that are too small And as a result we end up hitting the maximum number of steps were allowed to take before we find the optimal value wah wah Hey Josh, these vanishing slash exploding gradients are a total bummer. Is there anything we could do about them? Yes, and we'll talk about a popular solution called long short-term memory networks in the next stat quest Ka-bam Now it's time for some Shameless self-promotion if you want to review statistics and machine learning offline Check out my book the stat quest illustrated guide to machine learning at stat quest o RG It's over 300 pages of total awesomeness Hooray we've made it to the end of another exciting stat quest if you like this stat quest and want to see more Please subscribe and if you want to support stat quest consider contributing to my patreon campaign Becoming a channel member buying one or two of my original songs or a t-shirt or a hoodie or just donate the links are In the description below. All right until next time quest on",
        "output": "### Introduction to Recurrent Neural Networks (RNNs)\nRecurrent neural networks (RNNs) are neural networks with feedback loops, allowing them to work with different amounts of sequential data to make predictions. They have weights, biases, layers, and activation functions, like other neural networks.\n\n### RNNs for Predicting Stock Prices\nTo make predictions about stock prices or any other time-sequential data, we can use an RNN. A key feature of the RNN is its ability to adapt its input to the amount of data available.\n\n### Unrolling the Feedback Loop in RNNs\nThe feedback loop in an RNN can be \"unrolled\" by creating a copy of the neural network for each input value. This makes it easier to understand how the network operates with sequential data. Regardless of how many times an RNN is unrolled, the weights and biases remain the same across all inputs.\n\n### Vanishing/Exploding Gradient Problem in RNNs\nThe vanishing/exploding gradient problem is a significant difficulty when training RNNs, particularly when dealing with long sequences of data. Gradients can \"explode\" when some weights become too large, causing difficulty in finding optimal weights and biases. Conversely, gradients can \"vanish\" when some weights become too small, also making it difficult to find optimal parameters.\n\n### Long Short-Term Memory (LSTM) Networks\nLong short-term memory (LSTM) networks are a popular solution to the vanishing/exploding gradient problem, and will be discussed further in later StatQuests.\n\n### Conclusion\nRecurrent neural networks allow for predictions based on sequential data, with the ability to adapt to different amounts of data. They can be used to predict stock prices when trained and optimized correctly. However, the vanishing/exploding gradient problem can create challenges when training RNNs, making LSTM networks an essential solution for such cases."
    },
    {
        "instruction": "<|SYSTEM|>You are a scholars that is taking markdown notes about some video, you would be giving some written scripts for the task.\nYou have finished writing the notes, and you want to extract the title from the notes.\nThe title should only be the name of the technology or algorithm, no extra words needed.\nPlease write the notes in markdown format. No need to write \"Title:\" for the Title. Make sure the title is within 10 words.\nHere is an example:\n## Hidden Markov Model\n<|ASSISTANT|>## ",
        "input": "<|USER|>### Introduction to Recurrent Neural Networks (RNNs)\nRecurrent neural networks (RNNs) are neural networks with feedback loops, allowing them to work with different amounts of sequential data to make predictions. They have weights, biases, layers, and activation functions, like other neural networks.\n\n### RNNs for Predicting Stock Prices\nTo make predictions about stock prices or any other time-sequential data, we can use an RNN. A key feature of the RNN is its ability to adapt its input to the amount of data available.\n\n### Unrolling the Feedback Loop in RNNs\nThe feedback loop in an RNN can be \"unrolled\" by creating a copy of the neural network for each input value. This makes it easier to understand how the network operates with sequential data. Regardless of how many times an RNN is unrolled, the weights and biases remain the same across all inputs.\n\n### Vanishing/Exploding Gradient Problem in RNNs\nThe vanishing/exploding gradient problem is a significant difficulty when training RNNs, particularly when dealing with long sequences of data. Gradients can \"explode\" when some weights become too large, causing difficulty in finding optimal weights and biases. Conversely, gradients can \"vanish\" when some weights become too small, also making it difficult to find optimal parameters.\n\n### Long Short-Term Memory (LSTM) Networks\nLong short-term memory (LSTM) networks are a popular solution to the vanishing/exploding gradient problem, and will be discussed further in later StatQuests.\n\n### Conclusion\nRecurrent neural networks allow for predictions based on sequential data, with the ability to adapt to different amounts of data. They can be used to predict stock prices when trained and optimized correctly. However, the vanishing/exploding gradient problem can create challenges when training RNNs, making LSTM networks an essential solution for such cases.",
        "output": "## Recurrent Neural Networks"
    },
    {
        "instruction": "<|SYSTEM|>You are a scholars that is taking markdown notes about some video with Wiki style, you would be giving some written scripts for the task.\nAnd summarize each paragraph with a title.\nRemember to keep the technology or algorithm name, very important.\nPlease write the notes in markdown format.\nHere is an example:\n### Definiation\nA deadlock is a situation in which two computer programs sharing the same resource are effectively preventing each other from accessing the resource, resulting in both programs ceasing to function. \nThe earliest computer operating systems ran only one program at a time.\n\n### What it have to do with Relationsal Database ManagementSystem\nIn a database, a deadlock is an unwanted situation in which two or more transactions are waiting indefinitely for one another to give up locks. \nDeadlock is said to be one of the most feared complications in DBMS as it brings the whole system to a Halt. \nWhen you tries to insert or update data on database, sometime your record need to depend on the row record.  \n<|ASSISTANT|>#### ",
        "input": "<|USER|> Hello people from the future, welcome to normalize nerd. Today I am gonna talk about hidden Markov models. This is a concept derived from our old friend Markov chains. And it is very useful in bioinformatics, natural language processing, speech recognition and in many other domains. In this video, I will go through both intuition and the mathematics behind it. So make sure to watch this video till the end. And yes, I will also show you how Bayes theorem helps us in this process. So if you find value in this video, please consider subscribing and hit the bell icon, that will help me a lot. So let's get started. Well, to understand hidden Markov models, obviously you need to know what a Markov chain really is. So I would highly recommend you to watch my previous videos before continuing with this one. Okay, let's start our discussion with our friend Jack who lives in a hypothetical town. There exist only three kinds of weather in the town. Rainy, cloudy and sunny. On any given day, only one of them will occur. And the weather tomorrow depends only on today's weather. Yes, we can model this as a simple Markov chain. Let me add the state transitions. And here are the transition probabilities. Also assume that on any given day, Jack can have one of the two following moods, sad or happy. Moreover, his mood depends on the weather of that particular day. I'm gonna represent it with red arrows. Here are the corresponding probabilities. So according to our diagram, there's a 90% chance that Jack is sad given that it's a rainy day. Hopefully, it's clear to you too. Okay, so now our problem is, we don't live in Jack's town. So we can't possibly know what's the weather on a particular day. However, we can contact Jack over the internet and get to know his mood. So the states of the Markov chain are unknown or hidden from us. But we can observe some variables that are dependent on the states. And this my friend is called a hidden Markov model. In other words, a hidden Markov model consists of an ordinary Markov chain and a set of observed variables. Please note that Jack's mood, that is the observed variable, depends only on today's weather, not on the previous day's mood. To make things more clear, I'm gonna write the probabilities into a matrix. You are already familiar with the green matrix that is the transition matrix. And the red one captures the probabilities corresponding to the observed variables, which is also known as the emission matrix. Let me add the indices to make it more interpretable. This is the correct time to pause and convince yourself that you have understood everything so far. Okay, let's consider a particular scenario. We will look at three consecutive days. Suppose it was sunny and Jack was happy. Next day was cloudy and Jack was happy. And on the third day, it was sunny again but Jack was sad. Well, I know that we can't observe the hidden states, but just assume that this scenario happened. Trust me, analyzing this will help us a lot. Okay, the question I wanna ask here is, what is the probability of this scenario occurring? More precisely, what is the joint probability of the observed mood sequence and the weather sequence? Pause here if you'd like to try. By using the Markov property, we can compute this as a product of six terms. First, the probability of a sunny day, then the probability of happy mood given a sunny day, then the transition probability from sunny to cloudy and so on. But how we got these six terms? Please bear with me till the end to understand the underlying mathematics. We can fill the red terms from the emission matrix and the green ones from the transition matrix. But what about the first term? How can we find the probability of the first state? Yes, we need the stationary distribution of the Markov chain for this purpose. You can use the normalized left eigenvectors or repeated matrix multiplication to compute the stationary distribution. I have explained both of them in my previous videos. So, as you can see, the sunny state has the probability of 0.509. Now we can compute the product. And the answer turns out to be 0.00391. Okay, let me hide the states now. So, we only have a sequence of the observed variable. Here I'm gonna ask a more interesting question. What is the most likely weather sequence for this given mood sequence? Hum, there are many possible permutations, right? We can have rainy cloudy sunny, cloudy rainy sunny, rainy cloudy rainy and so on. To find the most likely sequence, we need to compute the probability corresponding to each of them and find the one with the maximum probability. We can calculate the probability just like we did in the last case. I wrote a python script to do the computations and found that this weather sequence maximizes the joint probability. And the probability is 0.04105. If you are still watching this, I believe you would also like to know the formal mathematics behind this. So, get ready for some symbols instead of emojis. I will represent the hidden states of our Markov chain by x and the observed variable by y. We can rewrite our problem like this. This simply means find that particular sequence of x for which the probability of x given y is maximum. Please note that in a hidden Markov model, we observe the sequence of y. That's why I have written x given y. Now, if you notice carefully, you will see there is no direct way to find this probability. Here comes the Bayes theorem. By the way, I have a video explaining Bayes theorem. You can watch that if you want. So, by using Bayes theorem, we can rewrite this as the probability of y given x times the probability of x upon the probability of y. For all practical purposes, we can neglect the denominator. And the numerator is just the joint probability distribution of x and y. Now we are gonna further simplify this. Let's take the first part, the probability of y given x. According to our assumption, yi depends only on xi. So, we can write probability of y given x as a product. We can fill all these terms from our rate matrix. For the second term, probability of x, we must use the Markov property that says xi depends only on xi-1. So, we can convert px into this product. Now, there is one subtlety involved. For x0, we must use the stationary distribution vector as I already showed you earlier. Okay, so now replace these things into the above expression. And we have got the thing that we want to maximize. You can clearly see that this expression is just a product of two n terms. I hope now you fully understand the origin of those 6 terms in the example that I computed earlier. Isn't it super elegant? Well, I guess it is. I know it's not the easiest thing. So, don't worry if you didn't get everything at once. You can always replay the video to remove your confusions. So, that was all for this video guys. Do comment below your thoughts and suggestions and a subscription will be a major help to me. Goodbye guys, stay safe and thanks for watching.",
        "output": "### Introduction to Hidden Markov Models\nHidden Markov Models (HMMs) are concepts derived from Markov chains and are useful in various domains such as bioinformatics, natural language processing, and speech recognition. This video discusses the intuition and mathematics behind HMMs and how Bayes theorem is utilized in the process.\n\n### Understanding Markov Chains and Hidden Markov Models\nTo understand an HMM, one must first be familiar with Markov chains. A simple example involving weather transitions and a person's mood is used to illustrate the process. In an HMM, the states of the Markov chain are hidden or unknown, but certain variables dependent on those states can be observed. In this example, these observed variables are the person's mood. The HMM consists of an ordinary Markov chain and a set of observed variables.\n\n### Transition and Emission Matrices\nThe probabilities of state transitions and observed variables can be captured in two matrices: the transition matrix (green) and the emission matrix (red). The probabilities in the matrices can be used to calculate the joint probability of observed mood sequences and weather sequences.\n\n### Stationary Distribution and Computing Probabilities\nTo find the probability of the first state, the stationary distribution of the Markov chain is needed. This can be computed using the normalized left eigenvectors or repeated matrix multiplication techniques as shown in previous videos. After finding the probability of the first state, the product of the probabilities can be calculated to determine the overall probability of a scenario occurring.\n\n### Most Likely Weather Sequence\nGiven a sequence of observed moods, the most likely weather sequence can be found by calculating the probability of each possible permutation and selecting the one with the maximum probability using the same computation method as before.\n\n### Formal Mathematics: Bayes Theorem and Joint Probability Distribution\nThe formal mathematics behind finding the most likely sequence of hidden states involves using Bayes theorem to rewrite the problem in terms of joint probability distribution. The numerator of the equation becomes a product of two sets of terms representing the probability of the observed variables given the hidden states and the probability of the hidden states themselves. By maximizing this expression, the most likely sequence of hidden states can be found.\n\n### Conclusion and Next Steps\nHidden Markov Models are powerful tools for understanding underlying hidden states based on observed variables. The process can be complicated, but understanding the mathematics and using Bayes theorem helps in developing a clearer comprehension of the concept."
    },
    {
        "instruction": "<|SYSTEM|>You are a scholars that is taking markdown notes about some video, you would be giving some written scripts for the task.\nYou have finished writing the notes, and you want to extract the title from the notes.\nThe title should only be the name of the technology or algorithm, no extra words needed.\nPlease write the notes in markdown format. No need to write \"Title:\" for the Title. Make sure the title is within 10 words.\nHere is an example:\n## Hidden Markov Model\n<|ASSISTANT|>## ",
        "input": "<|USER|>### Introduction to Hidden Markov Models\nHidden Markov Models (HMMs) are concepts derived from Markov chains and are useful in various domains such as bioinformatics, natural language processing, and speech recognition. This video discusses the intuition and mathematics behind HMMs and how Bayes theorem is utilized in the process.\n\n### Understanding Markov Chains and Hidden Markov Models\nTo understand an HMM, one must first be familiar with Markov chains. A simple example involving weather transitions and a person's mood is used to illustrate the process. In an HMM, the states of the Markov chain are hidden or unknown, but certain variables dependent on those states can be observed. In this example, these observed variables are the person's mood. The HMM consists of an ordinary Markov chain and a set of observed variables.\n\n### Transition and Emission Matrices\nThe probabilities of state transitions and observed variables can be captured in two matrices: the transition matrix (green) and the emission matrix (red). The probabilities in the matrices can be used to calculate the joint probability of observed mood sequences and weather sequences.\n\n### Stationary Distribution and Computing Probabilities\nTo find the probability of the first state, the stationary distribution of the Markov chain is needed. This can be computed using the normalized left eigenvectors or repeated matrix multiplication techniques as shown in previous videos. After finding the probability of the first state, the product of the probabilities can be calculated to determine the overall probability of a scenario occurring.\n\n### Most Likely Weather Sequence\nGiven a sequence of observed moods, the most likely weather sequence can be found by calculating the probability of each possible permutation and selecting the one with the maximum probability using the same computation method as before.\n\n### Formal Mathematics: Bayes Theorem and Joint Probability Distribution\nThe formal mathematics behind finding the most likely sequence of hidden states involves using Bayes theorem to rewrite the problem in terms of joint probability distribution. The numerator of the equation becomes a product of two sets of terms representing the probability of the observed variables given the hidden states and the probability of the hidden states themselves. By maximizing this expression, the most likely sequence of hidden states can be found.\n\n### Conclusion and Next Steps\nHidden Markov Models are powerful tools for understanding underlying hidden states based on observed variables. The process can be complicated, but understanding the mathematics and using Bayes theorem helps in developing a clearer comprehension of the concept.",
        "output": "## Hidden Markov Models"
    },
    {
        "instruction": "<|SYSTEM|>You are a scholars that is taking markdown notes about some video with Wiki style, you would be giving some written scripts for the task.\nAnd summarize each paragraph with a title.\nRemember to keep the technology or algorithm name, very important.\nPlease write the notes in markdown format.\nHere is an example:\n### Definiation\nA deadlock is a situation in which two computer programs sharing the same resource are effectively preventing each other from accessing the resource, resulting in both programs ceasing to function. \nThe earliest computer operating systems ran only one program at a time.\n\n### What it have to do with Relationsal Database ManagementSystem\nIn a database, a deadlock is an unwanted situation in which two or more transactions are waiting indefinitely for one another to give up locks. \nDeadlock is said to be one of the most feared complications in DBMS as it brings the whole system to a Halt. \nWhen you tries to insert or update data on database, sometime your record need to depend on the row record.  \n<|ASSISTANT|>#### ",
        "input": "<|USER|> Hello people from the future, welcome to Normalized Nerd. Today I'm gonna talk about Markov Chains. This is a concept that is used in many places from statistics to biology, from economics to physics and of course in machine learning. I guess some of you have heard of Markov Chains before but don't have a clear idea about it. I hope by the end of this video you will have a fair understanding of Markov Chains and you will know why they are so interesting. If you are new here then please subscribe to my channel and hit the bell icon. I make videos about machine learning and data science regularly. So let's get started. Well as you know me I like to start with an example. Assume there's a restaurant that serves only three types of foods- hamburger, pizza and hot dog. But they follow a weird rule. On any given day they serve only one of these three items and it depends on what they had served yesterday. In other words there's a way to predict what they will serve tomorrow if you know what they are serving today. For example there's a 60 chance that tomorrow will be a pizza day given that today is a hamburger day. I'm going to represent it in the diagram as weighted arrows. The arrow originates from the current state and points to the future state. I'm using the term state because that's the convention. Let's see another one. There's a 20 chance that tomorrow again they will serve hamburgers and that is represented as a self-pointing arrow. Well each arrow is called a transition from one state to the other. Now I'm going to show you all the possible transitions. The diagram you are seeing right now is actually a Markov chain. Yes the concept is that simple. Now let's discuss some properties of the Markov chain. The most important property of the Markov chain is that the future state depends only on the current state not the steps before. Mathematically we can say the probability that n plus 1th step will be x depends only on the nth step not the complete sequence of steps that came before n. At first glance this might not seem so impressive but on a closer look we can see that this property makes our life a lot easier while we tackle some complicated real world problems. Let's understand it a little better. Suppose the restaurant served pizza on the first day, hamburger on the second and again pizza on the third day. Now what is the probability that they will serve hot dogs on the fourth day. Well we just need to look at the third day and it turns out to be 0.7. This is the heart of Markov chains and it's known as the Markov property. The second important property is that the sum of the weights of the outgoing arrows from any state is equal to 1. This has to be true because they represent probabilities and for probabilities to make sense they must add up to 1. Here I should probably tell you that there are certain Markov chains with special properties but they deserve individual videos on their own. So here I'm just gonna stick to the basic kind of Markov chains. Now that you know what a Markov chain actually is let's explore our Markov chain by doing a random walk along the chain. Initially we are on a hamburger day. Let's see what they serve for 10 consecutive days. So after 10 steps we have a situation like this. Now we want to find what is the probability corresponding to each of the food items aka the probability distribution of the states. Well it's pretty simple. Just divide the number of occurrences of an item by the total number of days. For hamburger it's 4 upon 10, for pizza it's 2 upon 10 and it's 4 upon 10 for hot dogs. It's obvious that if we change the number of steps then these probabilities will vary but we are interested in what happens in the long run. Do these probabilities converge to fixed values or they continue to change forever? I wrote a python script to simulate the random walk for 100,000 steps. I found that the probabilities converge to these values and this probability distribution has a special name which is the stationary distribution or the equilibrium state. This simply means that this probability distribution doesn't change with time for this Markov chain. Okay so we have managed to find a stationary state but this method doesn't seem to be very efficient is it? Moreover we don't know if there exists any other stationary states. Well a better way to approach this problem is linear algebra. Let me show you. First of all let us think about how we can represent this Markov chain more efficiently. Well this is essentially a directed graph so we can represent it by an adjacency matrix. If you don't know what it is then please don't worry it's very simple. The elements in the matrix just denote the weight of the edge connecting the two corresponding vertices. For example the element in the second row and first column denotes the weight or the transition probability from pizza to hamburger. If an element is zero then it just means that there's no edge between the two vertices. This matrix is also called transition matrix. Let's denote it by A. Remember that our goal is to find the probabilities of each state. Conventionally we take a row vector called pi whose elements represent the probabilities of the states. Basically pi denotes the probability distribution of the states. Suppose in the beginning we are on a pizza day. So the second element which corresponds to the pizza becomes one and other two remain zero. Now see what happens when we multiply this row vector with our transition matrix. We get the second row of the matrix. In other words we get the future probabilities corresponding to the pizza state. Then we take this result and put it in the place of pi zero. We will do this once again. Now if there exists a stationary state then after a certain point the output row vector should be exactly same as the input vector. Let's denote this special row vector as pi. So we should be able to write pi A equals pi. If you have ever taken a course of linear algebra then you should find this equation somewhat similar to a very famous equation. Yes, I'm talking about the eigenvector equation. Consider lambda equal to one and just reverse the order of multiplication and you have got our equilibrium state equation. How to interpret this you might ask? Well, we can imagine that pi is a left eigenvector of matrix A with eigenvalue equal to one. Please pause the video if you need a moment to convince yourself. Now the eigenvector must satisfy another condition. The elements of pi must add up to one because it denotes the probability distribution. So after solving these two equations we get the stationary state as this. It tells us that the restaurant will serve hamburger about 35 percent of the days, pizza about 21 percent and hot dog about 46 percent. Using this technique we can actually see if there are more than one stationary states. Can you guess how? We just need to see if there exists more than one eigenvectors with eigenvalue equal to one. Isn't it super elegant? Now let's compare this result with the previous one that we got from our simulation. Look how close they are. They kind of validate each other. By now you should have a very good understanding of Markov chains and you should be able to identify Markov chains successfully. Obviously there is much more to learn about different kinds of Markov chains and their properties that cannot be covered in a single video. If you want me to make more videos on this topic then please let me know in the comment section. If you like this video do share it and please please please subscribe to my channel. Stay safe and thanks for watching.",
        "output": "### Introduction to Markov Chains\nMarkov Chains are used in various disciplines such as statistics, biology, economics, physics, and machine learning. This concept helps to predict future states based on current states.\n\n### Restaurant Example\nConsider a restaurant that serves three types of foods: hamburger, pizza, and hot dog. They serve only one item per day and follow rules based on the previous day's serving. A Markov Chain can be used to represent this scenario with weighted arrows indicating transition probabilities between different states.\n\n### Markov Chain Properties\n1. **Markov Property**: The future state depends only on the current state, not the steps before.\n2. **Sum of Transition Probabilities**: The sum of the weights of the outgoing arrows from any state must be equal to 1, as they represent probabilities.\n\n### Random Walk\nBy exploring the Markov chain through a random walk, one can find the probability distribution of different states. The stationary distribution or equilibrium state is the probability distribution that doesn't change with time for a given Markov chain.\n\n### Linear Algebra Approach\nThe adjacency matrix (named the transition matrix) can be used to represent Markov chains efficiently. The equilibrium state can be given by the left eigenvector of the transition matrix with eigenvalue equal to 1. This technique also allows checking the existence of multiple stationary states.\n\n### Conclusion\nMarkov chains provide a strong foundation for understanding various real-world problems. There's much more to learn about different types of Markov chains and their properties that cannot be covered in a single video."
    },
    {
        "instruction": "<|SYSTEM|>You are a scholars that is taking markdown notes about some video, you would be giving some written scripts for the task.\nYou have finished writing the notes, and you want to extract the title from the notes.\nThe title should only be the name of the technology or algorithm, no extra words needed.\nPlease write the notes in markdown format. No need to write \"Title:\" for the Title. Make sure the title is within 10 words.\nHere is an example:\n## Hidden Markov Model\n<|ASSISTANT|>## ",
        "input": "<|USER|>### Introduction to Markov Chains\nMarkov Chains are used in various disciplines such as statistics, biology, economics, physics, and machine learning. This concept helps to predict future states based on current states.\n\n### Restaurant Example\nConsider a restaurant that serves three types of foods: hamburger, pizza, and hot dog. They serve only one item per day and follow rules based on the previous day's serving. A Markov Chain can be used to represent this scenario with weighted arrows indicating transition probabilities between different states.\n\n### Markov Chain Properties\n1. **Markov Property**: The future state depends only on the current state, not the steps before.\n2. **Sum of Transition Probabilities**: The sum of the weights of the outgoing arrows from any state must be equal to 1, as they represent probabilities.\n\n### Random Walk\nBy exploring the Markov chain through a random walk, one can find the probability distribution of different states. The stationary distribution or equilibrium state is the probability distribution that doesn't change with time for a given Markov chain.\n\n### Linear Algebra Approach\nThe adjacency matrix (named the transition matrix) can be used to represent Markov chains efficiently. The equilibrium state can be given by the left eigenvector of the transition matrix with eigenvalue equal to 1. This technique also allows checking the existence of multiple stationary states.\n\n### Conclusion\nMarkov chains provide a strong foundation for understanding various real-world problems. There's much more to learn about different types of Markov chains and their properties that cannot be covered in a single video.",
        "output": "## Markov Chains"
    },
    {
        "instruction": "<|SYSTEM|>You are a scholars that is taking markdown notes about some video with Wiki style, you would be giving some written scripts for the task.\nAnd summarize each paragraph with a title.\nRemember to keep the technology or algorithm name, very important.\nPlease write the notes in markdown format.\nHere is an example:\n### Definiation\nA deadlock is a situation in which two computer programs sharing the same resource are effectively preventing each other from accessing the resource, resulting in both programs ceasing to function. \nThe earliest computer operating systems ran only one program at a time.\n\n### What it have to do with Relationsal Database ManagementSystem\nIn a database, a deadlock is an unwanted situation in which two or more transactions are waiting indefinitely for one another to give up locks. \nDeadlock is said to be one of the most feared complications in DBMS as it brings the whole system to a Halt. \nWhen you tries to insert or update data on database, sometime your record need to depend on the row record.  \n<|ASSISTANT|>#### ",
        "input": "<|USER|> Hey there and welcome to this video. In the following I'll be doing a paper explanation about cross attention. If you haven't heard about cross attention, don't be discouraged because it's the technique that makes so many deep learning models perform so well and enable so many impressive results. You definitely have heard about stable diffusion, image gen or muse. They all use cross attention. And for what? Cross attention is a way to condition a model with some extra information. The aforementioned models all use it to condition their models on text. And as of now, cross attention seems to be the best method for injecting text conditioning into a model. And not only that, this also enables so many cool follow up techniques as you might have seen with stable diffusion. And my intention with this video is to give you some intuitive and visual explanation about cross attention. Because I believe while many people have heard about it a lot, especially through the hype of stable diffusion, many don't really understand how it works and just treat it as a black box. And the same was true for me before I started making this video. So let's not lose any time and get into the explanation. So before going into cross attention itself, I want to take some time to talk about the standard attention, also called self attention. You probably have all seen this formula over and over. We have some input and construct three different matrices called Q, K and V. Then we multiply Q by K and divide by a constant and then take a softmax and after that we multiply by V. Voila, attention. But what does it actually do? For that I want to take an example about how to use self attention on an image. This is also done by stable diffusion for example, but note that this doesn't have anything to do with conditioning the model on some extra information. Self attention is a way to globally route information between the entire image, instead of just locally as with convolutions. You will see what that means in a second. For simplicity let's say our image has only 2 by 2 pixels, but let's also assume the image is RGB and thus has 3 channels. To apply self attention now, we first have to flatten our image like this, which gives us a shape of 4 by 3. And remember it's just 4 pixels with 3 channels for RGB. Now we want to get our Q, K and V matrices, which means we need 3 different weights to project our input pixels into the latent space where we will apply attention. In order to make things really simple and easy to visualize, I'm choosing to project the 3 dimensional input into 2 dimensions, but in practice you would go much higher. In order to do that, each weight matrix will have a shape of 3 by 2. Remember that to project some vector from a dimension of n into a dimension of m, you construct a matrix of shape n times m. In our case we want to go from 3 dimensions to 2 dimensions, which is why we construct the 3 by 2 matrix. Easy right? First let's make up some arbitrary values for our image pixels, which we can calculate with. Projecting the input to Q, K and V leaves us with all matrices of the shape of 4 by 2. Now let's take a look at the first step, which is to calculate Q by K. This operation basically multiplies each vector in Q, which stands for 1 pixel by all other vectors of the pixel in K. Since each of these is a dot product, the outcome will be a single number indicating the similarity between the vectors. And by doing this for all pixels we get a similarity matrix of each pixel to every other pixel. So multiplying Q by K can be interpreted as a similarity measurement. We can better see this if we visualise Q and K in a 2 dimensional grid. If the model thinks that 2 pixels should attend to each other or in other words should share some information, it can use the weight matrices for Q and K to project them close together as done with these two vectors for example. By applying the dot product between the two we get a high number for the similarity, whereas for example these two vectors are further away from each other and thus the similarity will be small and they will not attend to each other as much. Now in the formula you see that we divide by this factor, but for demonstration purposes I will omit it since it only scales the similarity matrix. This is mainly done for having a more stable training. Next we apply the softmax function. In our case we apply it over the rows, because we want to normalise the values for how much one individual pixel it tends to all others. You can interpret these values now as to how much a pixel will attend to some other pixel. For example this pixel here will attend 23% to itself, 33% to the second pixel, 27% to the third and another 17% to the fourth pixel. And here you can already see how attention enables us to route information between all pixels in a single layer. And the last step is now to multiply our similarity matrix by the V matrix. And here you will see that this basically just acts as a weighted average. You can see that best if instead of doing the normal matrix multiplication we take each row in the similarity matrix and multiply each value by the corresponding row embedding for each pixel in V. Let's look at the first row of a similarity matrix. We take the first value, which stands for how much the first pixel should attend to the first pixel, in that case 0.23 or 23%, and multiply it by the first row of the V matrix. Then we take the second value, 0.33 or 33%, which stands for how much the first pixel should attend to the second pixel, and multiply it by the second row. And we do the same for the third and fourth value. And at the end we sum up all vectors. And you can see it's the same outcome as with the normal matrix multiplication. And that's how we do it for every row of the similarity matrix. We end up having a 4 by 2 matrix, where each row is the weighted average of the embeddings of all pixels. And I really want to emphasize the last point. We started off with each pixel having its own embedding solely influenced by itself. And we end up with a mix of all pixel embeddings, and this mix and its proportions can be determined by the model itself. It's like you being a cook and you can determine how you want to mix ingredients together to get the best possible outcome in the form of a meal. And that was the core idea of how attention works, in the form of self-attention. Pretty easy and intuitive, right? The last thing that we still have to do is to project a 4 by 2 matrix back into the original space which had 3 dimensions for RGB. And we do that with a simple linear layer of shape 2 by 3. And now at the end we add the attention output to the input. The reason we do the skip connection with the input is not only because the gradients can flow better, but it's also because of the fact that the attention can fully concentrate to attend to whatever it wants. Without the skip connection each pixel would also need to pay a lot of attention to itself in order not to lose information about itself. With the skip connection, the original input stays fully preserved and can be modified by the attention output in whatever way the model thinks it's useful. I hope it makes sense, if not just ask in the comments and we can think of a nice example to explain it. Okay, now with that knowledge in mind, understanding cross-attention will be super easy. In cross-attention we do one significant change to Q, K and V. Instead of all matrices being projections of the input features, in our case the images, only Q will be a projection of the input features and K and V will be projections of the conditional information. In the example that we'll be doing, we'll assume the condition will be text, but basically anything else would work too. Let's first talk about how text is represented usually. Right now most text to image models employ some large pre-transformers and usually they only use the encoder. The first step is to take your caption and tokenize it. Then each token will be embedded and fed through the transformer layers of the encoder and eventually we'll get an output with a shape that sort of looks like this. The first number is just the batch size, the second number is the number of tokens and the third is the encoding dimension after all transformer layers. In our simple example we'll ignore the batch size and take as an example Ilove mountains, which I do. And let's say that we tokenize it and apply the transformer and get as output the matrix of shape 3x6. Three tokens, Ilove mountains, which all have an embedding of 6. And now let's construct all weight matrices. We again will project into a two dimensional space and we said that this time only Q comes from the image and K and V from the conditioning. For our 4x4 image we'll take the same projection of 3x2 and for K and V we'll create projections of 6x2 to project the six dimensional embeddings down to 2. So now we have Q which has a shape of 4x2 and K and V which have a shape of 3x2. Four pixels and three words. And now we do the exact same thing as before. We first multiply Q by K and to look at it in more detail every embedding of each pixel will be multiplied by each embedding of each word. So embeddings which are similar will have a high value and therefore a given pixel will attend more to a specific word. In our example the pixels containing the mountains might pay a lot of attention to the word mountain. After creating the similarity matrix now we again normalize the values with the softmax. The final step is to multiply the similarity matrix by the value matrix. And we can apply the same trick as before here to understand better what's going on. Let's focus on the first row for the first pixel. This pixel wants to attend 47% to the first word, 22% to the second word and 31% to the last word. And that's why we multiply the first row of the value matrix which remember comes from the text conditioning by 0.47. Then add the second row times 0.22 and then add the third row times 0.31 to it. This will make up the weighted embedding vector of all words for the first pixel. And now we do the same for the rest of the pixels and all the words. And by the end each pixel will have acquired information of the entire text conditioning and decided for itself what it wants to pay attention to the most. Beautiful, right? And now we have the final projection layer which goes back to the original space of 3 dimensions for RGB. Since we added a 2 dimensional space we will again construct this projection layer to have a weight shape of 2 by 3. Inputting the attention output of shape 4 by 2 transforms it into 4 by 3 and we can happily add it back to our input with the skip connection. And there you have it. That is cross attention. And the cool thing is that you can use cross attention for so many other things as also shown by stable diffusion. You just need to find a good way to bring your conditional data into a not too big dimensional space such that the computations don't explode and you're good to go. To conclude this video let's do a quick summarization. In self-attention we create 3 different projections from our inputs. Q, K and V. The goal of attention is to enable our model to route information together in a global manner over what it thinks is useful. We first multiply Q by K which serves as a similarity lookup and gives us a measurement how much each element wants to attend to every other element. After normalizing the output we then multiply the similarity matrix by V to route the essential information together. We apply a final linear layer to project back into the original space and as the skip",
        "output": "### Introduction to Cross Attention\nCross attention is a technique that significantly improves the performance of many deep learning models by conditioning them on extra information, such as text. Models like Stable Diff, Image Gen, and Muse all utilize cross attention to achieve impressive results.\n\n### Self-Attention Basics\nSelf-attention, also called standard attention, involves creating three matrices (Q, K, and V) from the input and performing several operations on them. These operations include calculating the dot product between Q and K to measure similarity, normalizing the resulting values with the softmax function, and multiplying the normalized values by the V matrix.\n\n### Self-Attention on Images\nWhen applying self-attention to images, we first flatten the image and then create our Q, K, and V matrices by projecting the flattened image into the latent space. We then perform the same operations as described earlier, ultimately enabling the model to route information between all pixels in a single layer.\n\n### Cross Attention Concept\nIn cross attention, a major change occurs in the preparation of Q, K, and V matrices: Q is still a projection of the input features, but K and V are projections of the conditional information (e.g., the text). We perform the same operations with these new matrices, and the result is a model that can acquire information from the entire text conditioning and decide which parts to pay attention to the most.\n\n### Cross Attention Application\nCross attention can be applied to many tasks by bringing the conditional data into a manageable dimensional space. Overall, cross attention enables global information routing and allows models to learn from additional conditioning, resulting in better performance and flexibility."
    },
    {
        "instruction": "<|SYSTEM|>You are a scholars that is taking markdown notes about some video, you would be giving some written scripts for the task.\nYou have finished writing the notes, and you want to extract the title from the notes.\nThe title should only be the name of the technology or algorithm, no extra words needed.\nPlease write the notes in markdown format. No need to write \"Title:\" for the Title. Make sure the title is within 10 words.\nHere is an example:\n## Hidden Markov Model\n<|ASSISTANT|>## ",
        "input": "<|USER|>### Introduction to Cross Attention\nCross attention is a technique that significantly improves the performance of many deep learning models by conditioning them on extra information, such as text. Models like Stable Diff, Image Gen, and Muse all utilize cross attention to achieve impressive results.\n\n### Self-Attention Basics\nSelf-attention, also called standard attention, involves creating three matrices (Q, K, and V) from the input and performing several operations on them. These operations include calculating the dot product between Q and K to measure similarity, normalizing the resulting values with the softmax function, and multiplying the normalized values by the V matrix.\n\n### Self-Attention on Images\nWhen applying self-attention to images, we first flatten the image and then create our Q, K, and V matrices by projecting the flattened image into the latent space. We then perform the same operations as described earlier, ultimately enabling the model to route information between all pixels in a single layer.\n\n### Cross Attention Concept\nIn cross attention, a major change occurs in the preparation of Q, K, and V matrices: Q is still a projection of the input features, but K and V are projections of the conditional information (e.g., the text). We perform the same operations with these new matrices, and the result is a model that can acquire information from the entire text conditioning and decide which parts to pay attention to the most.\n\n### Cross Attention Application\nCross attention can be applied to many tasks by bringing the conditional data into a manageable dimensional space. Overall, cross attention enables global information routing and allows models to learn from additional conditioning, resulting in better performance and flexibility.",
        "output": "## Cross Attention"
    },
    {
        "instruction": "<|SYSTEM|>You are a scholars that is taking markdown notes about some video with Wiki style, you would be giving some written scripts for the task.\nAnd summarize each paragraph with a title.\nRemember to keep the technology or algorithm name, very important.\nPlease write the notes in markdown format.\nHere is an example:\n### Definiation\nA deadlock is a situation in which two computer programs sharing the same resource are effectively preventing each other from accessing the resource, resulting in both programs ceasing to function. \nThe earliest computer operating systems ran only one program at a time.\n\n### What it have to do with Relationsal Database ManagementSystem\nIn a database, a deadlock is an unwanted situation in which two or more transactions are waiting indefinitely for one another to give up locks. \nDeadlock is said to be one of the most feared complications in DBMS as it brings the whole system to a Halt. \nWhen you tries to insert or update data on database, sometime your record need to depend on the row record.  \n<|ASSISTANT|>#### ",
        "input": "<|USER|> Hey there and welcome to this video. In the following I'll be doing a paper implementation of diffusion models. Diffusion models are extremely popular right now and are being applied to almost every single task which involves some kind of generation. The most popular tasks they are being applied to at the moment is image generation. And you have probably heard a lot from models like Dali2, stable diffusion or imagegen. Where all of them differ in their exact approach, they all have the same fundamental underlying technique. And we are going to implement this technique in this video. Before we are getting started though, in order to fully understand the video and the implementation, you should have a basic understanding of the theory behind diffusion models. If you are not familiar with it, you can take a look at my explanation video which I made recently or take a look at all the other incredible resources out there. Okay, so what is this video going to be exactly about? At first we are going to do a basic implementation of diffusion models in PyTorch. This will consist of coding all the tools for the diffusion process, so noising images, sampling images and so on. Of course we will also cover implementing the famous unit architecture as well as the training loop. Then we will train this model on some data set and look at the results. After that you have a great starting point to apply diffusion models to your own data sets. Furthermore, over time there were so many improvements to diffusion models making them faster or better. This is why I want to talk about a few and also implement them. The two main things I want to talk about are classifier free guidance and exponential moving average. Don't worry if you don't know what it is, I will explain that later. However these improvements will require us to rewrite some of the existing code to work conditionally since classifier free guidance only works with labels and our first implementation will be unconditionally. And of course we are also going to train this and look at the results afterwards. After that you should have a pretty neat understanding of how to implement diffusion models and maybe you can extend them even further with your own ideas. So before getting into the coding, if you need a short refresher about diffusion models, let past me give you a recap. Diffusion models are generative models which became super popular in recent times for beating GANs on image synthesis. The main idea behind diffusion models is that we have two processes. The first one is easy. We just gradually apply noise to an image over a lot of steps until the image has turned into complete noise. The cool thing is that we can also do this in one single step, which saves us a lot of compute and time. Now the goal of the reverse process is that we learn how to remove noise from an image. But as we saw, we don't instantly try to remove all of the noise, we do it step by step. That's just easier for the model to learn. For that we have a neural network which takes in an image and the current time step t and predicts the image at t-1. Well, to be more specific, we saw that we can reformulate it so that we only predict the noise in the image. Then we can subtract the predicted noise from the image at time t and get the image at t-1. We will do this over and over again until we arrive at t equals 0. And if the neural network was trained well, we now have an image which could occur in our training data. Alright, after this short reminder, let's dive into the code. Ok, so you can see I've created a new project and got what will be our main file up here. At first I will just do the casual imports for all the things that we need. The first thing that we will code, and this will be the most important thing out of all, are the diffusion tools. And as I said earlier, this will cover setting up the noising schedule, a function for noising images, and also for sampling images. And I'm just going to put all of this in one single class called diffusion. The main parameters will be for the number of sampling steps, the lower and upper end for beta, and the image size. And the default parameters you see here will also be the ones that we are going to use in all training runs. I'm setting the number of time steps to 1000 as proposed in the first papers. Then I also use the same values for the betas and the image size will be 64 by 64. I'm using a fairly small resolution here because I'm only training this on my RTX 3090 and higher resolutions would take much longer. And also going with the flow of recent state of the art models, you probably would rather train separate up samplers to get to higher resolutions instead of just training one big base model. But that just as a side note. In this video we will also just be using the linear beta schedule as proposed in the first papers. If you want to use the cosine schedule proposed by OpenAI, it should be very simple to change though. Okay so these will be our betas. And remember the alphas that were introduced, which are basically just for making the formulas look thinner and nice in our algorithms. We'll define these here. The alphas will simply be one minus beta. And then we also need the cumulative products of the alphas. You'll see in a second how exactly we are going to use them, so don't worry if you don't remember that. Okay so the first really important function that we'll need is to noise images. And if you remember from the explanation video, there were two options to do that. The first would be to iteratively add noise over and over again until we are at our desired But there was also this great insight that we can just do it in one single step. And of course that's exactly what we will be doing, because it's much faster. You can see the formula on the screen. And basically all we do is to take the square root of our alphas hat and then also the square root of one minus alpha hat. At last we will also need some random noise and plug all of it into the formula and voila this function is ready. Another helpful function that we need is one which can sample some time steps, because we need that in algorithm one for the training. And we can just do it in a single one-liner like this. Okay so now let's take care of the sampling. Our sampling function will take in the model which we will use for sampling and the number of images we want to sample. At first we are going to set it to evaluation mode and wrap the following context in a torch.nograd. Generally everything that we are doing here will follow algorithm two from the DDPM paper I also talked about in the explanation video. You see the first thing that I'm doing here is to create our initial images by sampling from a normal distribution using torch.randn. And now here comes the big loop going over all 1000 time steps in a reversed order, starting with the highest and going until one. The first thing in the loop is creating the time steps by just creating a tensor of length n with the current time step. And after that I will feed that into the model together with the current images. The last thing we will need is noise which we add and scale with the variance later. But remember we only need noise for the time steps greater than one, because in the last iteration we don't want to add it because it would just make our final outcome worse. In that case I will just set the noise to be zero. And now for the final operation in each iteration we will alter our images and remove a little bit of noise by making use of the formula that you see in the algorithm. And that's already all there is in the main sample loop. After that we will set the model back to training and clip the output to be in the valid range of minus one and one. The plus one and the division by two is just for bringing the values back to the range of zero and one, which we will further multiply by 255 to bring them into the valid pixel range and then change the data type for saving them later. And that's it. Easy right? Until now we've treated the model in the sampling loop as a black box, which just takes in the noised images and time steps. So let's start coding the unit now. I switched over to the modules.py file and defined the basic construct for the unit. It will take in the input and output channels, which will be both three since we are working with RGB images. But you can also feel free to maybe try it with just black and white images. The next argument is the dimension of the time step embedding, which you will see in a minute. As you probably know, a unit has an encoder, a bottleneck and a decoder. Since our model will be very easy and straightforward, I won't be making separate modules for all the three subcomponents and just put all the actual modules here. But you'll see which parts belong to the encoder, decoder and bottleneck. Also for now I'll just put all the names of the modules here to give you a brief overview what the unit will consist of. And afterwards we'll code each of the modules themselves. At first we'll start with a doubleconf, which as the name suggests is just a wrapper for two convolutional layers. Again you'll see the details soon. After that we'll put three downsample blocks, each followed by a self-attention block. The arguments to the downsample block are just the input and output channels and for the self-attention block the first argument is the channel dimension and the second one is the current image resolution. Each downsample block will reduce the size by two, so going from 64 to 32 to 16 to 8. After that we have the bottleneck, which just consists of a bunch of convolutional layers and after that we go straight into the decoder part, which is basically the reverse of the encoder and consists of three upsampling blocks. And each followed by a self-attention block. And at last we are projecting back to the output channel dimension by using a normal conf layer. The forward pass is pretty straightforward too. We'll take as input the noised images and the timesteps. As mentioned earlier, the timesteps will only be a tensor with the integer timestep values in it. But here comes the interesting thing now. Instead of giving these to our model in their plain form, we'll make it easier for the model and encode them. Most diffusion papers just use the sinusoidal embedding and we are also going to go with that. I'll put some resources in the description if you want to know more about them, but due to time reasons we'll just take them as given. The only thing to note is that we provide the explicit dimension which we want to encode them into. So, to give you an example, this is what it looks like if we have the following time tensor, and encode it using the sinusoidal embedding. Keep that in mind because we'll use that knowledge in some of the modules. Ok cool, and the rest of the forward function is literally the same that we talked about in the init method. So nothing new here, except that you can see that the upsampling blocks take in the skip connections from the encoder too. We'll see in a minute how they will be combined. But yeah, that's already all there is for the abstract layout for the unit. Let's now take a look how all the modules work in detail. The first thing that we'll look at is the doubleconf module. It is really just a normal convolution block as you have it in many other architectures and consists of a 2D convolution followed by a group norm and a gelo activation, and then another convolution and a group norm. Furthermore, there's also the possibility to add a residual connection by providing the residual keyword with true. Ok, next up is the downsample block. Its main component consists of a max pool to reduce the size by half, followed by two doubleconfs. It's as easy as that. But there's one more really important thing and that is the embedding layer you see down here. We see that we encode our time steps to a certain dimension. However, since most blocks differ in terms of their hidden dimension from the time step embedding, we'll make use of a linear projection to bring the time embedding to the proper dimension. And you see this just consists of a zelo activation followed by a linear layer going from the time embedding to the hidden dimension. In the forward pass you see that we first feed the images through the convolutional block and project the time embedding accordingly, and then just add both together and return. Ok, now let's move to the upsample block. And you can see that it's basically exactly the same for the init method, except that we have an upsample operation instead of a max pooling as in a downsampling block. And the main difference in the forward pass is just that we also take in the skip connection which comes from the encoder, and after upsampling the normal legs we concatenate it with the skip connection and feed it through the convolutional block. And then at the end we also add the time embedding to it again. Last but not least we have the self attention layer. And this is really just a completely normal attention block as you probably know it from transformers. This can be seen good in the forward pass here. We first have a pre-layer norm, which is being followed by a multi-head attention. Then we add the skip connection and pass it through the feed forward layer which also consists of a layer norm and two linear layers which are separated by a zelo activation. The first and last operations which you can see here are just for bringing the images into the right shape by first flattening them and then bringing the channel axis as the last dimension such that the attention can work properly. And voila, that's it for the model part. Once again you see that it's quite easy and not as hard as you might have expected it from the fusion models right? Okay, we are almost done already. We only need to write the training loop now. But before that I will write a few helper functions that will be useful in the training loop. The first method is just for plotting images and the second is just for saving them. The next function is for preparing the data. I just have some basic transforms here which will resize and crop the images and then convert them to a tensor and normalize them to be between minus one and one. After that I define a data set where I just use the build in image folder and then return the data loader. The last function you see here is just for setting up the folders for saving the model and the result. Awesome, let's go to the training loop. At first I just define all the necessary things such as the data loader, the model, the opt imizer, the laws, our diffusion class which we have written before and the logger to logger training stats. Now we can start the actual training loop. We will iterate over all our epochs and then during each epoch run over the entire data set. Everything that will come now follows exactly algorithm one from the DDPM paper. Our data loader will provide us with the images, then we sample random time steps in the range between one and one thousand and after that we will noise the images accordingly based on the time steps using the noise images function from the diffusion class. This will also return the noise which was used since we need that in the loss calculation. Then all we do is just feeding our noise images to our model which predicts the noise in the images and then we take the mean squared error between the actual noise and the predicted noise. And then it's just the usual optimizer stuff where we first call zero grad, then backward the loss and then take an optimization step. And that's already all there is for the main training loop. It's extremely simple right? I will just add two more lines for logging and after each epoch we will sample some images and log them too and save the model. At this point I just want to re-emphasize that diffusion models are generally pretty easy to implement. Especially if you think back to the explanation video and how math heavy that was. Another thing I want to mention is that when you look at other diffusion model implementations, especially the really popular ones, you will see that their code is very different and much harder to understand regarding the sampling process. That's why they are still using the lower bound formulation which I personally don't understand since the first DDPM papers clearly show that this is not necessary anymore. My only guess is that it makes sense for a few further modifications to diffusion models such as learned variance where we do need the lower bound again. But yeah, maybe if anybody of you knows why that is done, feel free to share it in the comments. However I think for understanding diffusion models this implementation is much better you can really focus on what was shown in the paper and don't get confused by 1000 lines of additional code which you didn't expect and have no idea where it is coming from. So yeah, I guess it's time for training this model now and see where we get. I'll be using a small landscape dataset for this unconditional training. It has about 4000 images in it. I'll train it for 500 epochs using a batch size of 12 and an image size of 64x64. I'll train on a GPU and use a constant learning rate. And I guess with that being said, let's start the training. So this has been training now for 500 epochs and you can see some results on screen here. I personally like them a lot and I guess we can definitely say that it is able to generate landscapes. Here are also some more results from different epochs and it's very interesting to see how these evolve over time until they become realistic landscapes. Awesome. So, now as promised earlier, we'll also look at two improvements for diffusion models which are classifier-free guidance and exponential moving average. Classifier-free guidance or CFG can be used to improve generations if we train with classes. It can help to avoid posterior collapse which simply means that the model ignores the conditional information and just generates any image. An example would be if we wanted to generate an airplane but the model would just generate an arbitrary class such as a car. And CFG can help to avoid that and often also results in better sampling outcomes. But you already heard that we need classes for that. And that's why we will need to adapt our Krone code a little bit. We'll be using Cypher 10 for that which has 10 classes. Originally this dataset came out in 32x32 resolution but I'm using an upscaled version at 64x64. Let's first talk about the model changes. There are plenty ways to condition a model but one of the easiest to implement and one that works reasonably well is to just add conditional information somewhere inside of the model to some intermediate result. In diffusion models we have an easy time since we already have the timestep as a condition. So we could just add the conditional information of the label to that. And again here, instead of adding them in their plane form being a number in the range of 0 to 9 for the 10 different classes, we'll also use an embedding for them. But this time the model can learn it itself. For that we extend our model by a num classes argument and create the label embedding for all classes. And note that this will have the same number of dimensions as the time embedding. And in the forward pass, literally the only thing that we're changing is that if labels are provided, we'll add their embedding to the timestep embedding. And that's already all the changes for the model. Easy right? Going over to our training code, we'll now expect labels to also come from our data loader and during training we'll also pass them to the model. Now our code works for our class conditions. Cool. Okay, so how does classifier free guidance work? During training for like 10% of the time, we'll train unconditionally. That way the model learns to do both, conditional and unconditional sampling. And during sampling we'll sample also both ways but linearly interpolate away from the unconditional sample towards the conditional one. And we'll do that in every iteration. I'll put the paper for CFG in the description so you can read it more carefully if you're interested. So the only thing we change in the training loop is that we set the labels to none for 10% of the time, which will result in that the model only uses the timestep and no class information. Okay, and then we also need to modify the sampling loop. You can see the exact formula for CFG here on screen proposed in the paper. It's basically a linear interpolation between the conditional and unconditional predicted noise. This is why we can use torch Lerp function which does exactly that. So if the CFG scale is bigger than zero, meaning that we want to use CFG, we'll first sample unconditionally and then just do the interpolation. And that's all we need for enabling CFG. We'll do some comparisons later to see if it actually works better than training without it. The second improvement that we will implement is exponential moving average or EMA. EMA is basically a way of enforcing a smoother training. It literally smooths the trajectory of the model updates. So if the training is really noisy and the direction of the optimization changes a lot, EMA can smooth this trajectory and lead to a more robust outcome since it's not so susceptible to outliers in terms of model updates as the main model. EMA works by making a copy of the initial model weights and then updating these based on the moving average from the main model. The formula for updating a single weight using EMA looks like this. It's also an interpolation between the old weight and the new parameter weighted by beta. Beta is usually around 0.99 which shows that the new weight only affects the EMA parameters a little bit which prevents outliers to have a big effect. Let's move to the code and implement it. I'm just going to create a new class for that which would take beta as an argument. We'll let the EMA updates start after a certain number of iterations to give the main model a quick warmup. During the warmup we'll always just reset the EMA model parameters to the main ones. After the warmup we'll then always update the weights by iterating over all parameters and apply the formula I just showed you before. Cool, that's it for the EMA class. Let's put it into action in the training loop. We'll first define it here and create a copy of a model. And then the only thing we'll add here is to call the step EMA function after every model update. And that's already it. We'll also compare some sample from the EMA model versus the original model after the training to see its effects. Okay then let's start the training on Cypher 10 with the new updated class conditional model which uses EMA and CFG. I trained this now for 300 epochs and you can see the results here on screen for the different classes. These are using the normal model and without using CFG. If we use the EMA model you can already see a significant difference from just looking at it. On the other hand if we only use CFG then these are the results here. And in the bottom right corner we have images sampled with both EMA and CFG. I guess that we can definitely say that EMA and CFG help and result in better generations and there's not even a big trade off that we need to encounter for getting better samples. But that's just looking visually at it. To show that it's actually better than the normal model you could do some analytic calculations like FID or other stuff. But for time reasons I won't be doing that here now. Alright so this sums up the training of the conditional diffusion model with classifier free guidance and EMA. You see that it's really not that hard and all of this can be implemented in a couple lines of code. And yeah that's it for this video where we implemented diffusion models from scratch in PyTorch. I really hope you enjoyed the video and feel free to suggest other topics I should cover or things that can be improved. And also thank you for a thousand subscribers. And yeah with that being said I wish you a nice day.",
        "output": "### Basic Diffusion Model Implementation in PyTorch\n- Implement diffusion models in PyTorch\n- Cover key components: noising images, sampling images, unit architecture, and training loop\n- Train on a small landscape dataset, size 64x64\n- Results indicate the ability to generate landscapes\n\n### Classifier-Free Guidance and Exponential Moving Average\n- Improve generation results when training with classes\n- Update models to handle class conditions\n- Implement exponential moving average (EMA) for smoother training\n- Train class conditional model with EMA and Classifier-Free Guidance (CFG) on Cypher 10\n- Sample and analyze the performance of the trained models\n- Observations show EMA and CFG improve image generation\n\nOverall, diffusion models can be implemented effectively in PyTorch, and when combined with Classifier-Free Guidance and Exponential Moving Average techniques, they lead to improved image generation results."
    },
    {
        "instruction": "<|SYSTEM|>You are a scholars that is taking markdown notes about some video, you would be giving some written scripts for the task.\nYou have finished writing the notes, and you want to extract the title from the notes.\nThe title should only be the name of the technology or algorithm, no extra words needed.\nPlease write the notes in markdown format. No need to write \"Title:\" for the Title. Make sure the title is within 10 words.\nHere is an example:\n## Hidden Markov Model\n<|ASSISTANT|>## ",
        "input": "<|USER|>### Basic Diffusion Model Implementation in PyTorch\n- Implement diffusion models in PyTorch\n- Cover key components: noising images, sampling images, unit architecture, and training loop\n- Train on a small landscape dataset, size 64x64\n- Results indicate the ability to generate landscapes\n\n### Classifier-Free Guidance and Exponential Moving Average\n- Improve generation results when training with classes\n- Update models to handle class conditions\n- Implement exponential moving average (EMA) for smoother training\n- Train class conditional model with EMA and Classifier-Free Guidance (CFG) on Cypher 10\n- Sample and analyze the performance of the trained models\n- Observations show EMA and CFG improve image generation\n\nOverall, diffusion models can be implemented effectively in PyTorch, and when combined with Classifier-Free Guidance and Exponential Moving Average techniques, they lead to improved image generation results.",
        "output": "Basic Diffusion Model"
    },
    {
        "instruction": "<|SYSTEM|>You are a scholars that is taking markdown notes about some video with Wiki style, you would be giving some written scripts for the task.\nAnd summarize each paragraph with a title.\nRemember to keep the technology or algorithm name, very important.\nPlease write the notes in markdown format.\nHere is an example:\n### Definiation\nA deadlock is a situation in which two computer programs sharing the same resource are effectively preventing each other from accessing the resource, resulting in both programs ceasing to function. \nThe earliest computer operating systems ran only one program at a time.\n\n### What it have to do with Relationsal Database ManagementSystem\nIn a database, a deadlock is an unwanted situation in which two or more transactions are waiting indefinitely for one another to give up locks. \nDeadlock is said to be one of the most feared complications in DBMS as it brings the whole system to a Halt. \nWhen you tries to insert or update data on database, sometime your record need to depend on the row record.  \n<|ASSISTANT|>#### ",
        "input": "<|USER|> Hey there and welcome to this video. In the following I'll be doing a paper explanation of diffusion models. This type of model has recently become super popular for image generation and achieves competitive results to usual state of the art GANs. Let's see what it is able to do. Especially in the generative art field, diffusion models have enabled really amazing results for text to image. Below the image you always see the caption with which the model was prompted. I mean just look at these. Ahead of Brock and Lee complaining about the weather. It is just so accurate. Or look at these. With Dali 2, which is also a diffusion model, you can also generate variations of images. The top left is me doing a handstand and the rest of the images are generated by Dali based on my image. Or you can also do inpainting. I generated the following image and then asked to remove the tree and this is the result. Just amazing. With other models like disco diffusion you can even create little animations given a text prompt. So you see that these models are capable of creating incredible results and it's so much fun to play around with them. So without further ado, let's try to understand them. Since diffusion models are still pretty new in the image synthesis regime and the number of relevant papers is not too big, I want to cover the most fundamental papers about DDPMs, which led to the rapid improvements for this technique in this video. Specifically I want to talk about the following four papers. First of all the original paper from 2015, which introduced this technique to the field of machine learning coming originally from statistical physics. Then five years later the second influential paper was published and introduced a few groundbreaking changes which led to a huge jump in quality. After that two researchers from OpenAI took on this challenge and introduced even more improvements in two consecutive papers in 2021 which resulted in better performance and faster runtime. The video will be structured as follows. I will be first talking about the general idea of diffusion models, how they work and why they work. I will try to do this on a really intuitive level without too much math involved. After that we'll be diving into the evil looking math formulas and along with the pre-built intuition I will try to explain the math behind DDPMs. Moreover I will also be mentioning the different improvements which came from each of the four papers. Oh and one side note, please forgive me if I won't be going into every single formula and detail of each paper since this would go beyond the scope of this video and would explode the video length. And also feel free to use the timestamps in the description to go through the parts which you are most interested in. So let's try to understand the idea. The 2015 paper described the diffusion model as the following. The essential idea is to systematically and slowly destroy structure in a data distribution through an iterative forward diffusion process. We then learn a reverse diffusion process that restores structure and data yielding a highly flexible and tractable generative model of the data. In short we apply a lot of noise to an image and then have a neural network for removing this noise. The idea now is, when the network for removing noise is learned properly we can start off with completely random noise and let the model remove noise until we have a new image which could occur on our training data. To better understand this let's take the previous statement apart. The authors talk about the forward and the reverse diffusion process. The forward diffusion process is applying noise to an image in an iterative fashion. So we start off with the original image and step by step add more and more noise to the image. If we repeat this for a sufficient number of steps the image will become pure noise. The original paper from 2015 decided to sample the noise from a normal distribution and this was not changed by any of the follow up papers. The authors showed that the image will converge to noise which follows the noise distribution. In our case the image will turn into one which could come from a normal distribution. So we see that going from an image to noise is fairly simple. But what about learning the reverse and going from just noise to an image? This is what the authors call the reverse diffusion process and it involves a neural network learning to remove noise from an image step by step. That way we can give our model an image which just consists of noise sampled from our normal distribution and let the model gradually remove the noise until we have a clear looking image. And if you are wondering why we are doing this stepwise and not going from noise to clear image instantly the 2015 authors claimed that this approach would not be tractable and lead to much worse outcomes. So how does this neural network look like? What will it take in and predict eventually? The DDPM paper from 2020 laid out three things the network could predict. First predicting the mean of the noise at each time step. Second predicting the original image directly. And third predicting the noise in the image directly which could then just be subtracted from the noise image to get a little bit less noise image. We already said that predicting the original image won't work well so we can ignore this choice and as you will see later the first and the third option are the same just parameterized differently and all authors decided to go with the third option and predict the noise directly. You may be wondering why do we just predict the mean and not the variance in the first option? What kind of distribution needs mean and variance? Well the authors of the 2020 paper decided to fix the variance and thus there is no need to predict it since it's always readily available. I'll show what they fixed it to in the math part. And later the choice of fixing the variance was rethought by the OpenAI authors in their first paper and they eventually decided to learn the variance too because it would lead to improvement in the log likelihood but more on that later. Also one more thing to note is that we don't employ the same amount of noise at each time step in the forward process. This is being regulated by a schedule which scales the mean and the variance. This ensures that the variance doesn't explode as we add more and more noise. The paper from 2020 employed a linear schedule. Again more on that later but to give you some intuition if we applied a schedule to an image the transformation would look like the following. Looking at that the authors from OpenAI found this approach to be suboptimal especially at the end of the noising process because you see that the last couple of time steps already seem like complete noise and might be redundant. Also they found that the information would be destroyed too fast. As a result they created their own called the cosine schedule. In the following image you can see the difference between both schedules. The cosine schedule destroys the information more slowly and solves both problems of the linear schedule which is too rapid in destroying the information and too uninformative at the end. So we have talked about what the model will take in and predict. Let's take a look at the architecture of it. The authors from the 2020 paper went with a U-net like architecture. This kind of architecture has a bottleneck in the middle. It takes as input an image and using downsample and resonant blocks it projects the image to a small resolution and after the bottleneck it projects it back up to the original size this time using upsample blocks instead. And also at certain resolutions the authors put in attention blocks and employed skip connections between layers of the same spatial resolutions. The model will always be designed for each time step and the way of telling the model at which time step we are is done using the sinusoidal embeddings from the transformer paper. This embedding is projected into each residual block. This is important because the forward diffusion process is using a schedule as I mentioned earlier which scales the mean and the variance. As a result different amount of noise is applied at different time steps and with this information the model can also take care of removing different amount of noise at different time steps which greatly benefits the outcome. I will be talking more about the schedules later in the maths part. In the second paper from OpenAI they heavily improved the overall outcome by improving the architecture. They made the following updates. First of all they increased the depth of the network and decreased the width. Then they included more attention blocks than the original proposal and also increased the number of attention heads. They also took the residual blocks from BigGAN and used this for the upsampling and downsampling blocks. Next they proposed what they call adaptive group normalization. This is just a fancy name for the idea of incorporating the time step slightly different and additionally also the class of the label. They first apply a group norm after the first convolution in each resonant block and multiply this by a linear projection of the time step and add a linear projection of the class label onto it. The last improvement is classifier guidance which uses a separate classifier to help the diffusion model to generate a certain class. However this is an entire topic on its own and I plan on talking more about this in another video so I will not be going into it much in this one. So now you should have a basic understanding of how diffusion models work. To briefly summarize, we have two processes. One for going from a normal image to complete noise which follows a normal distribution and then we have a reverse process going from complete noise to a real looking image using a neural network. Both processes are done iteratively and the architecture of the model follows a unit. Great, now it is time for jumping into the math part to deeply ground this rough intuition. Okay so let's approach this really easy and start with some basic notation so we are all on the same level. We will define our image as x with a number as a subscript. This number will determine at which time step we are. So x0 will refer to the original image and as time passes and we add more and more noise this number will increase. For example x subscript 42 will be our image after applying 42 iterations of noise. The final image which follows an isotropic Gaussian will be called xt. xt also varied throughout the papers. Initially it was set to 1000, however the follower papers were able to decrease this number to just a fraction. Next up we will define two functions. Q of xt given xt minus 1 corresponds to the forward process and takes in an image and returns an image with a little bit more noise added. So xt minus 1 goes in and out comes xt which has more noise. We will be using this notation often and in order to not get confused you can remember that t minus 1 always corresponds to a less-noised image and on the other hand t corresponds to an image with more noise. In short a smaller number means a smaller amount of noise and a bigger number means a bigger amount of noise. The second function is for the reverse process and we will call it p. p takes in an image xt and produces a sample xt minus 1 using the neural network. So this time xt goes in, which if you remember from before has more noise and out comes xt minus 1 with less noise. Great and with this knowledge we can now dive into the forward process. So Q of xt given xt minus 1 will be defined as the following. And don't be afraid by this formula, it is fairly simple, just the notation is a bit frightening. Let me explain. n is the normal distribution, x of t will be the output, this term will be the mean and this term will be the variance. Better refers to the schedule I was talking about before. All betters will be ranging between 0 and 1 and are ensuring that the data is being scaled so that the variance doesn't explode. To better understand this let's take a closer look at the linear schedule proposed by the 2020 paper. We find the smallest better to be 0.001 and to grow linearly up to 0.02. Flotting both better and the square root of 1 minus better against time results in the following two plots. You can see that we more and more scale down the image, which acts as a counterpart for increasing the variance over time and keeping the variance inbound. Cool, we now understand how to apply one forward step. To apply say a thousand forward steps we could easily repeat this formula a thousand times. But there is an easier way of doing this in just one step. How cool is that? For that we need to define a little more notation, so bear with me it's not much. We'll define alpha t to be 1 minus better t and then we'll also define the cumulative product of all alphas from 0 until t. So we just multiply all alphas together. You'll see in a second how we are going to use this. Alright first of all we can rewrite the before established formula for one time step like the following using the reparametrization trick. Epsilon is taken from a normal distribution with mean 0 and standard deviation of 1. Now we rewrite it using alpha. Pause the video to see that they are actually equal. And now comes the trick. We can extend this to earlier time steps and go from t minus 2 directly to t by just chaining the alphas. And also from t minus 3 to t and so on. And you see where this is going. We can go all the way to the beginning and go in an instant fashion from x0 to xt. And now you also understand why we introduce the cumulative alphas to simply shorten the formula. All of this can now be also rewritten in the original form like this. And that's already it for the forward diffusion process. It isn't that hard right? So now let's move to the reverse diffusion process. So if you remember we already established a notation for the reverse process and called it p of xt minus 1 given xt. We can also express this formula in the following way. In that case we would have two neural networks which parametrize the normal distribution, which we can sample from to get xt minus 1. We already said that we fix the variance to a certain schedule, so we don't need to predict it and can take this as given. Now I could just throw you the rest of the formulas at your head and say that you should take this for granted. Then the video could be over in less than a minute. But I believe that's not the point and it wouldn't be useful. As a result I want to really show how all the things play together to come to the conclusion which leads to the final objective where we just have to predict the noise. So that's the end goal. Arriving at a formula which uses a neural network to predict the noise in an image between two time steps. Keep that in mind. In order to have the easiest time we will start by looking at the loss function of this whole thing. This will simply be the negative log likelihood. But there's a problem. The probability of x0 isn't nicely computable as it depends on all other time steps coming before x0, starting at xt. This would mean keeping track of t minus 1 other random variables which is just not possible in practice. As a solution we can compute the variational lower bound for this objective and arrive at a more nicely computable formula. Note that the overall setup of diffusion models is pretty similar to that of variational autoencoders. So make sure to look that up if some of the things may be unclear to you. I also put some references in the description. If you're not familiar with the idea of a lower bound let me give you some intuition. Say we have some function f of x which we can't compute. In our case the plain negative log likelihood. And we can prove that we have some function g of x which is always smaller than f of x which we can compute. Then by maximizing g of x we can be certain that f of x will also increase. In our case this would be ensured by subtracting the KL divergence which is a measure of how similar two distributions are and is always non-negative. So by always subtracting something non-negative from a function will always result in having something which is less than the original function. But I wrote a plus sign instead of minus. This is simply because we always want to minimize instead of maximizing. That's why we added and thus have something which is always bigger than the plain negative log likelihood. And then we can minimize this objective. However in this form the lower bound is not computable either since yeah we still have the quantity in which we can't compute. And that's why we need some clever reformulations to arrive at a nicer looking quantity. At first we rewrite the KL divergence to be the log ratio between the two terms. Now let's apply the Bayesian rule to the quantity at the bottom. The upper term can be summarized to a joint probability and now we can see that we can just write this as the probability of x0 until t. Now we can pull the bottom quantity to the top by using the fraction rules and split up the logarithm. And now you already see the magic. These two terms just cancel each other and thus we get rid of the annoying quantity which we can't compute. Great now this is our variational lower bound which we can minimize. And note that we know all of these things. The term at the top is just a forward process starting at some image from our data and the lower part can be rewritten as the following. And you see that we can calculate p of x t and the product of all these terms is just using our parametrized model p. Now in order to make this analytically computable the authors applied a couple more steps of reformulation to arrive at a nice looking lower bound which we will also do now. And I promise this will be the last math heavy part. So let's take our current lower bound and write it in product form which I showed before. By applying the log rules we can bring out p of x t from the denominator so that we are left with the fraction of the forward process and the reverse process. Next we apply more log rules to bring out the product which becomes a sum. Now the authors do something which might appear weird at first but will make sense in a minute. They take out the first term of the sum which now starts at t equals 2 instead of t equals So they split up the first term from the entire sum. They now rewrite the nominator in the sum using base rule which can be expressed like this. Pause the video and see for yourself, it's just a standard base formula. Now there is a problem. All of these quantities have a really high variance since we don't know what we started from. So this is why the authors drastically reduced this variance by also conditioning on x zero. Because just imagine this. In this picture you might be pretty uncertain where it came from. So there's a high variance. But say I also give you the original noise free picture now, the choice for possible candidates decreases and you might be more certain. And it gets even better. This formula has a closed form solution which I will talk more about later. Fusing this back into the formula will give the following. And now you might already see the reason why we pulled out the first term. If we hadn't done this, the first term would look like the following after applying base rule and conditioning on x zero. You see that we have a loop and other things which don't really make sense. Okay now we almost made it. Just a few more modifications to be done. So bear with me. First we split up the summation in two parts because we can. Now let's take a closer look at the second summation. See something? Let's take an example case and say capital T equals 4. Writing the summation out will give us the following. And you see that most of the things cancel and only the last term from the top and the first term from the bottom survive. That's why we generally can simplify the summation to just this. And now the formula will look like the following. Now for the last step we take a closer look at the last two terms and by applying the log rules and converting the inner quotient into an outer subtraction we see that these terms cancel each other. Now the authors bring this term to the front and fuse them in one logarithm which leaves us with the final analytically computable objective. We can now write the first terms in KL divergence notation and voila objective. And it gets even better. We can ignore the first term completely since Q has no learnable parameters it's just a forward process which adds noise over and over and P of XT is just random noise sampled from our Gaussian and also by the aforementioned theory that Q will converge to a normal distribution we can be certain that the KL divergence will be small. I also want to give one more hint since often these big math derivations might be logical and one can follow them but you barely understand why this is being done. And the most crucial part of the previous derivation is the part where we apply a base rule to convert Q of XT given XT minus 1 to this equation. Obviously one can understand that this is true but it doesn't seem obvious, at least it didn't to me, what the reason for that is. And to give some intuition for the reason we can take a look at the final objective. Look at the second KL divergence. It is a really nice quantity which we can compute as we will also see later. And deriving at this would have not been possible if we hadn't applied base rule. This reformulation and the extra conditioning on X0 gave us the possibility to have the forward process in the same form as the reverse process. Now we also saw the other additional benefits where a lot of things cancelled or simplified. So we are left with these two terms. Let's first focus on this one. We already showed that we can express P of XT minus 1 given XT like this. Nothing new here. One new network for the mean and one for the variance. However the variance is a fixed schedule as discussed before. And I also mentioned that Q of XT minus 1 given XT and X0 has a closed form solution Which we can also write in the form like we did with P. Deriving mu and beta would take quite a bit of time and would make the video even longer. That's why I decided not to do this derivation here since it is also not relevant for understanding the algorithm. However I will put a link in the description where you can find more about it. So the final version for mu and beta will look like this. Big formula, I know, don't be afraid, we'll get rid of it. Again beta is not relevant since it's fixed so let's focus on mu. We can simplify this formula. How? Remember the closed form of the forward process for generating XT in one single step? We can rewrite this in terms of X0 and express it like this. Plugging this into mu of XT and X0 will give us this. And you also see that mu doesn't depend on X0 anymore. And through some further simplifications we end up at this tiny formula. And you see that essentially we are just subtracting random scaled noise from XT. That's it. Again take a look in the video description to see the full derivation. Now this is what our neural network mu needs to predict and the authors decided to use a simple mean squared error between the actual mu and the predicted mu. And now they saw the following. XT is available as input to the model. So why predict this whole quantity and not instead just the noise? And that is why we can rewrite mu like the following. And as you see it's completely the same as mu t except for epsilon which is supposed to predict the noise. So let's rewrite the mean squared error like this and through simplification we arrive at this friendly quantity. Just the mean squared error between the actual noise at time t and the predicted noise at time t by our neural network given some image. And believe me or not this quantity is about to become even more friendly. The authors found that ignoring the scaling term at the front results in better sampling quality and generally an easier implementation. And I mean look at this. All of this heavy math derivation starting at the negative log likelihood going to the lower bound reformulating and simplifying this lower bound seeing that we only need to predict the mean which leads us to the insight that we only need to predict the noise and then simplifying even more because it just works better. Isn't that beautiful? So getting back to our starting formula we can now put in our new findings. The neural network for mu becomes this and once again the neural network for the variance is not needed and can be replaced by beta t. And to actually sample x t minus 1 we can apply the reparametrization trick again and arrive at this equation. So just the mean plus the standard deviation plus noise sampled from a standard normal. Now there's one more thing which we haven't talked about yet. The last term in our lower bound also needs to be taken care of. For up some clarification. Images have values between 0 and 255. The authors scale the images to be between minus 1 and 1 to have them in the same range as the prior standard normal distribution centered at 0 with variance of 1. Now we can just calculate the probability of x0 given x1 like this. Ok that was maybe a bit too much at once but stay with me I'll explain. D is the data dimensionality which is the number of all pixels in our case. So we iterate over all pixels in the image. And we have this evil looking integral but this just means that we integrate over the range around the actual value of the pixel in x0. And if this quantity actually predicts a mean value in the area of the true pixel then the integral will be high. And if that happens for each pixel then the product will result in a high number and thus the probability of x0 given x1 is high. However if we always predict a different pixel far away from the true value then the mass in the true pixel area is low and the overall probability will also be low. Let's take an example for one pixel. Say the true pixel value would be 10 over 255. Also say x1 has a value of 14 over 255 at that specific position. We now apply our network and it predicts a new mean of 11 over 255. Now we can plot this normal distribution centered at 11 over 255 with some arbitrary variance given by our schedule. And now we could integrate over the true region where the pixel in x0 is. Starting one pixel value below and going one pixel value above. Since the network did a good job at prediction the mass in that area is quite high. If however the network would have predicted a mean of 20 over 255 then the mass in that area would have been super low leading to a low probability. I hope this gives some intuition for understanding the formula. If not don't worry. The authors also just decided to get rid of it and approximate it by our neural network and the only difference is that at sampling time we don't add noise to it. Yeah, that's it. Sorry if you just spent your valuable time trying to understand this. And now our entire objective fits into one single line and can be written like this. That is what we optimize. T is sampled from a uniform distribution between 1 and t and this long formula in our neural network is just xt and we could also write it like this. That looks even more peaceful if you imagine where we started. So now let's take a look at the algorithms for training and sampling summarized. This one is the algorithm for training and you see we first sample some image from our data set. Then we sample t and we also sample noise from a normal distribution. And then we optimize our objective via gradient descent. Super easy right? Sampling is also not that hard. We first sample xt from a normal distribution. That's our starting point and now we iteratively use the formula which I already showed before that uses the reparametrization trick to sample xt minus 1. And here you can see how the case for t equals 1 is approximated. If t is equal to 1 we don't want to add noise. Why? Let's imagine we would add noise. If t is equal to 1 we are predicting x0 given x1. Our neural network predicts the noise in x1 and then we subtract it from x0. Now it wouldn't make sense to add more noise since we are at our final image and the additional noise would make the quality of x0 worse. So yeah that's the simple reason and eventually we are returning x0. And if we trained well we now have an image which could come from our data set. So you made it. We are done with the fundamental part of the fusion models. Great job. Both what I just explained was taken from the 2015 and the 2020 paper. Now we already said that there were more improvements done by the follow up papers from OpenAI. I'm quickly gonna show the updates which they made but I won't go into the details for time reasons. The first improvement is reconsidering the variance. Before we just fixed it to either better or better head which are the upper and lower bounds on the variance. Now we are also going to learn it. We are learning an interpolation between better and better head using the following formula. The authors just found it to work better than to learn it directly because these numbers are in a really low range. But now we just need to learn a number between 0 and 1 which works quite well. This also results in a slight change of the loss since it doesn't depend on the variance in its plain form. We add the variational lower bound to our loss and scale it with lambda which was defined as 0.001. Next up it was also proposed to use a better noise schedule. The new proposal doesn't look as simple as the linear and is defined by the following formula for alpha head from which we can derive better. Here you can also see what it looks like when we plot both schedules. It was designed to have little change at the extreme points at the beginning and at the end and to be linear in the middle region. And I've shown this figure before but once again you see that the cosine schedule behaves better and takes more time for destroying the information in the image. Make sure to check out the paper to get more explanation on the improvements and other things which were tried. And now for the last part let's quickly take a look at the results of the papers. Specifically I want to take a look at the FID scores of the different diffusion model papers and also compare to other non-diffusion models. Here I will only look at the scores on ImageNet 256 by 256 but feel free to check out all the papers to see different datasets and also other metrics such as bits per dimension. Unfortunately the 2020 paper hasn't trained on ImageNet but it obviously would perform much worse than the improved diffusion model which outperformed it in all other categories. The improved DDPM achieved a score of 12.3 on ImageNet. That's quite a good result but the improvements from OpenAI completely outperformed it and achieved an FID score of 4.59. By the way an ADM stands for Ablated Diffusion Model. Later in the paper they also proposed a technique for upsampling in the diffusion process which I didn't talk about in this video. With that they improved even more and achieved a score of 3.94. These results are incredible but let's take a look at how diffusion models compare to other models. You see that the improved DDPM ranks last among all these state of the art models. However the approaches from OpenAI are placed 4 and 6 and even better the results from the paper BigRock which is also a diffusion model which I didn't cover in this video, ranked even higher with an even lower FID score. But still the gold medal is being hold by a GAN paper with some margin. Although I believe that diffusion models will easily outperform GANs in the near future. We've been working so long on GANs to get them working and diffusion models have just recently raised and not much effort has been going into its research. So I'm definitely excited where all of this will be going. Also let me know if you would like to have a video about any of the other papers listed here. For example last time I already did a video about VQGAN. Check it out if you want and also feel free to suggest other papers which aren't listed here. And now finally we are done. Great job if you followed all the way until here. But don't worry if you didn't understand every single part. Therefore let's quickly do a recap so you take out the most important things of the video. Diffusion models are generative models which became super popular in recent times for beating GANs on image synthesis. The main idea behind diffusion models is that we have two processes. The first one is easy. We just gradually apply noise to an image over a lot of steps until the image has turned into complete noise. The cool thing is that we can also do this in one single step which saves us a lot of compute and time. Now the goal of the reverse process is that we learn how to remove noise from an image. But as we saw we don't instantly try to remove all of the noise, we do it step by step. That's just easier for the model to learn. For that we have a neural network which takes in an image and the current time step t and predicts the image at t minus 1. Well to be more specific we saw that we can reformulate it so that we only predict the noise in the image. Then we can subtract the predicted noise from the image at time t and get the image at t minus 1. We will do this over and over again until we arrive at t equals 0 and if the neural network was trained well we now have an image which could occur in our training data. Then we also looked at a lot of improvements which were introduced over time. We saw a lot of interesting architecture improvements but also looked at updates in the training process by also learning the variance for example. All of this is grounded by a lot of maths but if you got the main points you are already on a good way. And that should be it for this video. I really hope you enjoyed it and could learn something new. It took a lot of work so let me know about things I can improve in the future or other topics which I should cover. And with that being said I wish you a nice day.",
        "output": "### Introduction to Diffusion Models\n\n#### Diffusion Models for Image Generation\n- Diffusion models are popular for image generation tasks and competitive with state-of-the-art GANs.\n- They can generate highly accurate and realistic images based on text prompts and have applications in generating variations of images, inpainting, and creating animations.\n- Four fundamental papers on diffusion models introduce and develop various improvements to the technique.\n\n#### Overview of Diffusion Models\n- The main idea behind diffusion models is that they consist of two processes, forward and reverse diffusion processes.\n- Forward diffusion process involves iteratively applying noise to an image, turning it into complete noise.\n- Reverse diffusion process uses a neural network to iteratively remove noise from the noisy image, eventually producing a clear image.\n\n#### Architecture and Training Process\n- The architecture follows a U-Net-like structure, which takes an image and the current time step as inputs, and predicts the image at the previous time step.\n- Neural network predicts noise in the image rather than predicting the mean or the original image directly.\n- A schedule for noise application controls the amount of noise applied to the image, with the cosine schedule outperforming the linear schedule in the image-to-noise transformation.\n- Various architecture improvements are done over time, such as adaptive group normalization and classifier guidance.\n\n#### Math Behind Diffusion Models\n- The forward process can be expressed analytically as a series of formulas involving normal distributions, schedules for noise application, and probability distributions.\n- The reverse process is based on variational lower bound optimization and also involves a series of mathematical manipulations, resulting in a relatively simple final objective.\n- Several improvements have been made in the learning process, including learning the variance of the noise, a better noise schedule, and approximating certain terms in the loss calculation.\n\n#### Results and Comparison\n- Diffusion models have shown competitive results in image generation tasks, with some models outperforming GANs.\n- OpenAI's improvements to diffusion models have notably improved performance and enabled better image synthesis capabilities.\n- Diffusion models show potential for further improvements and surpassing GANs in the future."
    },
    {
        "instruction": "<|SYSTEM|>You are a scholars that is taking markdown notes about some video, you would be giving some written scripts for the task.\nYou have finished writing the notes, and you want to extract the title from the notes.\nThe title should only be the name of the technology or algorithm, no extra words needed.\nPlease write the notes in markdown format. No need to write \"Title:\" for the Title. Make sure the title is within 10 words.\nHere is an example:\n## Hidden Markov Model\n<|ASSISTANT|>## ",
        "input": "<|USER|>### Introduction to Diffusion Models\n\n#### Diffusion Models for Image Generation\n- Diffusion models are popular for image generation tasks and competitive with state-of-the-art GANs.\n- They can generate highly accurate and realistic images based on text prompts and have applications in generating variations of images, inpainting, and creating animations.\n- Four fundamental papers on diffusion models introduce and develop various improvements to the technique.\n\n#### Overview of Diffusion Models\n- The main idea behind diffusion models is that they consist of two processes, forward and reverse diffusion processes.\n- Forward diffusion process involves iteratively applying noise to an image, turning it into complete noise.\n- Reverse diffusion process uses a neural network to iteratively remove noise from the noisy image, eventually producing a clear image.\n\n#### Architecture and Training Process\n- The architecture follows a U-Net-like structure, which takes an image and the current time step as inputs, and predicts the image at the previous time step.\n- Neural network predicts noise in the image rather than predicting the mean or the original image directly.\n- A schedule for noise application controls the amount of noise applied to the image, with the cosine schedule outperforming the linear schedule in the image-to-noise transformation.\n- Various architecture improvements are done over time, such as adaptive group normalization and classifier guidance.\n\n#### Math Behind Diffusion Models\n- The forward process can be expressed analytically as a series of formulas involving normal distributions, schedules for noise application, and probability distributions.\n- The reverse process is based on variational lower bound optimization and also involves a series of mathematical manipulations, resulting in a relatively simple final objective.\n- Several improvements have been made in the learning process, including learning the variance of the noise, a better noise schedule, and approximating certain terms in the loss calculation.\n\n#### Results and Comparison\n- Diffusion models have shown competitive results in image generation tasks, with some models outperforming GANs.\n- OpenAI's improvements to diffusion models have notably improved performance and enabled better image synthesis capabilities.\n- Diffusion models show potential for further improvements and surpassing GANs in the future.",
        "output": "## Diffusion Models"
    },
    {
        "instruction": "<|SYSTEM|>You are a scholars that is taking markdown notes about some video with Wiki style, you would be giving some written scripts for the task.\nAnd summarize each paragraph with a title.\nRemember to keep the technology or algorithm name, very important.\nPlease write the notes in markdown format.\nHere is an example:\n### Definiation\nA deadlock is a situation in which two computer programs sharing the same resource are effectively preventing each other from accessing the resource, resulting in both programs ceasing to function. \nThe earliest computer operating systems ran only one program at a time.\n\n### What it have to do with Relationsal Database ManagementSystem\nIn a database, a deadlock is an unwanted situation in which two or more transactions are waiting indefinitely for one another to give up locks. \nDeadlock is said to be one of the most feared complications in DBMS as it brings the whole system to a Halt. \nWhen you tries to insert or update data on database, sometime your record need to depend on the row record.  \n<|ASSISTANT|>#### ",
        "input": "<|USER|> Hey there and welcome to this video. In the following we will be doing a paper implementation of the VQGAN model. With this paper you will be able to generate new beautiful looking images of a given dataset. For example the authors trained on the Flickr landscape dataset and were able to generate these realistic looking images. These are not only really accurate to what you could find in real nature, but with this method you can also generate much bigger image sizes than you normally trained on. And for this video we will be using the Oxford flowers dataset which consists of about 8000 images of flowers and our goal is to generate new flowers. For up I would highly recommend you understanding the theory behind VQGAN because I will not go over this in much detail in this video. So check out my explanation video which is linked down below if you haven't already. Ok so let's go over to the code. Ok let's start with the encoder and the decoder. Both of the models have a lot of modules in common and we will thus create a helper.py file where we will code all these shared modules. So we start off by importing all the torch modules. And the first module that we will need is a group norm. So this is the first module that we will need and another one that is being used quite often is the swish activation. And this is just an alternative to the relu activation and it shows some small empirical improvements. So having these basic modules established we can now go to the next module which is going to be a resnet block which is heavily used by the encoder and the decoder. So this is going to be the residual block. It is taking in the number of input channels and the number of output channels. And it is defining the block to be a torch sequential and this consists of a group norm and the swish we have defined above. And then it is going to use a convolution with a kernel size of 1, a stride of 1 and a padding of 1 and then all of this repeated once again. The group norm, the swish and the 2D convolution. Then also what we are going to add is a channel up. This is just securing that the skip connection will work because if we have different input and output channels the skip connection will crash since both elements are not the same. So we simply define a channel up and you can see how this is being used down here. We have the input channels. If they are not equal to the output channels then we will first channel up basically our input and then use this as our skip connections and add this to our block that is defined up here. And if both the input channels and output channels are the same we simply have the normal skip connection where we add the normal inputs to our block. Next up we will be coding a class for a down sample block and an up sample block which are being used by the encoder and the decoder respectively. The up sample block is using a single convolutional layer with an additional pre-scaling of the input features. This results in an output which is double the size of the input. For that it makes use of torches interpolation function which takes in the feature and the scaling factor and will simply double both the height and width and fill in the new spots by their nearest neighbors. You can see an example here where I defined an input tensor and applied the interpolation and the result is a tensor twice as big and having the blanks filled in. So now let's define the down sample block. As you can see it's just the reverse of the up sample block in which it is cutting the size of the tensor by half. First the padding is added to both the height and width of the input tensor and then the down sampling magic happens through the convolution which has a stride of 2. Then here's an example how the padding works by inserting one new row and one new column and filling it with zeros. This is done because else we would get a tensor which is not exactly half the size of the original one but is off by one. Okay for the next part we will be coding the non-local blocks and as I already mentioned in the explanation video this is just sort of an attention mechanism just with pure convolutions and again I will link the paper down below so you can check it out if you're interested. So we have now defined all the layers that we're going to use. You can see that it is almost the same as the normal attention. We have the Q, the K and the V linear transformation and additionally we also have a group norm and we have a projection for the last part. So let's define the forward pass. So first we normalize the inputs by our group norm and then we just transform our inputs into the Q, K and V matrices. So and now we will do some reshaping so that the dimensions for the matrix multiplications will match. So that's basically just a reshape and you will see in a second how this is important for the matrix multiplication. And what we're doing here is simply the normal attention. First multiplying the Q and the K matrix together by using Torch-Spech matrix multiplication and then we simply scale by the square root of the channels as also done in a normal attention mechanism and then we apply the softmax to get a probability distribution and then again some permutations so that the sizes match for the upcoming value multiplication. And here we construct the A matrix so we attend to the important things by using again Torch-Spech matrix multiplication and then we can reshape to bring it in the original shape back again. And then what is also different to a normal attention layer we again make use of a skip connection. So yeah and that's it for the non-local block and as you see it is simply an attention mechanism based on convolutions. And these are already all the sub-modules which we will be using in the encoder and the decoder so we can now go over to coding them. Okay so we will be starting by coding the encoder and first of all we import all the necessary stuff. And now we import all the modules which we just coded in the helper.py file. Okay so now let's define the encoder class. So first we define the channels and the channels is just a list of all the different channel sizes which we will be using. So we start off with an image with three channels and we scale it by the residual blocks to 128 and 128 and so on until we finally arrive at 512. Then the attention resolutions since we are downscaling we are decreasing the resolution size and at this resolution we will imply another non-local block. And the number of resonant blocks simply tells us how many resonant blocks we want to put in at each stage so this is also a measure of how big our encoder model will get. The resolution is a measure of how big our images are. Okay so now let's define all the other layers. Okay so this code is simply appending to our layers all the necessary things. So we go over all the channels and then we always add the number of resonant blocks which we want to add in our case we defined it to two. So for each channel size we append two residual blocks and then we check if the current resolution is in our attention resolutions and since we only have like one element in it then we append only at the resolution of 16 a non-local block. And then for each iteration except the last two we append a downsample block and decrease the resolution by half. Okay so these were the M residual blocks and downsample blocks and now we can append the rest of the blocks which are shown here. And now we added a resonant block, a non-local block, another resonant block, a group number switch and then an output convolutions which maps to our latent dimension. And then we construct the model down here by putting everything in a sequential and then in the forward method we simply call our model and put our inputs through. Okay so yeah that's already all there is to say about the encoder and now we can switch to coding the decoder. And once again importing everything we need for the decoder. Now let's define the decoder class. And if you're wondering what args is it is basically just all the necessary arguments which we will later provide by Python's argument parser. And as already mentioned in the explanation video the decoder is simply the reverse of the encoder however it is quite bigger than the encoder. So we again define our channels, our attention resolutions, so the resolution where we will put in another non-local block then the number of resonant blocks as you already see it's three instead of two so that's where the bigger size comes from and we define our resolutions. And you can see we start by 16 by 16 images and that is also the image size that the encoder is encoding images into. Okay so now let's define our layers. So these are the first four layers our decoder consists of and now let's define all the other layers. So again this is pretty similar to the encoder and using the loop we append the main layers to our layers list and now we simply append the rest of the three modules to our layers. And the forward pass is simply just again calling our model. So the next part which we will be coding is the codebook which is used for the vector quantization and as usual we start off with the normal imports. So now let's define the torch class and again this takes in our arguments. Here we simply define the number of codebook vectors which we want to have and then also the latent dimension of these codebook vectors and we define beta and beta is a weighting factor for the codebook loss as you will see in a minute. So here we define the essential thing of the codebook which is the embedding matrix and also additionally we initialize the weights by a uniform distribution. Okay so that's already everything for the initialization now we can go to the forward pass of the codebook. So here we prepare the latent vectors for finding the minimal distance to the codebook vectors and we simply do this by putting the channels as our last dimension and then flatten the latent vectors. Next up we can now calculate the distances to the codebook vectors. And if this formula looks like gibberish to you and you are totally overwhelmed don't worry this is basically just the expanded version of the L2 loss. So you can pause the video now and actually see for yourself that it's really nothing more than just the expanded version where we have the first term squared, the second one squared, both of these added and then simply subtract twice the multiplication of the first element by the second element. And now we have the distances between all the latent vectors and all the embedding vectors and now we can find out which ones are the closest. And now we need the indices of the embedding vectors to each of the latent vectors and we do this by simply using Torch's argument function, passing our distances in and looking along the first axis for finding the closest vectors. And now having the indices we can now get the actual codebook vectors from our embedding matrix and then we additionally reshape them to bring them back to our original shape Z. And now for the next part we need to construct the codebook loss. And you may recognize that this is already the part where we imply the stop gradient function which is called in Torch the detach function. So in the first part we remove the quantized latent vectors from the gradient flow and then subtracting it from the original latent vectors, squaring it and then for the second part and you see how better it's being used here as a weighting factor and this is now simply the opposite where we now detach the original latent vectors and subtract it also again from the quantized ones and getting a distance measure by that. And yeah don't be confused by this part. This is simply for preserving the gradients for the backwards flow and then finally we have to switch the axis again and we move the channel axis for coming after the batch axis now so that it has the original shape as our inputs. And then we are returning our quantized latent vectors, the indices of these codebook vectors and our loss. And if you're wondering why we are also returning the indices they will be used later in the transformer part. Okay so having the codebook ready too and the encoder and the decoder we got now every module that we need for building the VQGAN. So again let's start with the usual import and furthermore we will also import the encoder, the decoder and the codebook. So now let's define the VQGAN class. The arguments we take in will be again just the args which is the arg parser and now let's define the encoder, the decoder and the codebook to be a sub module of the VQGAN and moreover we will also put the encoder, the decoder and the codebook to the specified device. And the original VQGAN is also making use of a pre-quantization convolutional layer and a post-quantization convolutional layer. And they are just normal convolutional layers which will be applied before the quantization and after the quantization. And you will see in a second how this is exactly done. So now let's define the forward pass. Okay first of all we just encode the images using the encoder. Then we apply the pre-quantization convolution and then we quantize the encoded images and get back the codebook mapping, the indices and the codebook loss. And for the last part we try to reconstruct the images by using the decoder. And finally we return the decoded images, the indices and the codebook loss. So essentially this would already be all there is to code. However we will now be adding two more functions, one for encoding and one for decoding. They will be later used by the transformer separately and this will make our lives way easier. So what that is, it is essentially just a forward method split up in two separate functions. For one the encoding of an image and the second for the decoding of an image. And if you saw the explanation video you know that the authors used a variable lambda as a weighting factor between the VQVAE loss and the GAN loss. So let's write a function for calculating lambda. So if you take a look at the calculation of lambda again you see that we are taking a ratio. G stands for the decoder and L resembles the last layer of the decoder. So this notation means to take the gradient with respect to the last layer of the decoder for both the perceptual loss and the GAN loss. In the code you see that we are first grabbing the last layer and then its corresponding weight. So we are calculating the gradient using Torch's autograd's grad function for both the perceptual reconstruction loss and the GAN loss with respect to the last layer. Retaining graph means that we want to keep the current computational graph which is just important that the backward pass can function properly. Now we take as described in the formula the ratio between both gradients and clipping the values to be between 0 and 10000 and then returning lambda. One more thing that the authors suggested is to start a discriminator later in training. This ensures that the training doesn't fail to converge. That way the generator has time to learn already something about reconstructing images before the discriminator joins, who would otherwise have an easy time predicting which images are real and fake if it joins right from the start. The discriminator factor is an additional weight by which we will weight the discriminator. However for this video we will just set it to 1. But feel free to try different values or even schedules. And we will write one more function which is also being used later by the transformer and that is a function for loading the checkpoint for the VQ GAN. Okay yeah then for the next part we have the discriminator and I will not be coding this right here because it is just taken from the official CycleGAN and Pix2Pix paper. So you can check out the link up here and I also simplified quite a lot of the code. If it still looks really unreadable have a look at this screenshot here. This is just basically what the init method is constructing. You see that we have this architecture which consists only of convolutions, leaky rail use and batch norms. And I also said in the explanation video that we are predicting each patch and here is an example of what the output will look like if we feed in an image. And as you can see we are getting back a 30 by 30 matrix which resembles each patch being real or fake. Okay so yeah for the next part we are going to look at the perceptual loss which is gonna be the LPUBs. Sorry if I mispronounce that. And yeah out of time reasons I'm also just gonna show the code right here because it is also just taken from the authors and for the overall understanding of the VGGAN it is not that important. But anyways we are going to start off by downloading the model which is provided by the authors and then here is the main class the LPUBs where we are going to make use of the VGG16 which is defined down here. And this is also loading a pre-trained model from the TorchVision module. And basically the only thing that is important for us is the forward pass of the LPUBs. You can see that we will feed both the real and the fake images through the VGG and first applying a scaling layer which you can see down here is just a normalization. And then we are taking the differences between each of the different feature channels and eventually we are just going to take the average and returning this. And the way we are going to use it is shown here by constructing random real and fake images and then applying the loss onto it. And you can see we are getting back a scalar which is a representation of how similar the images are. Ok so now we are almost set for actually writing the training code. However there is one more thing which we have to add and I am also just going to show this right here. And this is a utils file of different classes and methods which we are going to use throughout the video. And we start off by our data set and this is just a class for how we are going to load the data that we are going and specifically loading the Oxford flowers data set. And you can see that we are inheriting from data set so we are constructing a Torch data set and for that we need at least two methods. That is first of all the length method which is going to return as the name suggests the length of our data set and then the get item. And get item will take in an index and will return an image. And you can see that we are applying a preprocess function on it and this is just opening the image converting it to RGB if it is not already. Then we are putting it into a NumPy array and applying a preprocessor. And the preprocessor is simply a rescaler so if the sizes don't match we will bring it to the desired sizes that we want and we are also applying center crops. So after applying this preprocessing to the image we will scale the image to be between minus one and one and then also transposing the channels because the normal pillow library will load images and put the channel dimensions to be the last one. However Torch wants it to be the first one so that's why we are applying the transpose And then we are simply returning the image. And down here you see we are actually loading the data so we first are constructing the image paths method which I just showed up here which takes in the data set path which is provided by our argument parser and then the size. I already set it to 256 because that's the size we will be dealing with and then we are constructing the data loader which takes in our training data our batch size and we say that we do not want to shuffle the data. However feel free to set this to true if you want to play around. And then down here we will define two more methods and you will see in a minute where they will be used which is going to be an initialization for weights for convolutional layers and for batch norms and then down here a plot images function which we will be using in a transformer stage for plotting basically reconstructions new samples and so on. And now finally we will be coding the actual training loop for the VQGAN and as usual I will start off by importing all the necessary modules. As you see we have our perceptual loss to VQGAN then the aforementioned utilization functions for the data loading and the weight initialization then the discriminator and then other standard modules for example the argument parser. Okay so now let's define the training class. And here you can see how we are using the weights initialization function and they are going to be used for the discriminator. Now we are going to define the perceptual loss and the last thing we are going to need is to define the optimizers for the training. And we will be going to write this function in a second. However before that we are also going to call a prepared training function which will just set up all the directories for us. And then finally when all of these things are done we can actually call our training function and also one more thing we are going to take off in here we are going to push all the models to the desired device. Okay so now first of all let's define the configure optimizers function. Okay so what we are doing is pretty simple we are just going to construct two optimizers and we are going to be using Adam and for the first optimizer for the VQVAE part we are just going to get the encoder parameters, the decoder, the codebook and from the to pre and post quantization convolutions all the parameters and bring them all into one big list and then feeding these parameters to the Adam optimizer. And for the discriminator optimizer we are also going to be using Adam and same here we are going to get the parameters of the discriminator and the learning rate and the better arguments will be later defined in the argument parser and then we are returning both optimizers. So next up let's define the prepared training function and the prepared training function in our case will be as simple as creating two directories one for the results where we will be logging images to and then one for checkpoint where we will be saving the models to. Okay and last but not least we are finally going to write the training function. First of all we are going to load the data and here we are going to call the load data function which we have defined in our utils file. Next we are going to assign a variable to the number of batches we will have in one epoch and now we are going to write the loop. For that we are going to iterate over all the epochs. Here we are just using TQDM for better visualizations of the training process and now we are going to iterate over all the batches. Since we are training this with a GPU we will first bring all the images to the proper device. Next we are going to call our V2GAN model and get back the reconstructions of the images and the code book loss. Next we will be sending both the real and the reconstructed images to the discriminator and remember we wrote a function for a discriminator factor meaning that we want to start the discriminator later in training. We are now going to calculate the discriminator factor and you see that the threshold will be an argument which we will also provide an argument parser which will indicate when to start the discriminator. Next up we will be calculating the perceptual and the reconstruction loss and maybe one thing to note is that we are not going to use the L2 loss for the reconstruction loss we are going to use the L1 loss meaning we only take the absolute difference between both the images and the decoded images. Here we are just going to fuse them together and take the mean value and the G loss will be the discriminator loss for our generator. Now it's time to calculate lambda and now finally we can calculate the VQ loss and you can also see how the discriminator factor is being used here as to that the discriminator loss will not be taken into account until we have passed the discriminator star threshold. And now here comes the second part for the hinge loss which is going to affect the discriminator and if you're confused where the formulas for both of these terms and the G loss are coming from this is just a hinge loss which is inspired by support vector machines and you can see a screenshot of the loss here and I will also link some resources in the description where you can learn more. And in this line we're adding the real and fake discriminator loss and also weighting it by the discriminator factor. And now it's time for the backward pass. First we will zero-grad both optimizers and backward the loss. And now we're going to do the weight update and also make sure to do all the last steps in this order because else you will get some really annoying errors where I spent a lot of time solving these. And now we're going to write some code for logging images, logging both losses to TQDM and making sure we are saving the VQGAN after each epoch. And yeah that's it. That's all that we need for the VQGAN training. And now finally let me just define all the arguments that we are going to need for the training. Okay as you see I've now defined all the arguments and they consist first of all of the latent dimension of the image sizes of the number of codebook vectors. We will be using 1024. Then the beta which is the weighting factor in the codebook loss. Then image channels we will be using RGB images so it's going to be 3. Then I define a data set path then the device to be default for CUDA. So GPU training. Then I will use a batch size of 6. I will be doing 50 epochs. This is going to be the base learning rate. Here both beta 1 and beta 2 for the atom optimizer. Then we are going to start the discriminator after 10,000 steps. And then as I already mentioned before the discriminator factor will be equal to 1. And two more weighting factors one for the reconstruction loss and one for the perceptual loss will also be initialized with 1. Then I set the specific path for my data set. Then I set the specific path for my data set where the oxford flowers are located. And then I'm going to call our training class. And I also want to mention one more thing is that my code is heavily oriented on the structure of the original implementation. So if there's anything which is unclear make sure to check out the original implementation which I also linked down below. So the model has now been training for about 50 epochs and you can see the results here. On top you see the original images and below that you always see the reconstruction for each image. And as I go through here you can see that the method is working, that it sort of reconstructs. It gets the shape pretty good I would say but it still is a pretty unsharp and also sometimes the colors are a bit off. And you see this weird sort of pattern which goes over the whole image. I'm not quite sure where this is coming from but I am quite sure that if you let this model train for even longer that you will at some point get to the results which the paper also has showcased because I just trained it for 50 epochs. But feel free to try it out even longer and maybe try even other hyper parameters for example the learning rate or any other hyper parameter. And maybe just a quick side note if anyone's interested I trained this on my own machine and rtx 3090 and it took the gpu about 12 hours for about 50 epochs with a batch size of six. And I will also upload the checkpoint from the last epoch that I trained it on so you can try it out yourself for the flowers data set and see yourself so that you don't have to train it on your own. So after having the first stage of the VQGant trained we can now take care of the second stage. So the basic idea behind this stage is that we encode images to their codebook vectors and then replace some of these vectors randomly and then have a transformer for finding the wrong codebook vectors and have them replaced. So we need some transformer architecture and the office decided to go with a GPT like architecture and specifically minGPT. Since this video is more focused on the usage of the transformer instead of how the transformer is constructed I will just briefly go over the code here. So and this is the code as you can see we have the main class the GPT class here and then it is really just a transformer architecture. You see that we have the token embeddings here which is an embedding matrix and then we have the positional embedding then the dropouts and here comes the real transformer part which is defined in the blocks up here and also just here you can see that we have self attention and the MLP on top. So really just a basic procedure of a transformer and then we have a decoder head which consists of a layer norm and a linear layer and you can see that the output size will be the same as our vocabulary size in our case 1024. And furthermore down here we have some weights initializations and then down here you can see the actual forward method where we first encode the tokens and then down here get our positions embeddings and then add them together apply the dropout and then feed them through all the blocks and apply the layer norm and then finally we get predictions by feeding it through the prediction head and then returning the predictions. So yeah that's essentially the whole transformer class which we will be using and now we can go over to actually coding our VQGAN transformer part. So let's get into the code and start by importing all the modules and defining the transformer class. Also a quick note in this video I will only be considering the unconditional case for the second stage and not the conditional for time reasons. However it is fairly straightforward while bringing the current code through the conditional case with which you could train to generate class specific images. First of all we will be defining a start of sentence token so that the transformer always has a valid token to start from. Next we load the VQGAN model using a function we will create in a second and define the transformer config and initialize the transformer. And the last variable we need will be the ratio of tokens we keep and don't replace by random tokens. This also acts as a hyper parameter and you can feel free to tune it as you want. Now let's define the function for loading VQGAN which also sets it in evaluation mode. We will need a couple of helper functions which we will define now. First of all a method for encoding an image through the codebook vectors with an additional flattening of the tokens since the transformer expects a flat sequence. Next up the reverse of the encoding method which takes in the indices of the codebook vectors and then returns the image as a number array. Specifically we will be calling the embedding matrix of the codebook with the indices and retrieve the matching latent vectors. Then we reshape the tensor and move the channel dimensions to be after the batch dimension and finally calling the decode method for our VQGAN. This method will be later used for plotting images. Now we can define the forward pass for the second stage. We will take in a batch of images and first encode it to their corresponding quantized latent vectors. However we will only consider the codebook indices since that is what the transformer takes in. After that we will define a tensor which contains the SOS tokens which we will later insert at the beginning of each element in the batch. Now we can take care of replacing random indices by other random indices. This is done by first constructing a mask where each element will be a 1 with a probability of pkeep. So if pkeep equals 0.5 then our mask consists of 50% 0s and 50% 1s on average. Next we sample random indices in the shape of our codebook indices and then create the new indices by leaving all indices untouched if the mask has a 1 at this position and take the random index if the position has a 0. Now we simply concatenate the SOS tokens to the beginning and feed the altered indices in the transformer which is supposed to predict the original indices back. And finally we are returning both the predictions and the original indices. So that's it for the basic model and its forward pass. In the following I will be creating a few more functions for actually sampling images from the transformer. First of all a topk function. In general this sort of function returns the k highest values in the tensor. PyTorch already provides such a function however only returns the k elements and their indices and we want to have a tensor which has the same dimension as the input and sets every element to some really small number and keeps the topk entries. And this is what our function will be doing. This will be important in the next function we are going to write which is a method for sampling images. Let me first explain the idea. The transformer is autoregressive meaning we are generating each token one at a time. So say we want to generate a completely new image. We start with the SOS token and feed it in. Then we let the model predict which token could be the next one. Now we have the SOS token and the first image token. Now we take both of these and feed it back in again and then end up with 3 tokens in total. We do this over and over until we have enough tokens. For our normal images we need 256 because our latent dimension is 16 by 16. So our steps would be equal to 256. And in detail the method works by first setting our transformer in evaluation mode and prepend the SOS tokens just as in the normal forward method. Now we iterate over the number of steps and feed the current image into the transformer and then only take the very last prediction and scale it by the temperature. Now we apply the topk function which we wrote earlier and convert the predictions into probabilities using a softmax. Finally we sample the new token based on the probabilities and append it to the array of image tokens. And after all the image indices are generated we cut off the SOS token, set the transformer back to training and return the image. Super simple right? Oh and one more thing. Remember that with VQGAN we can generate images much larger than we trained on right? Now you can understand why. Because we don't generally have to stop after we have generated for example 256 tokens. We could just go further and generate 500 or even more. And since our decoder is fully convolutional it doesn't care how many tokens it receives. So now for the final method for the VQGAN transformer. We just write a function which helps us investigate the quality of the model. The outcome of this method will be a dictionary which contains the normal image we fed in, the reconstruction of this image which isn't using the transformer, then the famous pictures where we cut off half of the image and let the transformer sample the bottom half and then a completely new sampled image. Now this will be the last file which we have to code and then we are ready with the whole VQGAN model. And as already said this will include the training class for the VQGAN transformer. And as always let's import all the stuff and define the class. So you see that we are defining the model to be the VQGAN transformer. Then we again need to write the configure optimizers which will return the optimizer and then we are calling the self.train which will take care of the training loop. So this really long function is basically just serving one purpose. Is that some modules are supposed to get weight decay and some modules are not. And in particular all the modules which are in here, so all the linear modules, they are supposed to get the weight decay and layer norms and embedding layers and also the positional embeddings, they are supposed to not receive weight decay. And this is essentially all that is happening in this function and finally we are constructing the AdamW optimizer and returning it. And after this really long method we can now write the training loop. So until here that's completely the same as the VQGAN training. We are first loading the training data set and then iterating over all the epochs, then again constructing the TQDM which is just for the visual representation of our loss and the process of the training in general. And then we loop over our training data set and get the images out. Now we will put them to the proper device. And as you can see here we are now calling our model which takes in the images and returns the logits and the targets. We will be using cross entropy for calculating the loss. So this will be the loss and now we can backward the loss and then calculate the weight update for the transformer. And now I will just write the code for updating the progress bar of TQDM. So that is the whole training loop and now after each epoch we will sample some images and then save them to the disk and also plot the images and save the transformer checkpoint. And now we got the whole training loop for the transformer training set up and for the final step I will be again creating the argument parser, creating all the arguments and then calling our training function. And as you can see I just copied all the arguments from the VQGAN argument parser and I simply added just these two things that we need. First of all the probability for index keeping which we were using in the transformer and also the start of sentence token which is also used in the transformer. Ok so now I will start the training and I will come back when the training has finished and show you the results. So the training of the transformer has now been finished and you can see the results shown here. And I gotta say you can recognize that these are flowers but obviously it is not perfect. I've been training it for a hundred epochs and the loss has still begun down so I believe that training it for even longer will again result in better results. And again you see here that this weird pattern appears again and this clearly shows that the second stage is dependent on the first stage. If the first stage is not well trained the second stage will also not magically create beautiful looking flowers. So I believe that this is a matter for better training the first stage and then I'm sure that you will also get pretty nice results here. And again just for the record it took my computer about 8 hours to complete the 100 epochs. And just quickly one more thing if you're interested in running the code on your own you can check out the github repository where I uploaded all the files. So and that's it with the implementation of the VKGAN model. I really hope you enjoyed it and let me know about things I can improve since this one and the explanation video are basically my first videos on YouTube. And with that being said I wish you a nice day.",
        "output": "### VQGAN Implementation\n#### Encoder and Decoder Modules\n- Define group norm"
    },
    {
        "instruction": "<|SYSTEM|>You are a scholars that is taking markdown notes about some video, you would be giving some written scripts for the task.\nYou have finished writing the notes, and you want to extract the title from the notes.\nThe title should only be the name of the technology or algorithm, no extra words needed.\nPlease write the notes in markdown format. No need to write \"Title:\" for the Title. Make sure the title is within 10 words.\nHere is an example:\n## Hidden Markov Model\n<|ASSISTANT|>## ",
        "input": "<|USER|>### VQGAN Implementation\n#### Encoder and Decoder Modules\n- Define group norm",
        "output": "VQGAN"
    },
    {
        "instruction": "<|SYSTEM|>You are a scholars that is taking markdown notes about some video with Wiki style, you would be giving some written scripts for the task.\nAnd summarize each paragraph with a title.\nRemember to keep the technology or algorithm name, very important.\nPlease write the notes in markdown format.\nHere is an example:\n### Definiation\nA deadlock is a situation in which two computer programs sharing the same resource are effectively preventing each other from accessing the resource, resulting in both programs ceasing to function. \nThe earliest computer operating systems ran only one program at a time.\n\n### What it have to do with Relationsal Database ManagementSystem\nIn a database, a deadlock is an unwanted situation in which two or more transactions are waiting indefinitely for one another to give up locks. \nDeadlock is said to be one of the most feared complications in DBMS as it brings the whole system to a Halt. \nWhen you tries to insert or update data on database, sometime your record need to depend on the row record.  \n<|ASSISTANT|>#### ",
        "input": "<|USER|> Hey there and welcome to this video. In the following I'll be doing a paper explanation of VQGAN, which stands for Vector Quantized Generative Adversarial Networks. Yes, that's a tongue breaker, but it's actually a pretty cool model for image generation. The authors took a lot of existing methods and combined them in an amazing way to produce images as the ones you can see here. One special thing is that this model can generate images much larger than it was trained on. They turned in resolutions of 256 and were able to generate images in the megapixel regime by adding a couple cool tricks as you will see later. So let's get started. The VQGAN model is an extension of the VQVAE model and simply integrates it into the usual GAN architecture. I will quickly do a recap of the VQVAE model here. However, if you want to dive deeper into it, I will leave some resources in the description. VQVAE stands for Vector Quantized Variational Autoencoder. A variational autoencoder is no more than a fancy autoencoder and an autoencoder is a model for learning a compression of some data. The end goal is to be able to generate new samples from your data distribution. Usually this is done for images. So let's take an example. Say you have a whole bunch of images of flowers. Your goal is now to generate new flowers from your data distribution, which simply means to generate new images of flowers which could occur in your training data. So the idea behind an autoencoder is to learn a mapping, or which is also called an encoding, from the image space to a low dimensional latent space. This latent space usually has a dimension of 256, sometimes less, sometimes more. After mapping the image to the latent space, the image will be reconstructed or decoded by a mapping from the latent space to the image space. We will refer to the mapping as the encoder and the decoder. Both the encoder and the decoder can be simple convolutional architectures. To be more precise, the encoder could take in an image of size 512x512 and map it to the latent space which has a dimension of 256. After that the decoder would take in the 256 dimensional latent space and try to reconstruct the image and map it back to the original 500x512 image space. To try this you would take some sort of reconstruction loss which compares the original image to the reconstructed image. Great, so now you have learned what an autoencoder does in a nutshell. The variational autoencoder is just an extension of this and just reparametrizes the latent space to predict means and standard deviations. The reason for that is that the original autoencoder is using the latent space kind of not as we wish. In short, the latent space of images which are similar to each other do not lie close to each other in the latent space. So an image of the same flower just taken from another angle could lie miles apart in the latent space which is not intended for a lot of reasons. The second problem is that the distribution is not centered in the latent space so the latent vectors of images could lie literally everywhere. With everywhere I mean vectors like this. However we would want them to be close to the origin to have them in a similar scale. So this is where the VAE comes into play. As I already said it reparametrizes the latent space to be centered around the origin plus to have images lie around similar images. If you want to know how this is exactly done you can take a look at the video description. So that's all good and nice but why do we need to go further and introduce VQVAE? The main advantage that VQVAE adds is that it turns the latent representation of your data into a discrete one. The normal VAE simply learns a continuous latent space where all values could be taken and decoded back to an image. However this would result in a massive number of points which could be hard to properly learn by the VAE. Instead we would like to restrict the possible number of latent vectors so that the model can concentrate on these. And due to the sparsity of data and the discrete nature of classes turning this problem into a discrete one favors the learning process and results in better outcomes. To realize this we make use of vector quantization. The idea is to learn what's called a codebook. This codebook consists of a discrete number of vectors which all share the same dimensionality. You can imagine this as having our 256 dimensional latent space again, however this time just a certain number of points can be taken. And these are vectors from our codebook. So having our codebook established what we do is the following. After encoding an image to our latent space we take this vector and replace it by its nearest neighbor from the codebook. For example imagine that our codebook has 100 vectors which are all of dimension 256. After feeding our image through the encoder we take the resulting 256 dimensional latent vector and loop which vector from the codebook is its closest neighbor and replace it with that. I should mention that in practice this will be a bit more complex as that our encoder outputs not only one vector but multiple. And then each of these vectors will be replaced by their corresponding nearest neighbor. This just increases the number of possible latent vectors. Otherwise if we were to have 100 codebook vectors we could only learn 100 image compressions. However by letting our encoder output multiple latent vectors for each image we drastically increase the number of possible image compressions. Think of outputting 32 vectors instead of just one. This for each of the 32 vectors could be replaced by 100 different codebook vectors and this results in a pretty big number. So this is essentially all there is to say about the VQVAE and its benefits over the standard VAE. To make this work in practice though we need to add some tricks. For example to our loss. However since this also applies to our VQGAN I will come back to this later. The other method VQGAN is making use of is to convert our problem into the GAN regime by introducing a discriminator to our training. If you are not familiar with GANs let me quickly do a summary of how they work. GANs consist of two neural networks which compete against each other. One is the generator which as the name suggests generates our new samples in our case images. On the other hand we have the discriminator which acts as the detective to distinguish the real images contained in our data set from the fake ones that the generator created. This results into what is called a min max problem and simply means that the generator is trying to push the loss into one direction and the discriminator tries to push it into the opposite one. In other words the generator aims to fool the discriminator into thinking that his images are also coming from the real distribution and the discriminator tries to avoid this. This training procedure results into the generator becoming better and better at approximating the original data and has turned out to result in a huge line of research and amazing results. Coming back to the VQ GAN this is also exactly what is happening here. Our encoder converts our image into the latent space. Latent vectors will be replaced by its nearest neighbors from the codebook and the decoder takes this latent representation and converts it back to the image space as usual. Then we take some reconstruction laws and have our VQ VAE part ready. Now additionally we also have a discriminator in our training loop which takes in the reconstructed images and the original images and tries to distinguish between the real and fake ones. This provides some additional information to our encoder and decoder as to how they have to change according to fool the discriminator and hence mimic the original data distribution better. You can imagine this as the discriminator being an additional supervisor for our encoder and decoder which gives his opinion on the generated images and tells it via the loss and its gradient how to do better. With this setup we can now train our VQ GAN model to learn to reconstruct images by first them to our latent space and afterwards decoding it back to our image space. But we said that our main goal would be to generate completely new images from our data distribution and not just reconstruct images which we already have. And that's where the second stage of the VQ GAN starts. You see how every image is encoded to our latent space and after that it's replaced with the closest codebook vectors, right? Say we have a lot of images of flowers with a green background representing some leaves or so. We have a decoding process so the encoding of the image and the codebook mapping would learn this pattern and would assign specific codebook vectors for a green background. The same holds for other types of backgrounds, say sky or water or even the parts of a flower, for example the color or the shape. All these things are learned and are in some sense represented in our codebook. You can also imagine our codebook as a toolbox for constructing flowers. If we want to create a flower which has a red flower head, a long green stem, big green leaves and a blue background for our sky, we simply take all these things out of our toolbox and fuse them together to build our flower. And this is exactly what the transformer part is used for. It learns which codebook vectors could make sense together and which not and after that it can generate new possible codebook vectors which it thinks could go along together. We are using a transformer because our codebook vectors representing an image is just a sequence and transformers are really good at sequence tasks. Furthermore, I mentioned at the beginning of the video that you could create images which are way bigger than the original training images. The reason for that is both the encoder and the decoder are fully convolutional architectures and as a result are invariant to image sizes. Technically, only the decoder is used for this and the transformer will sample more latent vectors than usual in an autoregressive manner. Check out the video description for more details. And that's basically it. You have now learned the key idea behind VQGAN. Congratulations! In the following, I will get into a bit more details concerning the actual training and architecture of the models. However before that, if you are interested in a coding tutorial for actually implementing the VQGAN in Python using PyTorch, you can take a look at the video description or at the i in the top right corner to get to my explanation video for how to implement the VQGAN model in PyTorch. The VQGAN model consists of four submodels. They are the encoder, decoder, the codebook and the discriminator. The office uses simple CNN architectures for both the encoder and decoder. The encoder starts off with a normal convolutional layer and then adds M-residual blocks and downsample blocks. After another resonant block, a non-local block is added. If you have never heard about it, don't worry, it's just sort of an attention block, just with a different name. It was introduced by Facebook AI Research in 2018. I will link the paper below if you want to check it out. At the end, we add a group norm, a switch activation and another convolutional layer to map to our latent space. All in all, the encoder has about 22 million parameters. On the other hand, the decoder is essentially the same as the encoder, just in reverse. And obviously instead of downsample blocks, we use upsample blocks. However, it is also way bigger and has about 42 million parameters. This comes from adding more residual blocks and non-local blocks than the encoder. The discriminator they use is from the patchGAN paper. Other than usual discriminators, patchGAN focuses only on rough structures and classifies each end-by-end patch as real or fake. In general, it is also just a convolutional architecture consisting of blocks of convolutions, batch norm and a leaky relu. The codebook actually is not really a model, as it just consists of n codebook vectors and performs a mapping from any input vector to the closest vectors from the codebook. Internally, this is represented as an embedding matrix, which is also learned over the course of training. Make sure to check out my implementation video if you want to know how this is exactly done. The second stage only consists of a transformer, which is a GPT architecture. So a decoder only. Specifically, they use minGPT, which is just a tiny version of the actual GPT models by OpenAI. The first training stage is performed in an end-to-end fashion, meaning that everything is learned at the same time. Our loss function consists of the following two parts. The first loss is exactly the same as in the original VQVAE paper. It consists of a reconstruction loss and the two parts for the vector quantization. The reconstruction loss includes both a pixel-wise mean squared error, or L2 loss, and a perceptual loss. The second part of a perceptual loss is that you feed both the real image and the reconstructed image through a pre-trained network. In our case, the authors used a VGG16. And then you take the difference between these features. That way the model is supposed to learn not only to be precise on a pixel-wise manner, but also to focus on keeping semantic information of images. The last two terms that you see here are for the codebook mapping. SG stands for stop gradient and means not to calculate the gradient for this item. This however is just something relevant for the practical implementation, so don't worry if you don't understand it. You can also see that the two terms are the same except that the stop gradient is at a different position. The first term ensures that the codebook vectors will be changed more towards the encoder output, and the second term enforces that the encoder outputs vectors closer to the codebook. So you can imagine that both the encoder outputs and codebook vectors will be brought closer together in latent space, so that the replacing of the encoder output by the corresponding codebook vectors will not result in a huge change. The second loss is for the GAN part and is a normal GAN loss, where the discriminator tries to correctly classify real images from fake ones. In this exact formula, the discriminator will try to classify real images as 1 and fake images as 0, taking the logarithm of its simile and shows numerical stability. Fusing the two parts together results in the following loss. Lambda is a weighting factor which is computed in each iteration by the following formula. The transformer training can be formulated as the following. You encode an image in the latent space and map it to the codebook vectors. Then you randomly replace some of the codebook vectors by other random codebook vectors, and the transformer is supposed to recognize these wrong codebook vectors and correct them. So you feed the modified codebook vectors into the transformer and take the output and compare it to the original codebook vectors, the ones before the random replacement. The transformer is trained by optimizing the cross entropy loss between the unmodified codebook vectors and the modified ones. And that's it. You now understand the theory behind the popular VQGAN model. However, theory is not all and you should definitely go and check out the implementation video linked down below. And with that being said, I wish you a nice day.",
        "output": "### Introduction to VQGAN\nVQGAN, which stands for Vector Quantized Generative Adversarial Networks, is a powerful model for image generation that combines existing methods to produce high-resolution images. It is an extension of the VQVAE model and incorporates it into the GAN architecture. This model can generate images much larger than it was trained on.\n\n### Recap of VQVAE\nVQVAE is an abbreviation for Vector Quantized Variational Autoencoder. It is an improved autoencoder that focuses on learning a mapping (encoding) from the image space to a low-dimensional latent space and then reconstructing (decoding) the image back into image space. VQVAE addresses the problems of the original autoencoder by reparametrizing the latent space and turning the continuous latent space into a discrete one, which helps in the learning process.\n\n### Using GANs with VQVAE\nTo integrate VQVAE into the GAN architecture, the authors added a discriminator to the VQVAE. The generator, which is the decoder part of VQVAE, along with the discriminator, helps in learning the representations better by providing additional supervision during the training process.\n\n### Second Stage: Generating New Images\nAfter training the VQGAN, we can then generate completely new images from the data distribution using the learned codebooks and a transformer. The transformer learns the relationships between the codebook vectors and is able to sample new latent vectors to create new images.\n\n### Model Details: Encoder, Decoder, Codebook, and Discriminator\nThe VQGAN consists of four sub-models: the encoder, decoder, codebook, and discriminator. The encoder and decoder are simple CNN architectures, while the codebook is represented as an embedding matrix. The discriminator is based on the PatchGAN architecture.\n\n### Training VQGAN\nThe training process consists of two stages: an end-to-end training stage in which encoder, decoder, codebook, and discriminator are all learned simultaneously, and a transformer training stage in which the transformer is trained autoregressively to generate new images.\n\n### Loss Functions\nThe loss function for VQGAN is composed of two parts: the reconstruction loss from VQVAE, which includes a pixel-wise mean squared error, perceptual loss, and vector quantization terms, and the GAN loss, which involves the discriminator trying to correctly classify real and fake images."
    },
    {
        "instruction": "<|SYSTEM|>You are a scholars that is taking markdown notes about some video, you would be giving some written scripts for the task.\nYou have finished writing the notes, and you want to extract the title from the notes.\nThe title should only be the name of the technology or algorithm, no extra words needed.\nPlease write the notes in markdown format. No need to write \"Title:\" for the Title. Make sure the title is within 10 words.\nHere is an example:\n## Hidden Markov Model\n<|ASSISTANT|>## ",
        "input": "<|USER|>### Introduction to VQGAN\nVQGAN, which stands for Vector Quantized Generative Adversarial Networks, is a powerful model for image generation that combines existing methods to produce high-resolution images. It is an extension of the VQVAE model and incorporates it into the GAN architecture. This model can generate images much larger than it was trained on.\n\n### Recap of VQVAE\nVQVAE is an abbreviation for Vector Quantized Variational Autoencoder. It is an improved autoencoder that focuses on learning a mapping (encoding) from the image space to a low-dimensional latent space and then reconstructing (decoding) the image back into image space. VQVAE addresses the problems of the original autoencoder by reparametrizing the latent space and turning the continuous latent space into a discrete one, which helps in the learning process.\n\n### Using GANs with VQVAE\nTo integrate VQVAE into the GAN architecture, the authors added a discriminator to the VQVAE. The generator, which is the decoder part of VQVAE, along with the discriminator, helps in learning the representations better by providing additional supervision during the training process.\n\n### Second Stage: Generating New Images\nAfter training the VQGAN, we can then generate completely new images from the data distribution using the learned codebooks and a transformer. The transformer learns the relationships between the codebook vectors and is able to sample new latent vectors to create new images.\n\n### Model Details: Encoder, Decoder, Codebook, and Discriminator\nThe VQGAN consists of four sub-models: the encoder, decoder, codebook, and discriminator. The encoder and decoder are simple CNN architectures, while the codebook is represented as an embedding matrix. The discriminator is based on the PatchGAN architecture.\n\n### Training VQGAN\nThe training process consists of two stages: an end-to-end training stage in which encoder, decoder, codebook, and discriminator are all learned simultaneously, and a transformer training stage in which the transformer is trained autoregressively to generate new images.\n\n### Loss Functions\nThe loss function for VQGAN is composed of two parts: the reconstruction loss from VQVAE, which includes a pixel-wise mean squared error, perceptual loss, and vector quantization terms, and the GAN loss, which involves the discriminator trying to correctly classify real and fake images.",
        "output": "## VQGAN"
    },
    {
        "instruction": "<|SYSTEM|>You are a scholars that is taking markdown notes about some video with Wiki style, you would be giving some written scripts for the task.\nAnd summarize each paragraph with a title.\nRemember to keep the technology or algorithm name, very important.\nPlease write the notes in markdown format.\nHere is an example:\n### Definiation\nA deadlock is a situation in which two computer programs sharing the same resource are effectively preventing each other from accessing the resource, resulting in both programs ceasing to function. \nThe earliest computer operating systems ran only one program at a time.\n\n### What it have to do with Relationsal Database ManagementSystem\nIn a database, a deadlock is an unwanted situation in which two or more transactions are waiting indefinitely for one another to give up locks. \nDeadlock is said to be one of the most feared complications in DBMS as it brings the whole system to a Halt. \nWhen you tries to insert or update data on database, sometime your record need to depend on the row record.  \n<|ASSISTANT|>#### ",
        "input": "<|USER|> We introduce Structured 3D Features or S3Fs, a novel feature representation for 3D human reconstruction. S3Fs allow a single model to perform a number of tasks. From a single image, our model performs 3D reconstruction, including albedo and shading, novel view synthesis, 3D human relighting, avatar animation and even 3D virtual try-on. This is all done without post-processing using just one monocular input image. Let's see how our model works. Starting from a single image, we first extract image features using the Feature Extractor Network chip. This is similar to pixel-aligned features used by previous methods. However, there is no straightforward way to manipulate them for downstream tasks, such as reposing. In our pipeline, we first estimate 3D body pose and shape in the form of a parametric body model. We then sample 3D points on the body surface and pull per point features from the image feature map. The points only roughly approximate the true 3D geometry, so we let the network move them freely to discover optimal 3D feature locations. Finally, we pull new features based on the updated locations, obtaining our proposed structured 3D features. The final points better cover the full body geometry, including hair or loose clothing. Further, and in contrast to previous representations, S3Fs carry over semantic information from the body model and can be manipulated in 3D. This is key for various applications including animation or 3D virtual try-on. The structure of the scene is implicitly represented in the S3Fs and can be extracted using marching cubes. Given an input point, we aggregate S3Fs using a transformer architecture, and obtain its sign distance and albedo. We model scene illumination by predicting a latent illumination code from the bottleneck of the feature extractor network, allowing us to illuminate the 3D reconstruction. Here, we show our 3D reconstructions next to the images used as input. We compare this approach to ICON, shown in the bottom right. ICON is the previous state-of-the-art method in the task of monocular 3D human reconstruction, and can also take unconstrained body poses. Thanks to our pulling of features and body point displacement, our approach is able to cover loose clothing better and is more robust to challenging body poses, while not only recovering geometry, but also albedo and shading. We next focus on our results on color estimation, and here show our shaded reconstructions. We compare our approach against FORUM, a recent method that also performs monocular 3D human reconstruction, recovering albedo and shading. Our method leads to more accurate shading estimations that are more consistent with the input images, and is not significantly affected when tackling more challenging body poses or images in the wild. The estimation of albedo and the composition of the scene illumination enables relighting our reconstructions. This is important for 3D human compositing applications and realistically placing 3D humans in novel scenes. Here we show different examples being relighted consistently with a common scene illumination. The proposed representation relies on 3D points with semantic information, enabling different kinds of applications. For instance, we here repose the points and their features to new body poses using linear blend skinning, leading to avatar animation. The structured 3D features carry semantic information and enable some tasks like virtual try-on. For example, for a given input image that we want to edit, we first extract S3Fs. We do the same for a source image that might contain the target clothing ending up with a pair of structured 3D features. Now, given the segmentation mask of a target piece of clothing we want to edit, like upper body clothing in this case, we can easily find the target features that belong to it and combine the S3Fs from both images. By doing this, we can obtain the edited reconstruction showing the original person wearing the source clothing. We now show results on virtual try-on of upper body clothing by interpolating S3Fs smoothly between each garment. This leads to smooth cloth texture interpolations even between examples that have significant differences of style, viewpoint or body poses and shapes. Note that this process takes only a few seconds to complete. Note that this process takes only a single image of the target person and another single image of the piece of clothing and requires no additional post-processing. Moreover, each piece of clothing is related with the illumination of the person seen as shown more clearly in the bottom left example. Thanks for watching.",
        "output": "### Introduction to Structured 3D Features (S3Fs)\nStructured 3D Features (S3Fs) is a novel feature representation for 3D human reconstruction that enables a single model to perform various tasks such as 3D reconstruction, novel view synthesis, 3D human relighting, avatar animation, and 3D virtual try-on using just one monocular input image.\n\n### S3Fs Pipeline\nThe pipeline starts with extracting image features using the Feature Extractor Network, estimating 3D body pose and shape, sampling 3D points on the body surface, and pulling per point features from the image feature map. The network then moves the points freely to discover optimal 3D feature locations, resulting in structured 3D features that better cover the full body geometry.\n\n### 3D Manipulation and Applications\nS3Fs carry over semantic information from the body model and can be manipulated in 3D, enabling various applications such as animation and 3D virtual try-on. Using a transformer architecture, they can obtain sign distance and albedo, as well as model scene illumination for realistic lighting effects.\n\n### Comparison to Previous Methods\nS3Fs outperform previous state-of-the-art methods, such as ICON and FORUM, in monocular 3D human reconstruction by providing better coverage of loose clothing and more robust performance under challenging body poses. Additionally, they offer more accurate shading estimations that are consistent with input images.\n\n### Relighting and Avatar Animation\nThe proposed representation enables relighting of 3D reconstructions for more realistic placement in novel scenes and avatar animation using linear blend skinning.\n\n### 3D Virtual Try-On\nS3Fs enable virtual try-on by interpolating structured 3D features between target clothing and source images, resulting in smooth cloth texture interpolations even when there are significant differences in style, viewpoint, or body poses and shapes. This process requires only one image of the target person and one image of the piece of clothing, with no additional post-processing needed."
    },
    {
        "instruction": "<|SYSTEM|>You are a scholars that is taking markdown notes about some video, you would be giving some written scripts for the task.\nYou have finished writing the notes, and you want to extract the title from the notes.\nThe title should only be the name of the technology or algorithm, no extra words needed.\nPlease write the notes in markdown format. No need to write \"Title:\" for the Title. Make sure the title is within 10 words.\nHere is an example:\n## Hidden Markov Model\n<|ASSISTANT|>## ",
        "input": "<|USER|>### Introduction to Structured 3D Features (S3Fs)\nStructured 3D Features (S3Fs) is a novel feature representation for 3D human reconstruction that enables a single model to perform various tasks such as 3D reconstruction, novel view synthesis, 3D human relighting, avatar animation, and 3D virtual try-on using just one monocular input image.\n\n### S3Fs Pipeline\nThe pipeline starts with extracting image features using the Feature Extractor Network, estimating 3D body pose and shape, sampling 3D points on the body surface, and pulling per point features from the image feature map. The network then moves the points freely to discover optimal 3D feature locations, resulting in structured 3D features that better cover the full body geometry.\n\n### 3D Manipulation and Applications\nS3Fs carry over semantic information from the body model and can be manipulated in 3D, enabling various applications such as animation and 3D virtual try-on. Using a transformer architecture, they can obtain sign distance and albedo, as well as model scene illumination for realistic lighting effects.\n\n### Comparison to Previous Methods\nS3Fs outperform previous state-of-the-art methods, such as ICON and FORUM, in monocular 3D human reconstruction by providing better coverage of loose clothing and more robust performance under challenging body poses. Additionally, they offer more accurate shading estimations that are consistent with input images.\n\n### Relighting and Avatar Animation\nThe proposed representation enables relighting of 3D reconstructions for more realistic placement in novel scenes and avatar animation using linear blend skinning.\n\n### 3D Virtual Try-On\nS3Fs enable virtual try-on by interpolating structured 3D features between target clothing and source images, resulting in smooth cloth texture interpolations even when there are significant differences in style, viewpoint, or body poses and shapes. This process requires only one image of the target person and one image of the piece of clothing, with no additional post-processing needed.",
        "output": "## Structured 3D Features (S3Fs)"
    },
    {
        "instruction": "<|SYSTEM|>You are a scholars that is taking markdown notes about some video with Wiki style, you would be giving some written scripts for the task.\nAnd summarize each paragraph with a title.\nRemember to keep the technology or algorithm name, very important.\nPlease write the notes in markdown format.\nHere is an example:\n### Definiation\nA deadlock is a situation in which two computer programs sharing the same resource are effectively preventing each other from accessing the resource, resulting in both programs ceasing to function. \nThe earliest computer operating systems ran only one program at a time.\n\n### What it have to do with Relationsal Database ManagementSystem\nIn a database, a deadlock is an unwanted situation in which two or more transactions are waiting indefinitely for one another to give up locks. \nDeadlock is said to be one of the most feared complications in DBMS as it brings the whole system to a Halt. \nWhen you tries to insert or update data on database, sometime your record need to depend on the row record.  \n<|ASSISTANT|>#### ",
        "input": "<|USER|> Hi, Recently chatgpt has taken the world by storm. In this video, I will present a walkthrough tutorial on building a GPT-like model. By the end, we will generate some poems about cats. We will also discuss some new concepts beyond GPT. I believe everyone should learn about neural networks because it has relation to many fields. I am interested as a neuroscientist because it gives me more insight into how AI and the brain can inspire each other. I will try my best to avoid this and provide a gradual transition between the concepts. I assume zero knowledge in machine learning. If you know some programming, you can follow along. If not, you can still learn from the illustrations and the analogies. Let's start. Open your Python interpreter. If you don't have one, I suggest downloading Anaconda. After installing, open Anaconda prompt and type spider. The IDE opens. On the left side, type print hello world. Press the green button above to run it. If you see hello world on the right side, we are ready to code. You can also select each line, right click and run the selection. Intelligence is about predicting the outcomes. Let's take this simple example. Your brain tries to associate the switches with the lights after observing the events. One way to model that is using the good old fashioned AI. We define a button and then model the conditional events using if-else statements. But this can quickly get messy as the number of the conditions increases. Also, there is no obvious way to make this model learn from the data. Another approach is Perceptron. For simplicity, let's do one light for now. Change off to zero and on to one. Here we use the weighted sum of the events, then threshold the result and use it to predict the outputs. But different buttons have different relations with the lights. We can add the relations to our model too. All we need now to predict the output is to model the relations correctly. Now let's simplify the variable names. X's are our inputs, W's are weights or relations and the result is Y. This can still get tedious if we add more inputs. NumPy can help. So import NumPy as MP. Now you can put all the inputs into X's array. Put all the weights into W's array. Now you can simply type Y equal X's dot W's. The add sign here is the dot product, which calculates the weighted sum of all the inputs. Just like below, but in a much more compact way. Now we simplified this, so remove it. In real life, the relations are hidden from us. Comment the weights out. We can only observe the inputs and their outcomes. To predict the outcome, we need to figure out the relations. This is what learning is about in machine learning. But to figure out the weights, we need more than one event to observe. Let's observe a few more inputs and outputs. Now that's enough to figure out the relations between these inputs and outputs. If we figure out the weights, I mean the relations, we can predict the output for new inputs, like this input or this one. Delete this. We have now an optimization problem to solve. The number of inputs is 5, the number of outputs is 1. To find the correct weights, we need to search for the right combination. You can think of the solution as a multidimensional point. Searching here is not different from geographical searching, in which your search space is only two dimensions. One approach is random search. Let's define the initial weight as a random set of numbers. W's equal to weights, with the size of the inputs by outputs. Let's print our random guess. The correct weights are hidden, so we don't know if our guess is correct or not. We need a feedback. We can get a global feedback by checking the predictions from our guess and then comparing the output yh with the expected output ys. The sum of the absolute errors can be used as the global feedback. If it reached the threshold below 0.05, then we found the solution. Else, we make another random guess. Let's try random guessing for 5000 iterations. Let's record the outcomes in this array. To plot the results, we need to type this and import matplotlib library like this. After reading the simulation, you can see we haven't found the solution, and none of our 5000 iterations came close to our threshold of 0.05. The minimum error is 0.8. This brute force search becomes worse with more dimensions and more variables. So what can we do to find the needle in the haystack? A simple approach comes from evolution. Here we record the current error. The father makes a child. The child's location is slightly and randomly mutated. The child's error is assessed. If the child's error is more than the current error, then the child is removed. The father makes another child. We repeat the process. If the child's error is less than the current error, now the father can rest in peace. The child grows to become a father. The error is updated. If you repeat that, one day you reach the goal. Let's implement this. Define mutation as a small random set of weights. Copy the original weights and rename them as cw for child weights. Rename e to ce for child error. Compare the child error with the original error, and select the better weights for the next iteration. Let's run. The error is reducing but stuck right above our threshold. Let's reduce the mutation amount. Now it found the solution. Let's break the loop. The last error is now below 0.05, and the solution yh is close to the true solution ys. The guessed weights are also close to the true weights, which were hidden from us. Now let's make an arbitrary output where we don't know if we can even find the solution with a linear regression. Let's rerun. No solution is found, and the last error is high. Let's add a bias term because our data are not centered on 0. Bias terms are additional ones added at the end of the inputs, and they are necessary to model the shifts in the data. The error is still above the threshold. The reason is our simple network can only find linear relationships between the inputs and the outputs, but if the relation is non-linear, then we can't find a solution, because we cannot fit a line to a curve, no matter what slope or shift we try. To solve this, we need to add two things. First, another layer of weights connected to some middle nodes. Second, apply an unlinear activation function to the middle neurons. You can use any activation function from this list, but I prefer sine waves. We know from Fourier transform that we can approximate any signal by adding together different sine waves. Let's implement that. Add wi, which takes the inputs to the nodes. Now ws are getting their input from the nodes. Define 15 nodes. Dot product the axis with the wi. Then apply the sine wave to generate non-linearity. Let's add that to the child network too. But we need to change this to nodes. Let's rerun. Oh, we get an error because we accidentally changed the initial input axis. Let's rename all the intermediary results to x and only keep the initial inputs as xs. Rerun. Yes, we found the solution again, and the error is below the threshold. Let's reduce the number of nodes to 5 and rerun. Now no solution is found. It looks like we need more than 5 nodes to solve this problem within 5000 iterations. This is a simple problem, and it can be solved with fewer parameters if we have a better search for a more optimal solution. We can use parallel computing to find better solutions faster. For example, the father can make several children at once, and we select the best ones to breed, and they too can make more children in parallel. This way we can reach the solution faster. However, this requires sacrificing a lot of children, and that's horrible and immoral. Shameless DNA has been doing this for billion years. Now we live in the age of brain. Individual lives matter, so we need to find a better way. First, take advantage of the error magnitude, which is known to us. Second, even though it's dark and you don't know the direction towards the global maxima, but you can follow the steepest path from your immediate surroundings. Use derivatives to infer the direction of the steepest slope. From calculus, we know that derivative of x is 1, the derivative of x square is 2x, the derivative of sine is cosine, and so on. Apply the derivative to the error, and update the weights by the error. Some nodes have higher activities and contribute more to the error, so it makes sense to scale the update amount for each weight by x. One important note, the magnitude of the error doesn't tell how far we are from the solution, but it tells how steep your direction is. That's why you may overshoot the solution and the error plot will look noisy. Therefore, you need to scale down the gradients by another factor. It will take longer to reach the solution, but at least it won't overshoot the target. Let's implement that. Update the weights like this. The derivative of the output is just 1, because we haven't applied nonlinearity to the last output. Ok, now we know to learn from our mistakes. We don't need to sacrifice children, so delete this part. We rerun the code. We found the solution in just 300 iterations. Let's remove the break and run for the whole 5000 iteration. The error rate is almost reaching 0 after 5000 iteration. Cool. Let's see if we can find the solution with 5 nodes. Yes, the error is below 0.05. Let's see if we can find the solution with 2 nodes. Unfortunately we can't. The error stuck around 6. Because you follow the steepest direction in the immediate surroundings, you may get stuck on a nearby local maximum. That's why more nodes can help. More nodes means more dimensions. If you follow the local evidence in more dimensions, you may find a solution eventually. But we can do better. So far we only fine-tuned the outer layer. If we fine-tuned the inner layer too, we may capture the hierarchical structures and be able to model with fewer parameters. For example, you need at least 15 parameters to model 5 triple letters. However, if you add hierarchy, you can share the common pair with all the other letters. This can reduce the number of the parameters dramatically, especially when modeling deeply nested structures. Fine-tuning the inner layer is similar to that of the outer layer. Except now we need to propagate the errors backward. Rename the weights with indices to indicate the layer number. Cosine x0 to the initial input and rename all the intermediary variables appropriately. To backpropagate the error, just like the forward pass, you do the dot product between the error and the weights, and multiply by the cosine of the previous output, because cosine is the derivative of sine. Rename these scaling factors to lr for the learning rate, so that we can tune it as a hyperparameter. Now re-write the code. It found the solution even with two nodes, and that's awesome. Let's rearrange the code like this. If we add more layers, it will become obvious what's happening. Whenever you add new layers, the input of the previous layer will become the output of the next layer. Adding a new layer is just as simple as copying and pasting with proper indexing of the variables. You can see a clear pattern. Starting with the input, you do the weighted sum of the activations using the adult product followed by a non-linear activation, like the sine function. Repeat the process until the last layer. Then compare the output yh with the target ys. The mismatch is the error that's propagated backward in a similar way. You do the weighted sum of the error and then multiply it by the cosine of x. Finally, use the forward activations and the backward errors to update the weights in each layer. There are four main parts. The part where you define the structure of your neural net, the forward pass where the inputs are propagated through the network, the backward pass where the errors are back propagated, and the update part where the weights are updated to minimize the error. So the entire code can fit into a page. Let's rerun. As you can see, the error is almost approaching zero. Let's try other datasets. Let's try XOR problem. XOR has two inputs, so reduce the ins to 2. It solves the XOR problem perfectly. Let's try fitting some other functions. Define xs as a small range of even numbers from minus 10 to plus 10. Define the output as an arbitrary linear function of xs. Change the input size to 1. Let's increase the number of the nodes and reduce the learning rate. Rerun the code. The error plot is noisy. Let's reduce the learning rate more. Looks like a great fit. Let's try another function. It fits that too. It looks like we can fit any linear function now. Let's try an unlinear function. Ok now we are back to non-fitting and noisy error plots. We need to reduce the learning rate further. That looks better. Let's increase the learning rate slightly. We still underfitting. More nodes may be necessary. Let's use 100 nodes. Again noisy error plot. We need to reduce the learning rate. We may need to increase the number of the iterations. Finally a good fit. Ok now it's obvious we can fit any data if we tweak the number of the nodes, the learning rate and the iterations. Even though a multilayer neural network is a very powerful modeling tool. However, in its naive form, it comes with many side effects which needs to be addressed before you can use in any real world application. First, trying to solve simple linear problems using multilayer neural networks is like trying to kill a fly with a tank. It may do the job, but it's very messy and inefficient. Second, backpropagation is not bulletproof, especially for deep networks. Imagine several people collaborating to score a goal, but they make errors. You need to blame the errors on each player correctly starting from the last player back to the first one. Two problems may occur here. Either you blame too much error on the early players and by the time you reach the first layer you have nothing to blame. So the early players have nothing to learn. This is called the vanishing gradient problem or the opposite. You are conservative in blaming the errors and by the time you reach the early layers all the remaining errors are blamed on them. This is called exploding gradients, where the gradients become uncontrollably large. Mathematically speaking, the two problems happen when repeatedly multiplying values smaller than one or higher than one. Perfectly balancing this is quite challenging and you can't achieve it without using some extra tricks and tools. Here is some advice. If you have a simple linear relationship, then linear regression with a simple perceptron is more than enough. If you have an unlinear but a shallow problem, then a simple neural network with one hidden layer is probably enough for the job. If you have complex nested structures, then we need a deep neural network, but we also need some extra tricks to make it work. Talking about the extra tricks, sometimes you have recurrent and sideways connections between the layers as well as various mathematical operations. You need to be an expert in math and programming to properly propagate the error in such networks. Even then you cannot avoid countless hidden bugs. Therefore do not reinvent every single wheel. Now you understand the basic principles, it's a good time to add a deep learning tool for our next task. Close your spider IDE, go to the PyTorch website. Here choose conda, if you have a good GPU, choose this, otherwise choose CPU and copy this line. Open anaconda prompt again and paste the line here. Once all the installations are complete, reopen the prompt and restart spider IDE. Now you can import torch. From torch.nn import functional as if. To be compatible with PyTorch, do the following changes. Change every MP to torch. Set ws to ws require grad to true. To compute the error, set loss equal to mean squared loss which takes yh and ys as arguments. Initialize the optimizer with zero gradients. Define the optimizer above as follows. It takes a list of the weights and the learning rate as an argument. Here type loss.backward. Now you don't need this part because loss.backward takes care of that. Type optimizer.step. Now you don't need the update part because the optimizer takes care of that. One more thing is you need to change the numpy arrays to tensors so that they are compatible with PyTorch. Let's print the error every 500 iterations. Now re-run the code. Reduce the learning rate. Manually changing the learning rate is very tedious. It would be awesome if we have an adaptable learning rate. Fortunately there is. Change the optimizer type to Adam and increase the learning rate to 0.003. Now the error plot is smooth but slightly underfits. Let's increase the nodes to 200. We have a great fit. We even didn't need to change the learning rate after increasing the number of the nodes. That's because the optimizer now takes care of the learning rate to some degree. It adaptively changes the step size. For example, up to this point the optimizer makes small steps because the past steps change its direction frequently. Therefore the optimizer was not confident. But here the optimizer makes large steps because its past steps are in the same direction. So the optimizer is more confident to make larger leaps. Amazingly we can do that in the dark just by relying on the global score and our past steps. In programming it's a good practice to encapsulate the commonly used parts into their classes. Define class model, move the weights into the class and add self keyword so that they become part of the class. Define a forward function and move the forward pass to that function. We can simplify the variable names because now the optimizer takes care of all the trackings. Define the model here. Change the list of weights to params and define it as an empty list above. Then add the weights to the list once they are generated. Now we can use the model here. It takes the inputs and it spits out the output. Everything looks neat now. This part defines the structure of our network and this part takes care of the training. One important note before we generate cat poetry. Let's reexamine how neural networks learn a function like the square rule. You can see it fits the training data very well with error rates approaching zero. Now let's test it on unseen data. To test the model define a value and change it to a tensor. Pass the value to the model and print the results. You also need to append 1 for the bias. Let's try 4. We get a value close to 16 which is correct. Now let's try minus 5 which is a new value not seen by our model. The result is minus 3.7 which is not even close. So what's happening here? Didn't we fit the training data perfectly and get error rates close to zero? Well we only did fit even numbers in our small training set. You know these oversized networks have a lot of degrees of freedom. They can fit the training data however they want. One way to reduce the crazy fit is to regularize the network. A very simple solution here is just reducing the initial weights by multiplying them by 0.1. Now let's rerun. Again, a good fit for the training data, but let's see how it generalized to unseen data now. Let's try 5. We get 24.5. Not perfect but much better than before. When the initial weights were large, the output was complex and it was possible for backprop to find crazy solutions just by fine-tuning one or few impactful weights. However with smaller weights, no weight alone has a big impact and all the weights have to be fine-tuned and collaborate to fit the data. Another improvement can be achieved by changing the sine function to relu. Relu fits the curve with small straight lines and that makes it behave less erratically. So now with minus 5 we get 25.1. Let's try 8. It's a perfect 64. This is expected because 8 is already in the training set. For 9 we get 82.4. Still not perfect but we are getting better interpolation. Here is the real challenge. Let's try 100, which is very far from our training set. Unfortunately the result is also very far from the target. In light of these results, let's revise our conclusion. Neural networks can fit any data, provided that we have enough nodes in the hidden layer. It can interpolate fairly well. There is a condition here. It can't extrapolate far beyond the data distribution. To be honest, humans are also bad in extrapolation. To make your neural network good with interpolation, do this. Add more data and train the network for longer times. Finally add more constraints, like regularizers or other tricks. Honestly there is no hard rule here. Sometimes you have a great idea but it won't work, and sometimes you have an insane idea but it works. In short, you have to mess around a lot to find out. Many useful applications can be built with neural networks. For instance, image detectors, medical applications and translators. It can be used to model mapping between any inputs and outputs. Another useful application is autoregression. Here you divide the sequential data into past and future. Once you train the network on the past context to predict the future, you end up having a generative AI. You can do that for text, audio, image or any other sequential data. Let's take language as an example. You can train a network to predict the next letter in a sentence. The predicted letter will become part of the next context, this way you can generate sentences. Let's implement that. Open a text file using this python code. Or you can copy and paste your text between these triple quotes. I'm going to use this simple poem about cats for the training. The text is just over 3000 letters, which is considered very small, but let's keep it simple for educational purposes. First, change to lowercase to reduce the vocabulary size. Get the set of letters from the text. The neural net only understand numbers, therefore we need to assign a number to each letter and record that in a dictionary. Use the dictionary to map all the letters to their corresponding number. Now we have successfully converted the text to numbers. The vocabulary size is 48 unique letters. Let's move these settings here. Let's set the input size to 5 and keep the output as 1. This means we use 5 letters as input and the letter after that will be used as the output. Convert the data to tensor floats. X's are the stack of letters from i up to the input size of the next letters. Y's are the stack of letters that comes after the last input letter. Let's check X's and Y's. So all we did so far is to prepare the text data for training. We feed these arrays to our model and we train the model to predict the corresponding outputs. Delete the bias part because we will center our data later on. Now remove plus one here because we no longer have the bias term. Now let's run the entire script. This will take some time. You can see the error plot is reducing. Let's examine the fit. We are using a context of 5 letters so the fit is expected not to be perfect. Let's increase the context size to 16 letters. This will take ages to finish. It's a common practice to divide your dataset into smaller batches. So let's do that. Sample 100 trials randomly from the data and repeat for each iteration. Therefore move this part to the beginning of the training loop. Rerun the code. Let's zoom in to see how well the model fits the data. We see negative values in the output. That's because the network produces a floating value for the prediction. This can be converted back to a letter. We need to tell our network that we are dealing with categories in our output. To do so change the y's here to a long format. Change this to cross entropy which is suitable for categorical classifications. We need to change the output size to vocab size. Make these necessary changes in the code. Finally, set the output size to vocab size. So basically we do the forward and backward passes just like we explained before. To plot correctly we select the maximum probability and use that as our predicted value. Let's fix that typo here and rerun. We have a better fit now. We can now easily convert the predicted indices back to letters. Let's do that. Set s to be the initial context. Feed s to our model. The model produces a vector of probabilities. In this case the highest probability is for index number 15 which I think corresponds to the letter n. Here is an interesting trick. If we multiply yh by a factor less than 1, the higher probability outputs are suppressed more. This gives more chances for lower probability letters to be selected. We will come back to this useful trick later. For now let's select the maximum probability. Let's repeat that 3000 times. Each time we predict the next letter, we will roll the context s by one step and replace the last letter with the predicted one. Define gentext as an empty string and append the predicted letters to it. We also need to convert the letter indices back to alphabets. So we will define another dictionary, i to s, to reverse the indices back to the alphabets. Let's rerun the code. Finally we print the generated text. Now you can see it spits out the memorized text. Now instead of selecting the maximum probability, sample the letters according to their probabilities. We see newly generated words and phrases. However the overall structure doesn't look like a poem. That's because we use a small context of 16 letters which is not enough to model the entire sentences. Let's increase the context size to 64. Let's rerun. The model spits out a memorized text and by the time it reaches the phrase always free, which is the end of our poem, it continues with meaningless phrases. As I said, I do not expect high quality generation from such a small dataset. However, I do expect that a well trained model should be able to produce new phrases that looks like variations of the original text. The current network is very wasteful. For example, to learn the word cat, at least one of the nodes needs to activate whenever it encounters that word. The weights need to be adjusted properly so that the node can detect the word cat. The problem is this node is position dependent and it will not activate for any other cat words in other places. Hence other nodes need to be recruited to learn about cats in other places in the sentence. To solve that convolution can help. The relation among the letters are localized. We can use a filter that adjusts the adjacent weights and move it across our context. Once the filter learns to recognize a pattern, such as the word cat, it's position invariant and can recognize the word anywhere in the context. Different filters can be combined in the higher layers to produce phrases. For instance, this node can learn to detect the phrase cats fun, however, fun cats can't be detected. So, we need to recruit another node for that. Generally, we want a network that's invariant to position, rotation and scaling. The more invariant our network, the more generalizable it becomes. Let's imagine a simple filter that takes one input at a time and moves across the entire context. If we sum together the outputs from this filter, we will get a result that's invariant to both position and permutation. However, the capacity of this filter will be too small to learn and memorize any pattern. To solve that, we can represent each letter with a unique vector instead of representing it with a single value. Now, if we sum the embeddings for each letter, we will get a contextualized vector that's invariant to the position and permutation of the letters. So, putting this all together, you embed each letter into its corresponding vector, then pass the embedding through a simple linear network and record the output, then move the filter and do the same for the next letter in the context. Then sum all the outputs and pass the result through a non-linear neural network. The output of that network will be used to predict the next letter. Hopefully now this network can learn to recognize patterns regardless of their position. Let's implement that. Let's set the embedding size to 64, and let's associate each vocabulary with a random vector. X now passes through embedding, therefore we need to change the ins to n embed here. Change the data to long format because now we are using the data as indices for our embedding table. Now we use the fetched embedding and dot product with the weights of our filter. Assign the filter weights as wv which takes a vector of size of n embed and outputs a vector of the same size. Then we sum the output of the convolution across the input dimension before feeding it to the non-linear layers of our neural network. Let's rerun. The output is garbage because it spits out letters without caring about their position. We need to add the position information too. You can be clever about it, but a simple way is to mark each position in our context with a random vector, then multiply it with the embedding. Ok now we get better results. Let's put the filter into its class. Call the class head and move this part to the class just like before. Now we can easily reuse the class here by defining self dot heads and pass the input x to the forward pass of the head. You can easily make more filter heads. Let's make an array of four heads. Then we need to concatenate the output from all four heads and this will increase the embedding size by 4. To keep the output from expanding we need to divide it here by the number of heads. The reason for having more filter heads is to increase the capacity of our network so that it can detect various words and word connections. You may say four heads will not be enough to hold on all the possible patterns. Well, the trick is we are using distributed representation. It was only for illustration purposes we represented each word with one note. Despite learning the training data well, our model is still bad at generalization. One important reason is that we are using the longest context to predict the next letter. Predictions rely on the context length too. If the model has to predict what comes after the letter s here, then any of these letters are reasonable. However, if the pass context was the word cat, then a more reasonable prediction is the space bar. To make our network aware of various context lengths, we need to pass the inputs with various context lengths along with their corresponding outputs. Surprisingly this is easy to do. Change this part from i plus ins to i plus one. Now instead of summing the longest context, we need to sum the inputs of the smaller contexts too and feed them separately to our nonlinear network. So now the network learns what comes after each context length. One easy trick to do that efficiently is by doing the dot product between the input and a matrix of the same size filled with ones except the upper line masked with zeros. Now you can implement that like this. Make a matrix of ones with input size, then drill the upper right half. A good practice is to change the matrix values to probabilities by applying softmax function to it. However, before applying softmax, you need to mask the zeros with large negative values so that they turn back to zeros after softmaxing. Finally, you dot product the resulting matrix with the input x. Now after making our network aware of various context lengths, we are still going to use the longest context for our prediction. So here at negative one to fetch the last context. We still can't model long term dependencies. It's easy to model what comes right after t, but it's very difficult to model what comes after 10 steps because it depends on the context. Learning to form a proper context from the inputs is a challenging task. Since we are summing the inputs directly, backpropagation method will share the error equally over all the inputs. But that's not good for learning. Inputs need to be penalized appropriately. One of the old solutions comes from recurrent neural network. In this case, we use another set of weights to share the inputs from the past to the present in a recurrent way. These weights learn to keep important letters in their internal state. To train this network, we need to backpropagate through time. But remember we talked about the vanishing gradient problem. Well, in this case it's even worse because we need to backpropagate through many many steps. An interesting solution to vanishing gradient comes from LSTMs. The basic idea behind the LSTMs is to gate the inputs with a separate network that learns to re-weight the inputs according to their significance. The idea is we can reduce the number of steps by attenuating the non-significant parts. Hence, we can pay attention to more important parts. Talking about attention, if every player pays attention to other players, then they can learn to collaborate better and minimize their error. The basic idea is we need to transform these matrix to attention matrix, where the values indicates how much attention each input needs to pay to its neighbors. Then we re-weight the inputs accordingly. This network structure was the state of the art for language modeling up until 2017. However, it was computationally intensive and it was very hard to train in parallel because you cannot compute this part until you are done with the earlier step because its inputs rely on the output from the previous step. One group paid more attention to the attention part. They removed all the recurrent parts and they left only the attention part. And it worked. This was important step because now we can train this in parallel and take advantage of GPU to scale these networks up to 100 billion parameters. Ok now let's implement the attention part because that's all we need. First we need to produce the attention matrix. We already had the matrix of ones. To produce the attention matrix, we use the dot product between the input vectors with itself and use the similarity scores as our attention matrix. Let's implement that. Remove the part that produces the matrix of ones. Do the dot product between the input vectors with themselves but transpose it across the input dimension. Then normalize it. Now call that attention and pass it like this. Generally this X called Q for query and this one called K for the key, so rename them accordingly. To have final control over the attention scores, we need to pass the query and the key inputs through some learnable weights. Let's define these weights like this. For consistency with the literature, let's call this XV for value. Now the self attention is complete. One final important thing is that you need to change the multiplication to addition here for position embedding to work with attention properly. Let's rerun. As you can see now we get better results because now instead of adding the input vectors together in a dummy way, we have the attention scores which are used to re-weight the inputs appropriately before adding them together. Now you can see better phrases and better looking structure. This is just one block of attention. You can generally stack these blocks on top of each other to form a multilayered transformer. Let's move the attention part to class block. Now we can reuse the class. Let's define an array of three attention blocks. And let's stack them on top of each other. As we have mentioned before, learning becomes difficult with more layers because of the vanishing gradients. Another easy trick to mitigate this is to use residual connections. Think of the residual connections as convolutions across the layers. Just like convolutions share weight across the input dimension, the residual connections share input across layers allowing the learned patterns from the lower layers to be reused in the upper layers. To implement that, just re-add the input x across all the blocks. Ok, we get an error because we need to change the output of each block to embed size so that they match together when they are stacked on top of each other. Let's fix this typo here and rerun. This will take a while so grab a coffee. Ok, now we get a poem on cats that looks like the original poem. Overfitting happens here because we have a very small corpus and a relatively large network so a lot of the phrases are just regurgitated but there are some new made up phrases too. So we transformed this rigid neural network that only learns from one context and is not invariant to position or permutation into a more flexible network that learns from contexts of different lengths and is invariant to position and permutation. This is not a standard implementation of GPT because we are still missing some details. For a more standard implementation refer to Carpathi's awesome video. However, I would like to continue this with some alternative ideas and direction. I have been thinking a lot about where is the magic in self attention mechanism because the end result is just a matrix of values ranging from 0s to 1s which we use them to reweight the inputs for each layer. So I tried to reproduce the reweight matrix directly without the self attention mechanism using learnable lateral connection. In essence we are modulating each input by lateral connections from other inputs. You can think of this method as LSTM on steroids because now we are gating all the past inputs using separate gates. Let's implement that. Delete the self attention part. We are going to define lateral connections among all inputs and call them wr. Now we use this to generate the attention matrix which now we call a reweight matrix and let's pass that instead of attention and let's rerun the code. In my experience this method learns very well but it over fits the data if we have the same number of parameters as the self attention mechanism. So this method may benefit from more regularization or we can simply make the model sizes smaller. Let's reduce the temperature by multiplying yh with 0.8. As we said before this will allow us to generate more inventive text. Here it puts e at the end of storm to rhyme with the other sentence, basically reinterpolating as we have discussed. If you reduce the temperature further you may get more inventive phrases. However, if your dataset is too small or if your model didn't generalize well you will get more errors and nonsense phrases. By the way, if you have a good GPU you can run this faster by making these slight changes. Type device equal to CPU. Right after this line send the ws to device. You also need to send all the other tensors to device. Finally, send xs and ys to device too. To run on GPU set device to coda. I have already tested a bigger model on a colab instance with more layers and with a temperature of 0.7. So here you can see that even in the original text the term chasing was only used twice but our model learned to use this term in various other contexts like chasing rays, chasing milk, chasing toys, chasing in the sun, chasing strings, chasing stale and so on. So the idea here is if you train this model on a large corpus of poetry about cats and if you train the same model on a large corpus of texts about dogs then we can generate a poem on dogs that's new and doesn't exist in the training set because the model can now interpolate and integrate what it has learned about dogs into a poem. I believe we can simplify the current state of the art neural networks much more. In the end everything is translated to dot products and floating point operations. However we know in principle that all the arithmetic operations are translated back to simple logical operations on binary input. So in theory if we kept the middleman we may find a lot of redundant operations and simplify AI to few lines of logical operations on binary on symbolic inputs. So one day we may go back a full circle. This is important because we know that neurons in our brain don't do floating point operations. Instead they communicate with binary spikes and if we identify intelligence in its basic essence then we will have a better chance to understand intelligence and the human brain. The objective of any intelligence system should be aligned with human interest but human interests are not aligned with each other. Some people say they will follow truth even if it hurts their feeling. But what's truth anyway? If the base reality is subjective then I don't know what truth is. If the base reality is objective then actually I still don't know what truth is but I can entertain some thoughts. So basically our brain tries to predict the future. Now that's a bit tricky because we are not just observers but let's talk from the eye of an observer. The truth is the path that will actually happen. Therefore intelligence is the ability to see the path more clearly from the noise. So intelligence is the ability to predict correctly which is the ability to compress losslessly. This has even inspired competition for compression algorithms. And this resonates well with Occam's principle which is a simple guidance for finding more truthful hypotheses. Let's say this graph is all the knowledge that we can attain. The part inside the circle is already discovered by us. Finding new nodes inside the circle is interpolation. Finding new nodes outside the circle is extrapolation. Ideally we want to discover new emergent nodes with minimal trial and error. Let's say we have two models, a big model and a smaller model. But both are capable to fit and explain the discovered knowledge inside this circle very well. From Occam's principle we should trust the smaller model more because it has more potential to extrapolate truthfully. Now what's the smallest possible model that can fit all of our discovered data? The theory of everything which physicists are looking for is probably the smallest possible model. If we apply this model recursively to the initial state of our universe then its output can predict the Big Bang event, the emergence of galaxies and planets and the evolution of life up until the current point where we are trying to predict the next letter in the sentence. But that's too much computation just for predicting the next letter. So it's not possible to compute the model of our universe faster than itself. Therefore even though the theory of everything is the most truthful model it's useless for predicting the future because we can't simulate it faster than our universe. All the other models even our mental model have some degree of uncertainty so probably we can never find the absolute truth and we are cursed to live with uncertainty. Thanks a lot for watching. Stay tuned or watch a related video. See you next time.",
        "output": "### Introduction to Building a GPT-like Model with Python\n\n#### Neural Networks and the Learning Process\n- Neural networks involve learning by fitting data, which requires figuring out data relationships (i.e. weights) given inputs and desired outputs.\n- Evolution inspired methods like random search or making small random changes can be used to find weights, but they can be inefficient, particularly for complex relationships.\n- Optimization strategies like gradient descent can efficiently minimize errors by following the steepest descent path.\n- Forward and backward passes through the network allow for the fine-tuning of weights to learn data relationships with optimized methods like the Adam optimizer.\n\n#### Using the Simple Neural Network to Model and Test Functions\n- The given example demonstrates how a neural network can learn the square function.\n- However, the network does not generalize well to unseen data.\n- Ways to improve generalization include adding more data, training the network for longer periods, and adding more constraints or tricks (like regularization).\n- The neural network can excel at interpolation and has limited extrapolation capacity.\n\n#### Constructing Simple Neural Networks with PyTorch\n- PyTorch is useful for creating and training neural networks, with support for customizing the optimizer, learning rate, and other parameters.\n- The neural network model can be further refined by:\n  - Defining model classes\n  - Implementing forward and backward passes for the model\n  - Updating weights\n  - Testing the model with unseen data\n- Smaller weights or activation functions like ReLU can help a network learn better generalizations for unseen data.\n\n#### Real-World Neural Network Applications\nNeural networks have numerous applications, including:\n  - Image detection\n  - Medical applications\n  - Language translation\n  - Autoregression for text, audio, and image data\n  - Generating AI with past context and future predictions"
    },
    {
        "instruction": "<|SYSTEM|>You are a scholars that is taking markdown notes about some video, you would be giving some written scripts for the task.\nYou have finished writing the notes, and you want to extract the title from the notes.\nThe title should only be the name of the technology or algorithm, no extra words needed.\nPlease write the notes in markdown format. No need to write \"Title:\" for the Title. Make sure the title is within 10 words.\nHere is an example:\n## Hidden Markov Model\n<|ASSISTANT|>## ",
        "input": "<|USER|>### Introduction to Building a GPT-like Model with Python\n\n#### Neural Networks and the Learning Process\n- Neural networks involve learning by fitting data, which requires figuring out data relationships (i.e. weights) given inputs and desired outputs.\n- Evolution inspired methods like random search or making small random changes can be used to find weights, but they can be inefficient, particularly for complex relationships.\n- Optimization strategies like gradient descent can efficiently minimize errors by following the steepest descent path.\n- Forward and backward passes through the network allow for the fine-tuning of weights to learn data relationships with optimized methods like the Adam optimizer.\n\n#### Using the Simple Neural Network to Model and Test Functions\n- The given example demonstrates how a neural network can learn the square function.\n- However, the network does not generalize well to unseen data.\n- Ways to improve generalization include adding more data, training the network for longer periods, and adding more constraints or tricks (like regularization).\n- The neural network can excel at interpolation and has limited extrapolation capacity.\n\n#### Constructing Simple Neural Networks with PyTorch\n- PyTorch is useful for creating and training neural networks, with support for customizing the optimizer, learning rate, and other parameters.\n- The neural network model can be further refined by:\n  - Defining model classes\n  - Implementing forward and backward passes for the model\n  - Updating weights\n  - Testing the model with unseen data\n- Smaller weights or activation functions like ReLU can help a network learn better generalizations for unseen data.\n\n#### Real-World Neural Network Applications\nNeural networks have numerous applications, including:\n  - Image detection\n  - Medical applications\n  - Language translation\n  - Autoregression for text, audio, and image data\n  - Generating AI with past context and future predictions",
        "output": "## GPT-like Model with Python"
    },
    {
        "instruction": "<|SYSTEM|>You are a scholars that is taking markdown notes about some video with Wiki style, you would be giving some written scripts for the task.\nAnd summarize each paragraph with a title.\nRemember to keep the technology or algorithm name, very important.\nPlease write the notes in markdown format.\nHere is an example:\n### Definiation\nA deadlock is a situation in which two computer programs sharing the same resource are effectively preventing each other from accessing the resource, resulting in both programs ceasing to function. \nThe earliest computer operating systems ran only one program at a time.\n\n### What it have to do with Relationsal Database ManagementSystem\nIn a database, a deadlock is an unwanted situation in which two or more transactions are waiting indefinitely for one another to give up locks. \nDeadlock is said to be one of the most feared complications in DBMS as it brings the whole system to a Halt. \nWhen you tries to insert or update data on database, sometime your record need to depend on the row record.  \n<|ASSISTANT|>#### ",
        "input": "<|USER|> We're going to talk about Knowledge Graphs. Knowledge Graphs are an AI technology that is used pretty much anywhere online in any major platform or system or application that we're using on a daily basis. Anytime you ask Google or Bing or your favorite search engine a question, it will most likely use a Knowledge Graph alongside many other types of techniques to retrieve information and answer questions. So if, for instance, you are asking about Bush House, which is the building where we are now, any search engine will provide a list of documents, maybe some ads of some form, but somewhere on that result page it will give you some very specific information about Bush House. It will provide probably a picture of the entrance where you came in, and then it will say, well, this is an address of this thing, Bush House, that me as a computer without any sort of human understanding can nevertheless relate to. So that string of characters, Bush House, all of a sudden has an address, and it has perhaps opening hours. And it has particular departments at King's College that are located in this building. Now, if I go and query National Theatre, I will still get information about the address because it's still a building, but perhaps it will not tell me anything about any university departments because National Theatre is not a university or it's not a location for a university. So how does Google know all those things about the world around us, about London? Well, because it uses the knowledge gap. And I say the because actually Google is the company that has coined this term. But actually the principles and the technologies have been around for much, much longer, in fact decades, and decades where AI researchers were building so-called knowledge bases, which are very similar types of IT systems that are encoding knowledge, information, data. I don't think we need to go into a big discussion about what these three mean and how they are different from each other. They are. In this case, in the example I had, it's about knowledge about London. What connects all these different pieces of data together? So this is the knowledge graph. This is the knowledge graph. And the knowledge graph, as it is today, not just in the case of Google, but pretty much any large organization that provides this sort of Internet information services has one. They are huge databases with billions and billions of facts like this. Bush House is located in London at this address, right? This information is extracted from documents. Say on the King's website somewhere there is information about Bush House and it's written probably in text. And Google has the machinery to process all this text and recognize that this string Bush House is probably a building and learn that information, that structured information, and store that in the knowledge graph. So the knowledge graph, ultimately, you can imagine it as a database that stores facts about things in the world and relations between them. And we in AI and in knowledge graph engineering, we distinguish between the actual data, so Bush House, National Theatre, and then abstract data, which is theatres, buildings, universities. So any university, most universities in the world will have a set of common properties, right? They will teach students. They will be located on a certain campus. They will have a certain number of employees. King's College London has three or four actually different campuses. LSE, also university, shares many types of properties, also teaches students, also has employees, but has different campuses, different address. So there is the data about LSE in King's College London, and there's all these joint properties that all types of things like LSE, like King's College have, and that helps in web search, but in many other types of AI applications as well. So in question answering, in recommendations of movies, of products, and so on and so forth, because it basically allows that recommender or search engine to provide very specific, very accurate, to the point answers, rather than relying on extracting those answers from natural text. And you see the difference actually in search when sometimes you type something and Google says, well, people also asked for this and this, right? And then you have that question and you have that little drop down list and a snippet answer. Now some of those answers sometimes match what you're looking for, but sometimes they're actually only remotely or not all relevant. And that's the difference. A computer has struggles to process and extract very specific types of information from what we call unstructured data, which is text, images, videos, but it has no issue whatsoever or much less issue to work with data that is already structured in a graph, for instance. My simplistic view of graph being a thing you plot on a piece of paper, you know, to axes beyond paper, you could have many more axes. Is this what we're looking at here? So we're not talking about charts, right? So we're not talking about visually displaying data in a two, three or multidimensional visual space. We're talking about a graph in a mathematical sense. So a graph consists of nodes and edges. We know graphs from computer networks, right? So you have computers, which are the nodes and then the communication links between them. We know graphs from geospatial information systems, maps, right? So you have points on a map and then you have roots. You need all sorts of algorithms to compute edges, for instance, in the case of maps. If you want to go from A to B, you can do that by passing multiple intermediary nodes. So when you say an edge, I mean, we kind of had the example of a map and that being a root in the map. Yes. How do you define an edge? It's any type of relation, right? So for a bush house, it could be has address. Yes. So let's just say that we represent the nodes in these ovals, and then we represent the edges as lines between these nodes. So we're talking about Bush House and we're talking about King's College London, right? And then so what's the relationship between Bush House and KCL? Campus of or something like that? Campus of or we could call it building of KCL. We can have another relationship that says Bush House has picture. And here's my picture of Bush House. And it could be an actual image. All these lines can have different meanings. So I can have the has picture edge from Bush House from KCL. You could have something like has logo points to another image that is the logo. You could also have some sort of has picture with some picture of KCL. Who knows? And there's an image file here. So you can have different types of edges. This type of has picture edge and this one here are the same. So if you want to have all pictures of things, you can query just like you would query with SQL in a database. You can have any type of edge you want. You label it. So you give it a name. Mine have arrows. That's mostly just for my understanding because I want to say that the image is something that belongs to Bush House and not the other way around. Sometimes you have edges that could have arrows on both ends. So if you think about Elena has sibling Alex, my brother. So the edge has sibling is what we call symmetrical. So if I'm his sister, then he's my brother. Say the moment when I define the edge has sibling from the node Elena to the node Alex, the computer. I can actually tell the computer, by the way, there is also the opposite edge. If I have an edge or relationship like is located in Bush House is located in London. You see how the graph actually starts to take shape in London and Elena works at Bush House and London is located in England. You can tell the computer well is located in is a type of edge that is what we call in mass transitive. So if I know that Bush House is located in London and I know London is located in England, then I know that Bush House is located in England. So you don't actually have to look for that sort of question. Someone ever would ask is Bush House located in England? You don't actually need to look for the data specifically. You can what you call infer it. So you can from all this information that I have written down, you can now infer additional knowledge. This is a bit like an ontology. Yes, it is an ontology. Well, it is based on an ontology to be more specific. The ontology will be something about cities and about buildings and cities and buildings belonging to institutions and people working in certain buildings. Is there an easy sort of free way people could play with this or try this? Is there something out there people could download? For developers, the Google Knowledge Graph API is something that people can look at. There's also various open source knowledge graphs available. Let's say Wikipedia has one. It's called Wikidata and it's what they use to manage all the structured information they have. So, you know, when you go on a Wikipedia article and on the right hand side, you see what they call an info box. Bush House, go on Wikipedia and then you'll see that comes from a knowledge graph that is publicly available and people can download it. It's quite big. But at the same time, you have lots of information that you can play with and I don't know, train a neural network on and do all sorts of wonderful things. So, yeah, it's available and there are smaller versions as well for people to play with. It's super hard to know. I mean, there are different things going on here. There's intelligence, which I think you've got to say maybe some computer systems are intelligent, at least at certain things. There's sentience and there's consciousness and they're slightly different than intelligence.",
        "output": "### Introduction to Knowledge Graphs\nKnowledge Graphs are an AI technology used in major platforms and systems to provide accurate and specific information. They are used in search engines, question answering and recommendation systems.\n\n### Knowledge Graphs vs. Knowledge Bases\nKnowledge Graphs help computers better understand complex relations between different pieces of data. The term \"Knowledge Graph\" was coined by Google but it is based on the principles of earlier technologies called Knowledge Bases.\n\n### Structure of Knowledge Graphs\nKnowledge Graphs are mathematical graphs consisting of nodes (entities) and edges (relations). For example, a node could be Bush House, and an edge could be its address. Edges have labels, such as \"has_address\" or \"has_picture\".\n\n### Ontologies and Knowledge Graphs\nKnowledge Graphs are based on ontologies which describe abstract concepts like cities, buildings, and institutions. Ontologies help provide inferred knowledge. For example, if Bush House is located in London and London is located in England, the computer can infer that Bush House is located in England.\n\n### Accessing Knowledge Graphs\nDevelopers can access the Google Knowledge Graph API or explore open-source knowledge graphs like Wikidata, which is used to manage structured information on Wikipedia.\n\n### Intelligence, Sentience and Consciousness\nComputers can be considered intelligent in some aspects, but there's a distinction between intelligence, sentience, and consciousness. Knowledge Graphs contribute to the aspect of intelligence in computer systems."
    },
    {
        "instruction": "<|SYSTEM|>You are a scholars that is taking markdown notes about some video, you would be giving some written scripts for the task.\nYou have finished writing the notes, and you want to extract the title from the notes.\nThe title should only be the name of the technology or algorithm, no extra words needed.\nPlease write the notes in markdown format. No need to write \"Title:\" for the Title. Make sure the title is within 10 words.\nHere is an example:\n## Hidden Markov Model\n<|ASSISTANT|>## ",
        "input": "<|USER|>### Introduction to Knowledge Graphs\nKnowledge Graphs are an AI technology used in major platforms and systems to provide accurate and specific information. They are used in search engines, question answering and recommendation systems.\n\n### Knowledge Graphs vs. Knowledge Bases\nKnowledge Graphs help computers better understand complex relations between different pieces of data. The term \"Knowledge Graph\" was coined by Google but it is based on the principles of earlier technologies called Knowledge Bases.\n\n### Structure of Knowledge Graphs\nKnowledge Graphs are mathematical graphs consisting of nodes (entities) and edges (relations). For example, a node could be Bush House, and an edge could be its address. Edges have labels, such as \"has_address\" or \"has_picture\".\n\n### Ontologies and Knowledge Graphs\nKnowledge Graphs are based on ontologies which describe abstract concepts like cities, buildings, and institutions. Ontologies help provide inferred knowledge. For example, if Bush House is located in London and London is located in England, the computer can infer that Bush House is located in England.\n\n### Accessing Knowledge Graphs\nDevelopers can access the Google Knowledge Graph API or explore open-source knowledge graphs like Wikidata, which is used to manage structured information on Wikipedia.\n\n### Intelligence, Sentience and Consciousness\nComputers can be considered intelligent in some aspects, but there's a distinction between intelligence, sentience, and consciousness. Knowledge Graphs contribute to the aspect of intelligence in computer systems.",
        "output": "## Knowledge Graphs"
    },
    {
        "instruction": "<|SYSTEM|>You are a scholars that is taking markdown notes about some video with Wiki style, you would be giving some written scripts for the task.\nAnd summarize each paragraph with a title.\nRemember to keep the technology or algorithm name, very important.\nPlease write the notes in markdown format.\nHere is an example:\n### Definiation\nA deadlock is a situation in which two computer programs sharing the same resource are effectively preventing each other from accessing the resource, resulting in both programs ceasing to function. \nThe earliest computer operating systems ran only one program at a time.\n\n### What it have to do with Relationsal Database ManagementSystem\nIn a database, a deadlock is an unwanted situation in which two or more transactions are waiting indefinitely for one another to give up locks. \nDeadlock is said to be one of the most feared complications in DBMS as it brings the whole system to a Halt. \nWhen you tries to insert or update data on database, sometime your record need to depend on the row record.  \n<|ASSISTANT|>#### ",
        "input": "<|USER|> We propose editable nerf, editable topologically varying neural radiance fields that enable end users to easily edit dynamic scenes by key points. Input with an image sequence from a single camera, our network is trained fully automatically, and models topologically varying dynamics, using surface key points. Then end users can edit the scene by easily dragging the key points to desired new positions. And further render a video. And even slide on the piano keys, which is unseen in the training sequence. For an input sequence that includes shaking and lifting the blue or the red cup, our method can generate free movements for the two cups. Here we show qualitative comparison results with a state-of-the-art method Conurf. Conurf only supports one-dimensional editing and requires user annotations, while our method supports up to three-dimensional editing without manual supervision. We can also drive our modeled scenes by motions from other sequences. Here we show more results of our method. Here we show more results of our method. We implement a graphical user interface that enables users to edit the scene and change viewpoints, easily and intuitively. And supports recording and editing sequence, then rendering a video. Thank you for watching.",
        "output": "### Introducing Editable NeRF\r\nWe propose **Editable NeRF**, an approach for creating editable topologically varying neural radiance fields. Users can easily edit dynamic scenes using surface key points. The network is trained using an image sequence captured by a single camera, and it enables users to edit the scene by dragging key points to new positions for rendering videos.\n\n### Editing Dynamic Scenes\nWith Editable NeRF, users can perform various editing actions, such as sliding piano keys or moving objects like cups. These modified scenes can then be transformed into new video content.\n\n### Comparison with Conurf\nEditable NeRF outperforms the state-of-the-art method Conurf, due to its up to three-dimensional editing capabilities and lack of user annotations. Conurf only supports one-dimensional editing and requires user input.\n\n### Driving Modeled Scenes with Other Sequences\nThe Editable NeRF method allows for driving the created scenes with motions from other sequences, showcasing its versatility.\n\n### Graphical User Interface Implementation\nEditable NeRF features an intuitive graphical user interface that allows users to easily edit scenes, change viewpoints, and record and edit sequences before rendering a final video."
    },
    {
        "instruction": "<|SYSTEM|>You are a scholars that is taking markdown notes about some video, you would be giving some written scripts for the task.\nYou have finished writing the notes, and you want to extract the title from the notes.\nThe title should only be the name of the technology or algorithm, no extra words needed.\nPlease write the notes in markdown format. No need to write \"Title:\" for the Title. Make sure the title is within 10 words.\nHere is an example:\n## Hidden Markov Model\n<|ASSISTANT|>## ",
        "input": "<|USER|>### Introducing Editable NeRF\r\nWe propose **Editable NeRF**, an approach for creating editable topologically varying neural radiance fields. Users can easily edit dynamic scenes using surface key points. The network is trained using an image sequence captured by a single camera, and it enables users to edit the scene by dragging key points to new positions for rendering videos.\n\n### Editing Dynamic Scenes\nWith Editable NeRF, users can perform various editing actions, such as sliding piano keys or moving objects like cups. These modified scenes can then be transformed into new video content.\n\n### Comparison with Conurf\nEditable NeRF outperforms the state-of-the-art method Conurf, due to its up to three-dimensional editing capabilities and lack of user annotations. Conurf only supports one-dimensional editing and requires user input.\n\n### Driving Modeled Scenes with Other Sequences\nThe Editable NeRF method allows for driving the created scenes with motions from other sequences, showcasing its versatility.\n\n### Graphical User Interface Implementation\nEditable NeRF features an intuitive graphical user interface that allows users to easily edit scenes, change viewpoints, and record and edit sequences before rendering a final video.",
        "output": "Editable NeRF"
    },
    {
        "instruction": "<|SYSTEM|>You are a scholars that is taking markdown notes about some video with Wiki style, you would be giving some written scripts for the task.\nAnd summarize each paragraph with a title.\nRemember to keep the technology or algorithm name, very important.\nPlease write the notes in markdown format.\nHere is an example:\n### Definiation\nA deadlock is a situation in which two computer programs sharing the same resource are effectively preventing each other from accessing the resource, resulting in both programs ceasing to function. \nThe earliest computer operating systems ran only one program at a time.\n\n### What it have to do with Relationsal Database ManagementSystem\nIn a database, a deadlock is an unwanted situation in which two or more transactions are waiting indefinitely for one another to give up locks. \nDeadlock is said to be one of the most feared complications in DBMS as it brings the whole system to a Halt. \nWhen you tries to insert or update data on database, sometime your record need to depend on the row record.  \n<|ASSISTANT|>#### ",
        "input": "<|USER|> Okay, thanks. Okay, thanks everyone for joining. Welcome. So it's my great pleasure to use to you today's speaker Yang Song, so fan for to tell us tell us about the score based journey model models, which has been increasingly popular in recent days. So for example, it happens to be the latest door II to more opening I definitely also definitely also introduce a bit more about young so he's currently a final year PhD at Stanford University University and his research interest is in deep journey models and their applications to involve solving and and as it though he's all papers in this line of what has been recognized with with a standing people at last year's I clear and also an oral presentation near is 2019. He's also a GCC of the Apple PhD fellowship in AI machine and also the JP Morgan the PhD fellowship. Okay, so back to you. So you're very excited to hear about your talk. Yeah, thanks for the introduction. I saw a message in chat saying that the audio quality is quite and I wonder whether it is true for me as well. Is it fine? It's fine. So I can hear you clearly. Okay, sure. Sounds good. Yes, I'm very excited to share with you my research on a new method to generate data. And I will show you some quite exciting results that shows that you can rival the performance dance in some applications like image generation. So in the past few years, we have witnessed a lot of progress in the field of machine learning, we have so much progress in many applications such as autonomous driving, speech recognition, machine translation, recommendation systems and many others. So in almost all those applications, we follow the same machine learning pipeline. And in this pipeline, we first have access to a physical world. And we need human efforts to collect data from the world followed by data pre processing and cleaning, and then skip this gives us a curated data set, we then fit this data set to machine learning algorithms to train an algorithm to perform a certain task. So in this pipeline, data has become indispensable for the success of machine learning. And we need high quality data sets for 20 many different things. For example, we need a high quality image data set such as image net to train an accurate model for image recognition. We need high quality text data set like glue to train models for natural language understanding. And there are many other examples in other applications or machine learning. However, this data collection procedure is a laborious in most cases. And what's worse, it has to be repeated every time we want to obtain data on the new settings. So one research question I'm interested in is suppose we have collected some data, can we leverage those existing data to simulate the data generating process in order to obtain data on the new settings or with new constraints. So equivalently, can we build a data simulator, a data simulator is a machine that can generate a number of data points. And if this data simulator captures the essence of the data generation process, you should be able to generate new data points on the slightly different new settings. And also just like some simulators in other fields, we hope to provide potential data points to this data simulator and query the probability values that those data points could be generated from this data simulator. So those data simulators actually have been studied with a long history in statistics and in machine learning. In statistics, they are known as statistical models. And in machine learning, they are known as genetic models. So because I work in machine learning, I will refer to such data simulators as genetic models from now on. And because we use deep neural networks, I will use this symbol neural network to represent those genetic models. So why do we need those genetic models? Because they have so many important applications in the real world. Suppose we have a genetic model that captures the image generation process, then we should be able to use it to convert a stroke page into a photorealistic image that preserves the semantics. And here there are two images on the right hand side you can see, and those images were actually generated by a real method that I'm going to talk about in today's presentation. So given a genetic model on paintings, we may also use it to convert text description into high quality artwork that matches this text description. And genetic models can also help us solve challenging inverse problems, such as those in medical image reconstruction. Suppose we have a genetic model on medical images, we may use it to convert machine scanings of human bodies into medical images that are consistent with those machine measurements. In computer tomography, for example, we can use a genetic model trained on typical CT images to provide prior knowledge. And this prior knowledge can significantly reduce the number of x-ray projections for patients, and this can reduce radiation, of course. And with a genetic model on images of traffic signs, we may also use the probability values from those models to detect potential outliers. For a typical traffic sign image, a good genetic model will assign higher probability to such images, but for a suspicious traffic sign image, someone might have tampered with the traffic sign to cause confusion, and a good genetic model will assign lower probability to such images because those images, they appear very infrequently in the real world. So those probability values can quantify the frequency of those images, the typicality of those images, and they can be used for detecting outliers. And of course, genetic models are not restricted to image modeling. The same principle can be applied to basically every other possible data modality. And one example is we nowadays use genetic models to convert text to audio signals, convert text to speech, and we can also use genetic models to convert documentation to source code, and this can greatly improve the productivity of computer programmers. So now we know genetic models are so powerful, the question is how can we build such powerful genetic models? It turns out that we can do this by estimating the probability distribution of data. So suppose we have collected a large training dataset. As a toy example, this is a dataset of images of dogs. So in statistics and machine learning, we can typically assume that there exists an unknown probability distribution called the data distribution that has generated every single data point in the dataset. And if we can find another probability distribution as our model to approximate our unknown data distribution in some way, then we can use this model distribution for many different applications. For example, we can generate an unlimited number of novel data points just by sampling from our model distribution and we can also compute the probability values for any potential data point. For a relevant data point like this picture of a chihuahua, we design a higher probability, but for a relatively irrelevant data point like this picture of a muffin, we design a lower probability because those images are outside of the data distribution. So this model distribution can generate novel data points, it can also give us probability evaluation, so it is actually a good generative model. The biggest challenge here is our data distribution, however, can be extremely complicated in the real world, especially for high-dimensional data like images or audio. And as a result, we need to find a very complex probability distribution as our model to fit our complex data distribution. So how can we find such a complex model distribution? Recall that in statistics, we work a lot with those tractable simple distributions, such as a Gaussian distribution. The Gaussian distribution is fully tractable, but unfortunately, it is too simple to fit to complex data distributions. And in fact, a Gaussian distribution can be represented as a computational graph of two layers, where the first layer corresponds to the input. The second layer is basically a single unit that gives us the probability density function of the input. So here, mu denotes the mean of this Gaussian and d denotes the dimensionality of x. To build a more complex model distribution, we of course need a bigger and also deeper computational graph, and here is a deep neural network. Here, we hope to use a deep neural network to represent a probability distribution P theta, where theta denotes the weights in this deep neural network. And when we use deep neural networks to build genetic models, we get deep genetic models. Unfortunately, it is nontrivial to use a deep neural network to directly represent a probability distribution. This is because a deep neural network is basically a function that converts a high-dimensional input to typically one-dimensional output, which we denote as f theta. So this value f theta does not directly represent a distribution, because it may not even be positive everywhere. So to construct a distribution from this deep neural network, we typically need to first take the exponential of the output to make it positive, and then we can normalize the output by dividing by a constant z theta in order to construct a probability distribution that is positive everywhere, is properly normalized, and properly defined. So here, the denominator z theta is known as the normalizing constant of partition function. So this normalizing constant is given by computing the high-dimensional integral of the exponential function of f theta for all possible values of x. For Gaussians, this normalizing constant is fully tractable, because f theta has a special functional form that allows us to compute the integral in a closed form. But Gaussians are definitely too restrictive, and once we move from Gaussians to deep neural networks, you can see that this normalizing constant is typically intractable to compute for the most general families of neural networks. And in fact, even in the simplified case where x is discrete and this integral becomes summation, computing this normalizing constant is still a sharpie complete problem, which is at least as hard as empty complete. And this says that this normalizing constant is one of the fundamental challenges of deep genetic modeling. And actually, it is also an important challenge in some other relevant fields, such as thermodynamics and statistical mechanics. The question is, how does the current literature, how does the current research community address this difficulty of the intractable normalizing constant? There are many three approaches. The first approach is that we can approximate this normalizing constant with approaches like a variational inference or Markov chain Monte Carlo. So here are some examples. The cost is once we approximate this normalizing constant, we lose the ability to compute probability values accurately, because those probabilities are given by dividing by the approximate normalizing constant. The second major approach is we can use restricted neural network models to make sure that our normalizing constant is tractable by construction. So here are some examples. The cost is once we restrict our neural network models, we also limit the flexibility of models we can use for building our deep genetic models. And the final approach is to use the generative adversarial networks or GANs. So GANs can completely bypass this intractable normalizing constant problem, because they model the data generating process directly, not the underlying probability distribution. However, because GANs are not probabilistic, they cannot compute probability values easily. So those are some of the existing challenges of deep genetic modeling. How can we address all those challenges? If we want to build a better framework of genetic modeling, then what would be needed? So the first thing we want to harm is we hope to allow very flexible models. So this addresses the second difficulty. And this also helps us take full advantage of the deep learning revolution. This can allow us to build a very flexible deep neural networks to build a very powerful genetic models. And the second that this is the right time is we hope to allow accurate probability evaluation. So this can address the rest of the difficulties. And those accurate probability values can be very useful in outlining detection, in model comparison. They're also critical in lossless compression according to the standard theory of our coding theory and information theory. So finally, towards a better framework of genetic modeling, we of course want to generate data with better quality. So in the meanwhile, we also hope to control this data generation process in a flexible way so that this might be used for many other important applications such as those in medical image reconstruction. And in this talk, I will show you one such framework that satisfies all three Desireata. The key of this framework is to work with score functions to represent probability distributions. So what are score functions? Suppose we have a continuous probability distribution, in which case Px is known as the probability density function. We can define the Stein score function as the gradient of a log Px. So be careful the gradient is taken with respect to x, not with respect to model parameters. So this quantity is also called score function or scores for brevity. So what does a score function look like? So here is a figure that shows the density function and the score function for a mixture of Gaussian distribution. The density function is color coded, where darker color means a higher density. So the score function is a vector field of gradients and points to directions where the density function has the fastest growth. So given the density function, we can compute the score function by taking derivatives. And conversely, once we have the score function, we can also recover the density function by computing integrals. So mathematically, this score function preserves all the information in the original probability density function. But computationally, we have a lot of advantages. So by working with those score functions, we can build a new framework of genetic modeling, which we term score-based genetic modeling. So this framework satisfies all three deserata that we listed before. So first, it allows very flexible models because score functions can bypass this intractable normals and constants completely, as I will discuss right after. And we can use very flexible deep neural networks to model those score functions. And then we can learn those neural networks from data with principled statistical methods. And I will also show you that we can generate new data samples by directly sampling from our models, our score functions. And moreover, we can have very high quality new samples that can be comparable to grants in many applications. And the sampling procedure can also become controllable to allow us to solve many challenging new post inverse problems of those in medical image reconstruction. And finally, I will show you that it is possible to compute accurate probability values using those models of our score functions, and we can demonstrate better empirical performance. So now, as the next part, I will first focus on how score-based genetic modeling allows us to use very flexible models. Recall that one of the major difficulty in deep genetic modeling is due to our intractable normals and constants. So when we try to model probability distribution directly, then no matter how we change the model parameters or architectures, they always have to be normalized, meaning that the area below the curve has to be one. So this means if we want to use deep neural networks to model this probability distribution, we have to worry about this intractable normals and constant issue. But in contrast, if we use the score function to represent this probability distribution, then as this animation indicates, there is no such normalization restriction at all. And in fact, if we compute the score function for the deep neural network model on the left-hand side, you can see that the score function is just the difference between two terms, where the only term that depends on this intractable normalizing constant is always a zero, because the gradient of a constant is always zero. And as a result, the score function is the same as the gradient of this neural network. This gradient can be computed easily using automatic differentiation or with bank propagation. So from now on, I will use f theta to denote this deep neural network model for our score function, and I will call it a score model. So suppose we have collected a large-trend dataset. Here, each point represents one data instance. We may use x1, x2, to xn to denote every data point in this dataset, and we can use p data to denote the unknown data distribution that we hope to estimate. So with traditional statistical approaches, we can use maximum likelihood estimation to train a properly normalized statistical model, p theta, to approximate p data. The question is, can we do the same thing for score models? Can we train the score function to estimate the unknown score function of the data distribution? So mathematically, the problem is described as below. We have a dataset, and every data point was ID sampled from this unknown data distribution. Our goal is to estimate the score function for the data distribution. And we have a score model. This is a vector-valued neural network that maps a d-dimensional input to a d-dimensional output. And we hope to optimize this score model such that it is close to the unknown score function of our data distribution. So how can we train our score model? We need to minimize an objective function which compares two vector fields of our score functions, where one vector field is ground truth score function for our data distribution, and the other vector field is given by our score model. So how can we compare their difference? Let's imagine we can overlap them with each other, then we can compute the difference vectors between those pairs of vector fields. And then we can average over the length of those different vectors to form a single scalar objective to quantify the difference between those two vector fields. So mathematically, this intuition can be captured by the feature divergence objective function. So feature divergence is essentially the expected squared Euclidean distance between the data score and the model score, averaged over samples from the data distribution. However, this feature divergence cannot be directly computed because we don't know the ground truth value of the score function for our data distribution. But luckily, there exists an old method called score matching that uses integration by parts to convert our feature divergence into the following equivalent objective. And in the equivalent objective, there is no such dependency on the data score function. And the expectation in our score matching loss function can always been estimated using the empirical mean over the training data set. So this score matching idea is quite smart, but unfortunately, it's not very frequently used in practice. This is because the score matching objective has a critical computational challenge. That is, we have to compute the trace of the Jacobian of our score model. When X has a lot of dimensions, this Jacobian matrix will become a huge matrix. And computing the trace will involve a large number of bankruptations. So it's not a scalable approach. So in our research, we hope to address this difficulty by leveraging the idea of random projections. So the intuition is as follows. Suppose we have two high dimensional vector fields that are close to each other. We may project them along random directions to get random one dimensional scalar fields. So those scalar fields will also be close to each other if the original vector fields are close to each other. So this random projection can actually reduce the dimensionality of the problem to one. And based on this intuition, we developed a method called slice the score matching. So it is a computationally more efficient variant of a score matching, where the cost of computing the objective of score slice score matching is comparable to the cost of evaluating the output of the score model. So it's quite efficient. And we have additionally proved the consistency and asymptotic normality properties of slice score matching, meaning that this is a principled statistical approach. For detailed algorithm and proofs, I don't have time to cover it here. But if you're interested, please feel free to read our papers. So now I'm going to show you that score-based genetic modeling can allow very flexible models because the score function bypasses this normalizing constant. And as a result, we can use very flexible deep neural networks called score models to model our score function. And we can learn them from data using principled statistical methods like score matching. For the next part, I will talk about how we can generate samples from those score models. And moreover, how can we obtain high quality samples that can be comparable or better than GANs and how we can control this sample generation procedure for important applications in reverse problem solving. So we know that given a large data set, we can use methods like score matching to train our score model to estimate the score function of the data distribution. The next step towards building a genetic model is to find an approach to create samples directly from a given score function. So how can we do this? Suppose we are given a score function, and let's imagine that many random initial points scattered across the space. Can we move those random points to form samples from the score function? One idea is we can move those points along the directions given by the score function so that those points have higher and higher probability values. But this won't work in practice because all the points will collapse into each other. Luckily, we can address this collection issue by injecting the right amount of Gaussian noise into the same procedure. So this method of moving data points by following the noisy score functions is well known as the Langevin dynamics sampling procedure. And it is also well known that Langevin dynamics can give us correct samples as long as we keep this sampling procedure to be sufficient long until convergence, and as long as we set a sufficiently small step size. So now we know score matching can estimate a score function from data and Langevin dynamics can generate samples from this score function. So can we combine them together to generate new samples? Well, we can do this in principle, but it does not work well in practice. So here the left figure shows some existing images in the CIFI-10 dataset, and the right figure in comparison shows the newly generated samples by combining for matching Langevin dynamics directly. So clearly you can see that the newly generated samples do not look realistic at all. So clearly there is something wrong with this simple approach. And in fact, score matching and Langevin dynamics can be around for decades, but people were stuck on this problem of generating convincing high quality images for a long time. So what could be the reason here? In our research, we have identified several issues, and one of the biggest challenge is the difficulty of estimating score functions in low data density regions. So for illustration purpose, let's consider a toy example. The left figure shows the density function for mixture of Gaussian distribution. The middle figure shows the ground truth score function of this mixture of Gaussian distribution. And the right figure shows the estimated score function by training a score model using score matching on samples from this mixture of Gaussian. So clearly the estimated scores are anchored in high data density regions, which are given by these green boxes. In the low data density regions, unfortunately, you can see that the estimated scores are not accurate. And this is because we use score matching to estimate our scores. And this score matching objective only considers the difference between the data score and the model score averaged over samples from the data distribution. So in low data density regions, there are no existing data samples, and therefore not sufficient information to infer the true score function in those regions. And this means that when we use the estimated score function from score matching, large dynamics might have a lot of trouble exploring those low data density regions. To address this difficulty, we propose to inject the right amount of Gaussian noise to perturb our data set. So after doing this noise perturbation, we can perturb those data points to populate low data density regions to shrink the size of low data density regions. And this can greatly help score estimation in those low data density regions. So what does it mean to perturb our data points? Let's imagine our data set contains many images. Then using Gaussian noise to perturb them means we add independent Gaussian noise to perturb each pixel of the image to make them look noisy. So in this toy example, you can see that after injecting the right amount of Gaussian noise, the estimated scores are accurate almost everywhere. And this is a very encouraging observation. It says that those score functions of noisy distributions might provide faithful information to help large dynamics to move from low data density regions to high data density regions. But the cost is those noisy data distributions are no longer good approximations to the original true data density. So how to address this issue? In our research, the key insight is we can use a large number of different noise levels to jointly perturb our training data set. So as a toy example, let's suppose we consider three noise levels ranging from sigma 1 to sigma 3. We can use Gaussian noise or mean zero and standard deviation from sigma 1 to sigma 3 to perturb our training data set. So this will give us three noisy training data sets. And for each noisy data set, there will be a corresponding noisy data distribution, which we denote as P sigma 1 to P sigma 3. So in the context of images, this perturbation process will look like this. After obtaining those noisy data sets, the next step is to estimate the noisy score function for each of those noisy data distributions. So how can we estimate all those noisy score functions? One naive idea is we can train a large number of independent score models to estimate the score function for each noise level. However, this is not scalable at all, because in practice, we might need thousands of noise levels. So we cannot train so many different score models. In our research, we use a more scalable approach than we only need to train a single noise conditional score model. So this noise conditional score model is just a score model, but it takes sigma, the input noise level, as one additional dimension of the input. The output of this conditional score model corresponds to the score function for the input noise level sigma. So how can we train our noise conditional score model? We use the following score matching objective. So this score matching loss function is a summation of different individual score matching losses, where we have one score matching loss for each noise level sigma i. And we use the positive weighting function lambda sigma i to balance the scales of different score matching losses to help optimization. So this positive weighting function can be derived using principles of theoretical analysis, or it can be given using heuristics. So by minimizing this summation of our score matching loss functions, we are simultaneously minimizing each individual score matching loss. And as long as our original loss noise conditional score model has enough capacity, we can train this conditional score model to be anchored for every noise level using this objective function. So after training, how do we generate new samples? Well, we can first apply Lagrangian dynamics to sample from the score model with the biggest noise level. So the samples will be used as initialization to sample from the next noise level, and we can continue in this fashion until we sample from the score model with the smallest noise level. And those samples will be reviewed as the final samples from our noise conditional score model. So the sampling approach is called annealed Lagrangian dynamics, because the intuition is that we want to gradually anneal down the temperature of our noisy data distributions to generate samples. So how does this approach work in practice? This is the result when applying this method to several real image datasets. You can see that we start from a very noisy initialization, we gradually reduce the noise by modifying the images according to the noise conditional score model, and this can generate nice looking image samples much better than before. So the question is how good are those samples? We can compare with the previous best approach, which was GANs. So GANs were known to be the best model for sample generation for a long time, especially for image generation. And the training work we learned in a clinic has said that GANs are the most interesting idea in the last 10 years in machine learning, and they have attracted a lot of attention from big universities and corporations, and people have spent so much time on improving GANs, so that GANs can indeed generate very amazing image samples nowadays. The interesting thing is with such a simple approach of score-based model, we can actually outperform GANs in terms of our image generation on CIFAR-10 dataset. So CIFAR-10 is a quite competitive image generation benchmark, and this is the first time that a different type of genetic model can actually outperform GANs on this particular benchmark. And with better neural network architectures and better score matching methods, we can actually further improve the performance of sample generation on CIFAR-10. So here are some existing images in the training partition of the dataset. So CIFAR-10 contains many small images, but this is a quite competitive benchmark. You can basically find the results on CIFAR-10 in a large majority of genetic modeling papers in the past few years. Those are newly generated samples from our score-based models. So you can see that those samples look quite realistic and diverse, and they're also different from existing images in the training dataset, which means you cannot generate such samples by simply memorizing our training dataset. Quantitatively, we can compare with the previous best approach, which is CIFAR-10-2-ADA. We achieved significantly lower FRD scores and also higher inception scores. So those FRD scores and inception scores are the most popular metrics for comparing the performance of different genetic models in terms of image generation. This says that score-based models can challenge the long-time dominance of GANs on image generation. And our approach is also scalable enough to be able to generate high fidelity, high resolution image samples. So here are two random image samples from our score-based model. They all have the resolution of 1024 by 1024. So here are more such images from the same score-based model with the same resolution. So those images all look very realistic, although they are not stated without yet. They are already quite comparable to the best GAN method on this task. And the score-based models not only allow us to generate nice-looking samples, it can also control the generation process in a principled way. So suppose we have a genetic model that can generate images of dogs and cats, but we only want images of cats. And for this purpose, let's imagine we have a forward model and give us the image label Y for any image X. And we hope to provide a control signal, which is the target label Y here, and want to generate images of dogs only. So this amounts to finding the probabilistic inversion to the forward model. And this is the conditional distribution of X given the target label Y. And we call this the inverse distribution because it is basically an inversion to our forward model. So how can we find this inverse distribution? The most standard approach is to use the Bayes rule. So in the Bayes rule, we know the prior distribution. This is the unconditional genetic model. We know the forward model, this can be a classifier in this particular example. But we don't know the denominator. So this denominator is just the normalizing constant for our inverse distribution. So again, we can leverage score functions to bypass this intractable normalizing constant. And this gives us the Bayes rule for score functions. So this can be derived quite easily by taking the logarithmic derivative on both sides of the Bayes rule. So again, you can see then the term involving this intractable normalizing constant goes away. And the score function for the inverse distribution is just a summation of two terms, where the first term is the unconditional score function that can be estimated by training on unconditional score model. Second term is the gradient of the logarithm of the forward model. So for a classifier, this gradient of logarithm can be easily computed. Some other applications, we can directly specify the forward model. So this gradient of logarithm can also be easily computed. So what's nice about this formulation is that we can now plug in different forward models for exactly the same score model, so that we can apply a single score model for different applications and test the time without model retraining. So here is one example of this idea. We can use this to convert stroke patterns to photorealistic images. Here the forward model gives us a stroke pattern for any image. And we can train a score best density model on realistic images only without worrying about stroke patterns. And then we use this score best model to invert our forward model to convert stroke patterns into photorealistic images. And here is another result. This is excellent work done by other researchers. So here the forward model is an image captioning neural network and gives us the text description for any input image. We can train our score best model on images only without worrying about the text description at all. And then we can use this score model to invert this image captioning neural network to generate images conditioned on text. And here is another result. This is an application in medical image reconstruction. We consider the problem of computed tomography. So in this case, we use x-rays to project through human bodies. Those x-rays will pass through human flesh and hit the detector. So the detector gives us the synogram. And we hope to invert this detector observation synogram to the original cross-sectional medical image. And in particular, we care about this task of sparse field computed tomography, meaning that our observation is sparse and we use as few x-ray projections as possible to limit the radiation for patients. And in this case, the forward model gives us the synogram for any medical image. So this can be always given using physical simulation, can be designed by hand. And we can use genetic models here. We train genetic models on typical medical images to provide prior knowledge. So this can help us reduce the requirement on the number of x-ray projections. And here is the result on some real CT datasets. We consider only 23 x-ray projections, while in contrast, people typically need hundreds to thousands of x-ray projections. So here is the result of a traditional compressed sensing algorithm. You can see that with only 23 projections, the image is quite blurry. And by the way, we use PSNR and SSI to quantify the performance of different results. So here are results that uses deep neural networks to directly map 23 projections into a medical image. And here is our results. This is a purely generative approach. And here is the ground truth. So you can see that both qualitatively and quantitatively, we can achieve better performance than competing deep learning methods. And this is actually the first time that a purely generative approach can outperform deep learning methods specifically designed for a particular number of projections. While as a purely generative approach, our method does not require the knowledge of this task to entrain it. So we can easily adapt our model to different settings, such as different number of projections and test the time without model retrain. And people have observed a similar success on accelerated magnetic resonance imaging as well with score-based genetic models. So the research community has been quite excited about those line of progress in score-based genetic model. And people have reported new state-of-the-art results on many other tasks. This includes image generation on more challenging image data set like ImageNet. So here all four images were generated from a score-based model and they have a higher quality than even the best-gain approach on this task. And similar success has been observed on audio synthesis, text-to-speech generation, material design, shape generation, for 3D point clouds. And we have also observed the similar success on molecular confirmation prediction and on time series prediction. And some researchers from Oxford have also created a website called score-based-genetic-modeling.github.io which includes a superset of those examples. So far we have talked about how score-based genetic modeling allows flexible models and how we can generate high quality samples by directly sampling from those models with a controllable generation procedure. For the next part, I will focus on how we can compute probability values accurately with models of score functions. To compute probability values accurately, we will need to generalize the previous framework from using a finite number of iterations to using an infinite number of noise levels. So to gain some intuition first, let's suppose our data distribution is a one-dimensional mixture of Gaussian distribution. And let's imagine we only have three noise levels from sigma 1 to sigma 3. We can use Gaussian noise or mean 0 and the standard deviation from sigma 1 to sigma 3 to perturb our training data set. And clearly, if we have sufficiently large Gaussian noise, we can convert any data distribution into a single Gaussian distribution. For each noisy distribution, we can use a one-dimensional heat map to represent its density. So with more noise levels, we have more heat maps. In the limit of infinite noise levels, we will have a continuous two-dimensional heat map. So here we use pt to represent each noisy distribution, while t is a continuous parameter ranging between 0 and capital T. So here the capital T is a fixed constant. When t is a 0, p0 is always the same as the data because no noise is added at this moment. When t is capital T, p capital T contains a lot of Gaussian noise, and it will be close to a simple Gaussian distribution, which we denote as pi x. So suppose we are given this infinite number of noise levels. How can we generate corresponding noisy data sets for training our score model? Well, we use the idea of random of stochastic processes. So starting from the original data set, we can inject Gaussian noise smoothly to perturb our training data set. So the trajectories of those noisy images will form trajectories of stochastic processes. Recall that by the end of the perturbation, we obtain samples from this noisy data distribution p capital T, which is close to the simple Gaussian distribution pi x. So here the so-called stochastic process is a collection of an infinite number of random variables. Each random variable is indexed by the continuous parameter t. So for every random variable, there will be a corresponding probability density function. A stochastic process will then correspond to an infinite number of probability densities. If we find the right stochastic process, we can then use this stochastic process to represent an infinite number of noisy data distributions. So how to determine a stochastic process? We can solve a stochastic differential equation. So an SDE has the following form. F is called the deterministic drift function. This is is usually the same as what we would have in an ordinary differential equation. And we also have this infinitesimal white noise term. Wt is called Brownian motion. You can view dwt as infinitesimal Gaussian noise. So for illustration purpose, I will only consider simplified form for the SDE as given below. But our method can be applied to more general stochastic differential equations. So by finding the right stochastic differential equation, so yeah, by finding the right stochastic differential equation, we might also generate new samples by reversing this stochastic differential equation. So the procedure will look like below. So we start from random noise. We gradually remove the noise by tracing the original stochastic processes backwards in time. So given any forward SDE, for example, with a form shown below, we can always find the reverse SDE. And importantly, this reverse SDE depends on the score function. So here dwt is actually infinitesimal noise in the reverse time direction. But it is also a Brownian motion as long as time decreases from T to zero. So the entire framework will then look like this. We have an SDE to convert data to noise, and then we can reverse this SDE based on our score function to generate data from noise. The key question is how do we estimate this score function? We can use a time conditional score model, which depends on T instead of a sigma. But it is very similar to our original noise conditional score model. And we hope to optimize this time conditional score model to approximate the score function of our noisy data distributions. The training objective is quite simple. It's just basically a continuous generalization to what we had before for training our noise conditional score models. So here we have one score matching loss for every time step T. We have a positive weighting function lambda T to weight the scales of different score matching losses to help optimization. And here lambda T can be given using principle theoretical analysis or it can be given using empirical methods. So after training our time conditional score model, we can plug it into the reverse SDE. And then we can solve the reverse SDE with any numerical SDE method, any numerical SDE solver to generate samples from noise. So this approach actually generally generalizes noise conditional score models to time conditional score models. This approach also unifies diffusion probabilistic models into the framework of score-based genetic models. Now you can view diffusion models and the score-based models both of different discretization to the reverse SDE. So why we want to talk about this SDE formulation? Because this allows us to compute probability values very easily. We know that given the SDE, we can convert any data distribution to the noise distribution to the Gaussian distribution. The critical observation is actually the same thing can be done using an associated ordinary differential equation. So the trajectories of the ODE and the SDE look quite different from each other, but they all represent the same set of noisy data distributions as given by the headband in the background. And this ODE is critical for computing our probability values accurately. So for an SDE of the following form, we have shown that the ordinary differential equation can be given by the following formula. Here it depends on the score function only. And again, we can estimate this score function by training our time dependent score-based model. So after training this time conditional score-based model, we can plug it into the formula for our ODE to obtain the form for our ordinary differential equation. And now starting from some random noise vectors from our Gaussian distribution, we can solve this ordinary differential equation by crossing time. This can gradually convert our noise vectors into high quality samples from our score-based model. In the meanwhile, it also converts the original Gaussian distribution into a model distribution which we denote as P theta. And now I will show you that we can compute the value of P theta very accurately. So this depends on the following formula. In this formula, we connect log P theta on any data point x0 with the log Gaussian distribution and a one-dimensional integral that involves the trace of the Jacobian about time conditional score-based model. So this trace term can be computed in polynomial time. But if you want faster computation, you can always use random projections to form unbiased estimators of this term, just like what we did in the last score mention. This one-dimensional integral can be computed using an ordinary differential equation solver quite accurately. So using this formula, we can compute the value of log P theta on a data point x0 very accurately. And here are some existing results on real-world data sets. So we report results in negative log probability, which is better when lower. And the results are in the units of bits per dimension, meaning that what's the average number of bits for including each dimension of the data point if we use this probability distribution for lossless encoding, for lossless compression. And for images without any compression, it will be 8 bits per dimension. So here are some existing results to image data sets. And you can see that all existing results achieve bits per dimension that are lower than 8 bits per dimension. So this indicates effective compression. And here is the result of our approach. We achieved 2.83 on C510, which is quite close to the previous state of the art 2.8. We achieved a new state of the art 3.76 on ImageNet as well. So this says that score-based generative models can not only challenge the dominance of GANs on sample generation, but can also challenge years of dominance of autoregressive models and the variational autoencoders on achieving higher likelihoods on data sets. So now as a conclusion, we have shown that score-based generative models allow us very flexible models. They can generate a very high quality new data samples using a controllable generation procedure. And it can give us anchored probability values with better empirical performance. So with that, I'd like to thank you for being here in my presentation. And I'm happy to take any questions. Okay, great. Thanks for your presentation. Quite a lot of information for us. So for the people in the audience, you can ask questions by raising your hand or by typing the question in the chat that if you don't want to ask, don't want to ask yourself, you want me to ask on your behalf. Okay, so maybe I wanted to start the first question. Okay, so I mentioned that sampling from say the data distribution can be very difficult for the dynamics one, right? And tampering has been like a long-standing technique. I, for example, was dividing your energy with say T, right? So maybe can you please explain about the difference between this approach and your particular noise and perhaps talk about why sort of like the conventional tampering technique doesn't work? I see. So you are saying that we can also condition the energy-based models on noise by dividing by T, so using temperature to anneal the energy. Yeah, so basically you have to have two certain terms of annealing, right? One is adding noise, another is like divided by temperature, that's the temperature. Maybe can you tell me about the comparison and then say why it might not work? Yeah, so that's a great question. The major difference is when we anneal the data distribution by annealing noise, we can generate a noisy data sets quite easily. So we can get samples from that anneal the data distribution. But if we use the temperature to anneal the data distribution, we cannot generate samples easily from them. And that's the major difficulty. So the approach of annealing the energy with temperature is mostly useful when we have the energy itself already, but we don't have samples. So here the case is the opposite. We have samples, we don't have the ground truth data distribution. So we want to obtain samples from those annealed data distribution and anneal noise is the simplest way to do this. Okay, I see. I have another way, another problem is temporary. That's not a very efficient way to solve the problem of disconnected modes. Yeah, I think that's also a great point because anneal noise can actually connect different modes together, but changing the temperature may not always have this effect. Great. So there are some questions in the chat. Sixth question asks, how complicated is this whole pipeline implement and use as compared to GaN or VAE? Yeah, so this pipeline is definitely more complicated than GaN or VAE. So because the training is actually quite simple, but the sampling procedure requires implementation of SDE solvers or requires implementation of random dynamics. So it's a little bit more difficult than VAE solvents, but still I feel it's manageable difficulty. And people are more, it seems easier for people to implement GaN solvents, partly because people are so familiar with those concepts. So yeah, I feel with people who have that capability of implementing GaN solvents, it shouldn't be hard to implement this type of approach. Yeah, so that's my opinion. Okay, so next question. So Margarita asks, could you talk about the weakness of the solvents as compared to GaN and VAE? Yeah, so compared to GaN solvents, the biggest weakness is the first, the sampling speed is quite slow compared to those existing methods. So there are some recent approach that tries to improve the sampling speed, but still it's not comparable to the sampling of GaN and VAEs. And that's one major difficulty. Another difficulty is there is no natural latent space with a lower dimensionality here in our case. So it will be harder to obtain disentangled representations or doing some manipulation in the latent space compared to GaN solvents. But still we can actually have a latent space similar to normalizing flow models using the ODE formulation. It's just that this latent space will have the same dimensionality as the input space. So maybe it does not have that nice disentanglement. Okay, great. Okay, so Zhehua, if you want to ask the questions, this is the direct speak. Oh, hi. Hi, Yingzhen and Neang. Could you hear me? Yeah, I can hear you. Okay, I have two questions. The first one is about sample quality. Well, I have some experiences using diffusion models for speech waveform generation. Well, for speech waveform generation, now the SOTA models are GaN-based workholders. So the reason is that they use a lot of discriminators to improve the sample quality. The discriminators include the ones in time domain and time frequency domain. So for diffusion models in the training stage, we have only IL-1 or IL-2 regression laws. So although the generation process is a progressive method, it is a very strong method. But I think in my experiments, I think from the waveform sample quality, I still think the GaN-based workholders are better. So could I ask, is there any way to incorporate some other adversarial, I mean auxiliary losses such as adversarial loss into the training of diffusion models? I know that one way is that we can directly estimate the X0 from Xt and we can add adversarial loss as estimated X0. So do you think, is there any other method to improve the sample quality by using some auxiliary losses or other method? Thank you. Yeah, so I think there is a work by Alexia from Lila and she works on a similar project. So basically you can incorporate adversarial loss into the score matching laws as well. And this can sometimes improve the sample quality. And you can also just combine the idea of diffusion with GaNs. There is a work called denoting diffusion GaNs. Basically just use GaNs to do multiple denoise stage and dance a combination of score matching and adversarial optimization as well. So I feel in those cases, if you prefer GaN approaches, those methods might further improve the performance of dance on this task. Okay, thank you. And I have another question. Yeah, go ahead. For improving the sampling speed, many researchers have proposed methods to change the distribution of the noise, the noise distribution. So we can give some prior for the noise distribution. For example, we can change the mean of the variance of the Gaussian noise. But do you think it's better to use a noise which is similar to data distribution? I mean, for example, we can use a standard Gaussian distribution and we can also use a distribution which is very similar to data samples. But I know that this change will not bring any difference to the diffusion steps. We still need, for example, 1000 diffusion steps to guarantee we can transform the data distribution into our desired distribution. But do you think it's better to use a noise which is very similar to data samples or there is some trade-off between the, for example, one option is the data samples and data distribution and the other option is the Gaussian distribution? Yeah, so I think this is probably related to another question that changed. So basically, choosing a different type of noise distribution instead of using Gaussian, we can use some other distribution, maybe one with a heavy tail. So I actually think from a theoretical perspective, using a different type of noise other than Gaussian might not be optimal because the reversal is only possible when you use Gaussian distribution, otherwise the reversal won't be sent. So the special part about the Gaussian noise perturbation is when we perturb the data distribution with Gaussian, this means the reverse process can also be viewed as a Gaussian perturbation process. So if you change the Gaussian to something else, maybe a gamma distribution, then the reverse process may not be captured by injecting Gaussian noise or gamma noise. So that will complicate things a lot. So from a theoretical perspective, I feel it's probably not optimal to use a different type of noise other than Gaussian. So actually, even though we only use Gaussian noise perturbations, it's already quite flexible. So from the perspective of stochastic differential equations, SDEs always use Brownian motion as part of the driving stochastic process, but they can already represent very complicated types of noise, in addition to Gaussian noise, but also other things. If you change the drift function to be non-linear, then you can represent a lot of complicated things. So I feel using a different type of noise might have some improvement in practice, but from the theoretical perspective, I'm still a little bit doubtful about this approach. Yeah, because in my experiment results, I find that it's not always a good choice to change the noise distribution type. I mean, sometimes we can achieve better sample quality, but sometimes we can't. So yeah, thank you very much for your answer. Thank you. Hey, thanks a lot. So if there's no other questions, maybe let me end with the usual questions that we want to ask. So what do we add as an exciting frontier of research? Yeah, so for future direction, I think one thing is how can we improve the sampling speed? Even further, so there are some existing results, but still it's much slower than GANs. Can we make them as fast as GANs? This is one research question. Another research question is how can we use this line of approach for solving your post-inverse problems? How can we control the generation of those methods? So it's quite different from GANs the way is because there is no latent space, but it has its own advantage. It can combine on another component using the base true for score functions. So this might make it possible to solve many difficult problems that GANs and VAEs cannot solve. Yeah, and also another question is like theoretical understanding of why this line of approach can generate higher quality samples compared to existing approaches, especially existing likelihood-based approaches like VAEs or autoregressive models. Why we can significantly outperform them? This is also something we don't quite understand from a theoretical perspective. Okay, thank you very much Yang for giving us this great talk. So we learned quite a lot from your talk. Okay, so thank everyone. I hope you enjoyed this talk. Thank you for the",
        "output": "### Score-Based Generative Models: Definition and Advantages\n\nScore-based generative models are a new framework for generative modeling that work with score functions to represent probability distributions. They satisfy three main goals:\n\n1. They allow for very flexible models as the score function bypasses the intractable normalizing constants. This enables the use of flexible deep neural networks (referred to as score models) to model the score function, which can then be learned from data using principled statistical methods.\n2. These models can generate high-quality new data samples comparable to GANs and control the data generation process for important applications in inverse problem solving.\n3. Score-based generative modeling can be used to compute accurate probability values, demonstrating better empirical performance than other methods.\n\n### Estimating the Score Function\n\nScore functions can be estimated from data using a method called score matching, which compares two vector fields of score functions (one representing the ground truth score function of the data distribution and the other given by the score model). By minimizing the feature divergence of the two vector fields, the score function can be accurately estimated. The score matching loss function aims to address the difficulty of estimating score functions in low data density regions by incorporating the right amount of Gaussian noise to perturb the dataset. This results in learned score models being accurate across different noise levels.\n\n### Generating Samples with Score-Based Generative Models\n\nTo generate new samples, the Langevin dynamics algorithm is used to move data samples, following the noisy score functions while injecting the right amount of Gaussian noise. In practice, a large number of different noise levels are used to jointly perturb the training dataset, and the noise conditional score model is trained to estimate the score function for each noise level. Langevin dynamics is then run iteratively over these noise levels to generate new samples from the learned score functions.\n\nScore-based generative models have been shown to generate high-quality samples, making them a promising framework for various applications, such as medical image reconstruction, outlier detection, and lossless compression."
    },
    {
        "instruction": "<|SYSTEM|>You are a scholars that is taking markdown notes about some video, you would be giving some written scripts for the task.\nYou have finished writing the notes, and you want to extract the title from the notes.\nThe title should only be the name of the technology or algorithm, no extra words needed.\nPlease write the notes in markdown format. No need to write \"Title:\" for the Title. Make sure the title is within 10 words.\nHere is an example:\n## Hidden Markov Model\n<|ASSISTANT|>## ",
        "input": "<|USER|>### Score-Based Generative Models: Definition and Advantages\n\nScore-based generative models are a new framework for generative modeling that work with score functions to represent probability distributions. They satisfy three main goals:\n\n1. They allow for very flexible models as the score function bypasses the intractable normalizing constants. This enables the use of flexible deep neural networks (referred to as score models) to model the score function, which can then be learned from data using principled statistical methods.\n2. These models can generate high-quality new data samples comparable to GANs and control the data generation process for important applications in inverse problem solving.\n3. Score-based generative modeling can be used to compute accurate probability values, demonstrating better empirical performance than other methods.\n\n### Estimating the Score Function\n\nScore functions can be estimated from data using a method called score matching, which compares two vector fields of score functions (one representing the ground truth score function of the data distribution and the other given by the score model). By minimizing the feature divergence of the two vector fields, the score function can be accurately estimated. The score matching loss function aims to address the difficulty of estimating score functions in low data density regions by incorporating the right amount of Gaussian noise to perturb the dataset. This results in learned score models being accurate across different noise levels.\n\n### Generating Samples with Score-Based Generative Models\n\nTo generate new samples, the Langevin dynamics algorithm is used to move data samples, following the noisy score functions while injecting the right amount of Gaussian noise. In practice, a large number of different noise levels are used to jointly perturb the training dataset, and the noise conditional score model is trained to estimate the score function for each noise level. Langevin dynamics is then run iteratively over these noise levels to generate new samples from the learned score functions.\n\nScore-based generative models have been shown to generate high-quality samples, making them a promising framework for various applications, such as medical image reconstruction, outlier detection, and lossless compression.",
        "output": "## Score-Based Generative Models"
    },
    {
        "instruction": "<|SYSTEM|>You are a scholars that is taking markdown notes about some video with Wiki style, you would be giving some written scripts for the task.\nAnd summarize each paragraph with a title.\nRemember to keep the technology or algorithm name, very important.\nPlease write the notes in markdown format.\nHere is an example:\n### Definiation\nA deadlock is a situation in which two computer programs sharing the same resource are effectively preventing each other from accessing the resource, resulting in both programs ceasing to function. \nThe earliest computer operating systems ran only one program at a time.\n\n### What it have to do with Relationsal Database ManagementSystem\nIn a database, a deadlock is an unwanted situation in which two or more transactions are waiting indefinitely for one another to give up locks. \nDeadlock is said to be one of the most feared complications in DBMS as it brings the whole system to a Halt. \nWhen you tries to insert or update data on database, sometime your record need to depend on the row record.  \n<|ASSISTANT|>#### ",
        "input": "<|USER|> Hi everyone. It's my great pleasure to introduce our keynote speaker today. Johannes Bale is a staff research scientist at Google. Johannes received their doctoral degree at RWTH Aachen University in 2012. This was followed by a collaboration with Javier Portilla in Madrid and a postdoctoral fellowship at New York University with R.O.P. Cementelli studying the relationship between perception and image statistics. While there, Johannes pioneered the use of variation, creation, or Bayesian models and deep learning techniques for end to end optimized image compression. Johannes joined Google in 2017 to continue working on this line of research. Johannes has served as a reviewer for numerous publications in both machine learning and image processing. They have been active as a co-organizer of annual workshop and challenge on learned image compression since 2018 and on the program committee for data compression conference since 2022. Please give Johannes a warm welcome. So this talk is about learned image compression and I want to start with this example here. So in case you don't know about learned compression, this is a result that we got and one of our first models that we worked on in 2017. And what really struck me back then was the way that these learned compressors adapt to the data to the source. And in particular, if you compare the reconstruction here with the reconstructions you get from traditional coders like JPEG, JP 2000, you can see that they're essentially free of artifacts. So you have a rate constraint, you do lose detail, but somehow these compressors learn how what an image looks like and they reconstruct images that are plausible. Whereas traditional methods, you can see what type of transform they're using in the way that they show artifacts. And in particular, if you take a look at this contour up here, what really surprised me was that there is this little corner cut out of that contour and the rate isn't enough to represent that in the bit stream. But somehow the model kind of made up a contour that makes sense in this context. And so that is when we realized that there's really something special about the way that these models work. So machine learning really has kind of revolutionized a lot of fields, and it also has changed some of the things that we can do in compression. In particular, it's very data driven, so it's very easy to adapt it. Both two types of sources. So in the best case, if you want to train a compression model for a different type of source, you change the training data in the simplest case and you will get an adapted compressor for that type of data. And the same is true for how we measure success in terms of the reconstruction. What is how do we measure errors so we can plug in different types of error mode error metrics. And this is kind of what the talk here is about. We can do a lot better job at emulating human perception. So first I'm going to talk a bit about learned image compression, what it is, what sort of give a quick recap of that. Then I'm going to talk about distortion, which is, I think, the sort of most common way to frame how good is an imagery construction. But I want to expand a bit on that and also talk about what I call realism. And at the end, I'm going to talk about perceptual spaces, which is a concept that applies to both of them. So I probably don't need to explain this diagram very in very much detail. I think everyone here knows what JPEG is. So we have an image X here. We subjected to the DCT. We do quantization. We do entropy coding for the reconstruction side. We use the inverse DCT and then we can quantify the performance of this model, both in terms of rate and in terms of distortion. If you ask the question, like, why do we actually use the DCT and not something else? Well, it goes all the way back to the 70s where we were conceptualizing images as Gaussians. And under that assumption, the correlating transforms the KLT and it's optimal in that case. So if we make certain assumptions, we can prove optimality. And the DCT just happens to be a really fast approximation of this KLT. So that is why it became really popular and we're still using variants of it today. But of course, images are not really Gaussians. So. We cannot be sure that the DCT or KLT is actually doing the best possible thing here. And it's actually very likely that the optimal transform that we could use is actually a nonlinear transform. And this is where artificial neural networks come in because they are universal function approximators. So if we can train them, then we can potentially approximate the already optimal transforms with those networks. So this is the adaptive block diagram for what we did in 2017. So now, instead of having DCT and inverse DCT, we have two neural networks. Because neural networks, they subsume companding or compression. So we can, without loss of generality, just use a uniform quantizer. We don't need to worry about dead zones or anything like that. And then we just used arithmetic coding for coding the quantized coefficients. Now that raises the question. We have a bunch of parameters here, so both the analysis and the synthesis transforms our networks with parameters. Also, the entropy model has parameters. How do we get these parameters? And so we follow an approach that is very common now in deep learning, machine learning. We write down a loss function. Here, the loss function is a delagrangian between rate and distortion. So in this case, we use mean squared error. So we have an expectation over the image of a squared error. And here we have a cross entropy term. And we can write it all out so that it becomes visible what the parameters are. So we expand this. And now we have the transforms here, a quantizer, and then the cross entropy term. And here we have both transforms. So now we have a function that is the loss function. It is a function of all these parameters. And we use stochastic gradient descent to optimize this loss function. And stochastic gradient descent is different from gradient descent in the sense that we're using only small batches of samples in each step. So we don't have a very large batch size. We go to very small batch sizes, and that works because we can do a Monte Carlo approximation of the gradients. And so here in this equivalence, we can just exchange the gradient with the expectation because they're both linear operations. So this is all borrowed from machine learning, deep learning. And lastly, we have tools there as well that help us take those derivatives. You can imagine that the derivatives are inevitably very complex. And so we have toolkits that allow us to take these symbolic differences. Now there's one problem, which is the quantizer. So for stochastic gradient descent, we need gradients. We need to take the derivatives. And since we have quantizers in that loss function, we're going to run into trouble at the point where we try to take the derivative of those because the derivative is going to be zero except in singular points where it's infinite. So that's not a useful gradient that we can use for optimization. So the approach we took here was to transition from uniform quantization to digit quantization. And digit quantization has the nice property that for purposes of the conditional distributions and the expected value, we can just replace it with an additive uniform noise. And that gives us the same computation effectively if we're not trying to actually code it. And it is differentiable. So that allows us replacing going to this proxy Rd loss, which doesn't use quantization, but dithering that allows us to actually optimize the system. There's a technique that has been developed recently that interpolates between quantization and dithering, which is also very nice and that allows us to control this a little bit better now. So what does this look like if we actually apply this to some source? So this is a visualization of a two dimensional source. We have kind of a rich shape like a banana here, so we have a lot of probability mass here and then out here there's almost no probability mass. And what we did here was train linear transform coder with this paradigm. So linear transform coder is essentially a lattice coder, as you can see here. So we have the lines represent the quantization boundaries and the dots represent the reconstruction values for the respective quantization bins. And so you can see that the transform kind of adapts to the source somewhat because it approaches the sort of both fleeting angles. But of course, it's constrained in the way that it can sort of break up the space and we get a certain performance here. So we recorded the value of the Lagrangian as an average over samples from this source. If we do linear transform coding, you can see that it's still a lattice. So we still have a rectangular lattice because we're doing scalar quantization in the transform domain, but it's now a warped lattice. And so that's due to the non-linearity of the transforms. And you can see that that adapts to the source a lot better. Now we have like one dimension here that essentially goes along the ridge and another dimension that goes across the ridge. And so this type of adaptation is the same adaptation that we saw in the image in the beginning, where it adapts to the shape of the source distribution. And so comparing that with with a rate constraint vector quantizer, we can actually see that they get very close. So if you want to, if you believe me that this vector quantizer is somewhere close to the optimum, you can see that the difference in terms of performance is not very much. Not that much. So by allowing you to warp the space in a nonlinear way, we actually get much better performance out of it. So over the last couple of years. There had been a couple of limitations from the first instance of this type of model. One was that we had only one model per rate distortion operating point. So as you can appreciate in the loss function, we have we have to set this lambda right. And so if we train for this lambda, we get one model that compresses for this rate distortion trade off. And then if we wanted to have a different trade off, we needed to set a different lambda. We get a new model. So this has been addressed in the meantime. And then also we in terms of PSNR, we were better than JPEG 2000 at the time, but we were not competitive with more state of the art methods. And there we've also caught up. So we're we're in roughly the same ballpark. And most of it is because we introduce better entropy modeling. So. Many of the improvements come from a model that we call hyper prior model, which is essentially a forward adaptive entropy coding model where you take the legends, you extract some information from dolls again with the neural network that gives you a percent representation. And there's some some extra information that we send first. And then this information is decoded again with neural networks that predict the distribution of the actual coefficients. So this is an example of forward adaptive entropy coding. You can also do neural backward adaptation. So there's multiple works now in that direction where people experiment with different types of entropy models. And you can see the progression here, so we started somewhere. So this is JPEG. We started down here in 2017. Then we introduced the forward adaptive entropy model. And then here we started combining forward and backward adaptation. And now there there's a number of different models that all experiment with this with improved entropy coding, improved transforms and so on. So we're now relatively close. We're sort of the same ballpark as some of the state and the state of the art intracodex. So one big question, though, is still computational complexity. If we. So typically we have a different trade off between encoder and decoder, right? Because we have transform coder that is now much more complex than a DCT. We tend to have a big part of the complexity in the encoder in the network and then also in the decoder. So encoder and decoder complexity are typically kind of similar. Whereas in hybrid coding we have a much more complex encoder. But we can do some comparisons so we can look at a slice here. So if we look at all the models here that. Average to a certain number of bits per pixel, I think point five here. Then we can make a new plot and look at. Versus the number of floating point operations that they use. And so now we can see sort of the 2017 model was relatively cheap, still much more expensive than traditional coders. But then we can see that there's sort of a trade off emerging right? And now we're in this phase where we're starting to trade off complexity. And then we can see that there's a lot of different models that are coming out. And now we're in this phase where we're starting to trade off complexity. With more sophisticated ways of entropy coding and so on. David Minnan has given a lot of a lot more detail in this direction and his ICIP 2021 keynote. So I'm not going to go into a lot of detail here, but that's definitely one of the open questions that is still seeing a lot of active research. And so what I'm trying to focus on in this talk is subjective image quality, because we haven't really. We've seen that some of the results look really compelling, but we haven't really understood. Sort of what are the trade offs here completely? And one of the early results that we had. Was comparing a network that is optimized for squared error versus a network that is optimized for MSSAM. So here we're doing a comparison between two images that were compressed with different compressors. Both happen to have the same bit rate, and now you can compare those two reconstructions. And I don't know how well you can see it, but you should be able to see that. With MSSAM, there's a lot more detail in the face and in the in the texture here, and that is because. MSS the MSSAM coder moves rate moves bits from the background into those areas, so it affects the bit rate allocation. We see the same here, so here's a comparable example again. We see the same here, so here's a comparable example again. This is optimized for mean squared error and then this one is optimized for MSSAM. You should focus on the text in the center and the grass texture. There when we go to MSSAM, a lot of the bits are moved away from the text and into the grass texture. And that's because MSSAM models contrast masking. So basically it means great error will highly penalize errors here because we have a wide dynamic range of pixel values. Here is white to black, whereas the grass is low contrast, right? And so MSE will penalize this a lot, but deviations here are not penalized much and MSSAM compensates for that. So that's why we're seeing that bits are being moved around. And so an interesting observation here is that the rate allocation decisions are not explicit. We they're essentially amortized into the network and we only need to apply like a learning signal during training to tell the network. Where do we penalize distortions? And so. We don't need to have a distortion metric that is fast or we don't need to have a distortion metric that can be evaluated on single blocks. We have a lot fewer constraints in terms of what distortion metrics we can use. And I wrote here we can use perceptual models rather than distortion metrics. And so we can actually generalize beyond distortion metrics. What do I mean by perceptual models? Well, we have image quality assessment models and these are all models that are trying to emulate humans on certain tasks. Like we can show an image, we can ask, does that image look realistic? We can show two images, ask do they look the same? So there's a number of different ways we can query humans for image quality assessments. And I think this list here is not exhaustive, but there is, for example, we can talk about two alternative force choice. So we basically ask the humans to give a binary answer or we can talk about mean opinion scores. We can ask on a scale between one and five how good does this image look and et cetera. What I want to focus on here is a distinction between showing two images and comparing them. So a full reference metrics scenario or I would call this distortion. That versus a scenario where you show one image and so you have a no reference scenario and that is what I would call it realism. We're asking if this image is realistic without a reference. So first, let me talk a bit about distortion. So what do we need if we want to optimize a compression model for distortion? So what do we need if we want to optimize a compression model for distortion? Well, first of all, we want it to be a good model, right? So we wanted to predict human ratings. Well, we want it to be differentiable and well defined. So we need it to be differentiable because we have used to cast a gradient descent. We want some well definedness. For example, we want to have it be a minimum if we're putting the same image in twice. And then we want it to generalize across different images and types of distortions. So what does that mean? Generalize. So here's an example, an experiment that kind of demonstrates what this means. So a lot of distortion metrics actually are really bad for optimization. They have blind spots and this paper here, people did an experiment where they just wanted to see how well can you optimize for a given metric. So this doesn't even include any compression. All they did was they took an image that they initialized to noise. So like up here in the corner. And then they iterated on this image, optimized it to minimize the distortion between it and some reference image. So in this case, it was like an image of a church. And so in these images here are the results or different types of metrics. And this is quite surprising that not all of them actually give you something back that even remotely looks like the original image. So some of them produce essentially, it looks like garbage. They produce some stuff and apparently the metric says, well, this looks close enough to the original image. What does that mean? Why does it? Why do they need to generalize? Or the difference here is that we're optimizing for it rather than just evaluating it. So if we do quality assessment, a lot of these metrics are used for quality assessment after the fact, after we have artifacts in the image. And so when we evaluate them, we evaluate them on a particular type of distribution, which is the joint distribution of natural images and the distribution of distorted images. Right. And so this distribution of distorted images will only contain a certain type of artifact. So we're evaluating it in a certain part of the space where the type of artifact is determined by our compression scheme. So if we train a model, then we need to expand that domain. Right. Because a neural network, as it learns to produce reconstructions, it will explore the space. It will start going in all kinds of directions and produce reconstructions that could be whatever. And we need the distortion metric to tell us, OK, we need to go in this direction. And that is not the case for a lot of these metrics here. Why do we find out about that now? Well, it's because we are also we also have blind spots in our data sets. So, for example, the TID live and CSIQ data sets, very popular, very well collected data show images to humans. You collect the responses about similarity. It's all great, but they don't contain certain types of distortions. So we only evaluate the distortion metric again on these types of distortions. There's now a couple more data sets cropping up that actually have different types of distortions like these type of structural distortions here where you have more pinks and so on. But we're still quite limited on data sets that are good in actually evaluating distortion metrics. So it's not looking that great right now. We need a lot more work on on good distortion metrics, and if we want to use learned compression and optimize these compressors for such metrics, we need to be aware that neural networks will cheat. They will go to parts in the space where the distortion metric is not giving useful answers. OK, so we have a couple more data sets cropping up that actually have different types of distortions. OK, so how can realism potentially help with this? So here I'm looking at no reference metrics, which some of you might be familiar with, and I'm trying to give it sort of a more idealized notion, and I'm connecting with the notation that is used in a paper in the paper. So what's going on here? So idealized in an idealized way, we could say a critic, you know, a human or a model that tries to determine if an image is realistic. You could say it takes that image. It has a model of the natural image distribution that's PX up here, and it has a model of the distribution of reconstructed images or distorted images. And then it could do a likelihood ratio test between them, right? It could look at the likelihoods of both, compare them and then take a decision based on that. So in a realism type of scenario in a no reference metric. The critic here actually needs to learn a model of the distribution of artifacts, so it's it's actually that is actually what a lot of new reference metrics do. They are designed to detect a particular source of artifacts. So, for example, there are there are no reference metrics that detect block artifacts specifically super useful for hybrid coding, right? Because you know that the artifacts will have a certain type and you can detect that you can detect what how likely it is that you see an artifact. So that means here also we have a generalization problem. But there has been so that we have a new twist on this type of model actually through generative adversarial networks and these GANs take sort of this type of decision sort of to a next to the next level in the sense that. That they make it make it an optimization game, so we have a game between a critic that tries to guess if an image is fake or real and a generator generative model that produces the images done here. So essentially the generator tries to produce images that look natural that fool the critic and the critic learns to classify images into real or fake and thereby it's learning this likelihood ratio in different parts of the space. So this is also how some people define realism. We take this likelihood ratio and if we take the expectation over the image distribution, now we have a divergence, right? We have divergence between two distributions, natural image distribution and the distribution of the degraded images. So, for example, if we take this notation is due to Novacine, so they have formalized this and expanded it to F divergences. So if we replace F here with our log are then that becomes the Kovac library divergence, but we can also make other choices here. The bottom line is that if we. Participate in this adversarial game during training. That is one way to achieve better realism, so we can achieve better realism and not necessarily need to rely on fixed no reference metrics. So why do we want? Why do we want realism and not distortion? Why do we still want those distortion if we have realism? Why are both necessary? You know, do we maybe just need one? So this is actually a good example, so we're trying to. Trying to. We're trying to compress an image and now we have different models that are trained for either rate and distortion or rate and realism or rate and both. So if we train for rate and distortion. We're likely to get a reconstruction kind of like this. So we have a rate constraint, so that means we cannot. We cannot send all of the information in the image, so something has to be cut and that typically means that we get a slightly blurry reconstruction because the high frequency detail is going to be lost. But you can also say you know this you can detect this is not a realistic image because it's blurry all over right? So if you just see this image and you ask a human is this a realistic image? It might just be very blurry, so they might say no. What if we only train for rate and realism? So here. We might actually satisfy the requirement with any image right so we can we can have a decoder that produces just any realistic looking image and the realism constraint will be happy. But of course it will not have anything to do with the original right? So this is also not what we want. We need both, but both things are also kind of at odds with each other. So here we're kind of trying to train for all of them and now we hope to get a reconstruction that. Resembles the original, but also has some additional detail that makes it more look more realistic. There's a very nice paper. That was published at CDPR in 2019 that actually shows that there is a three way trade off, so for any given model that has a constraint like a rate constraint. We cannot be perfect in both in all three of them, so we cannot reach so that there's a three way trade off between rate distortion and realism. So this is another type of model that has become popular now where we have the same setup as before, but now instead of training for rate and distortion, we have a three way model. That approximates this divergence between the natural image distribution and the reconstructions. Approximates because it's not. It's unfortunately not a one to one correspondence, so there is an approximate relationship between these things. OK, and so here I'm going to show some results of a model like this. So I think you can see it here. This model produces some. Texture here. That is not in the in the reconstructions from the distortion only models. You can see that there is a little bit of a difference in the texture here. That. Is not in the in the reconstructions from the distortion only models. You might also notice that this texture so it looks more realistic, but it's not the same texture as in the original right? So it hallucinates some detail. That just sort of statistically looks like it's in the right place, and that's because it still has to work with the rate constraint right like you can't send the information. Pertaining to this instance of the texture, it kind of has to make up some of the detail. And we see the same in different images, you know. Here again we have. I think this is the original and then here we have reconstruction with this adversarial model. Whereas here in the distortion only model we get just mostly blurring, especially in the areas that are low contrast because this wasn't this is a model that is optimized from square error. So we did human studies on this model and it is generally preferred. So humans prefer the reconstructions that have been optimized with realism. This is showing too much information and just going to go a bit faster here. We do have some cases where. Which we would consider failure cases, and that is particularly in the situations where humans are extremely sensitive. So for example, in a in an image like this one, the face covers the entire image, so it's more like a global structure and there we don't have a problem. But if we look at images like this where we have relatively small faces, humans are still very attentive to these details, right? More so than attentive to the details up here, even though it's the same scale, right? Humans are extremely sensitive to cases, and so the model here is very similar to the image. Humans are extremely sensitive to cases, and so the model here treats it the same as any other texture and it just produces something that is kind of flesh like. So that's not ideal for your vacation photos, right? So why is this? Because. We're applying these realism and distortion constraints in a pixel representation, right? So we're trying to match distributions of pixels. And humans human vision doesn't really work in pixels, right? We have a lot of processing going on and many of the features that we attend to cannot be sort of adequately captured in a pixel space. And this leads me to the fourth part where I'm going to talk about perceptual spaces. So as I just said, the brain needs to process the information that's coming into its senses, right? And for the most part, that means we need to filter out information because we have so many. We have millions of cells in our eyes alone and like processing all of that consciously is not possible, right? So we need to rely on the machinery that pre process all of this and filters out the irrelevant bits. And that happens on all kinds of levels. So, for example, it happens in the eye itself because we only have we only have sensitivity to certain wavelengths, right? It happens in. Parts of the visual system that can compute things automatically pre attentively, so we might have masking effects. If there is a certain amount of stuff in a certain area, you don't see other stuff. It just kind of gets masked. And then we can also have influence from cognition. So where we direct our attention makes a big difference in terms of what we perceive. So low level example would be the fact that we have three or four, depending on how you count different types of photoreceptors in the eye. And our eyes are exposed to all these wavelengths and power spectral mixtures of wavelengths, but we don't perceive all of that. We just see a projection. We see a response of these four different types of cells to this mixture of wavelengths, right? And so that is one way in which we can see the different types of cells. Right. And so. That is one way in which we use the physiology to actually filter out information, and this is really easy to forget about because we have already built this kind of stuff into all our tech, right? So displays, cameras, even illumination now with LED lights. It like we wouldn't be able to have this kind of tech if we were actually able to distinguish all these different types of wavelength spectra. And then here we have an example from the other end. So here this is a more cognitive example where if you look at these two images on first glance, maybe you think they're the same. But then as you direct your attention to different parts of the news, scrutinize parts of the image, you can actually appreciate that there is a difference. And so that is one example where cognition has an effect on perception. Right, so it's a top down kind of thing, and that's of course even harder to model than the low level stuff. So how do we deal with this? So many image quality assessment models actually try to emulate this in a way. So they use proxy representations, which is essentially another transform. We're going from a stimulus space or a signal space. We're taking, you know, we're thinking of our image or audio signal as a vector in a space and we're applying a transformation that goes into a different space. And we might want to call it perceptual space. If it satisfies certain requirements, right, we would like to have. That if we take a distance between two points and then take the distance of the transformed points, we would like that distance to correspond to human notions of how similar these stimuli are. Right, so ideally points that have a similar distance in this space look roughly similar to the same amount. Or we could also frame it in terms of just noticeable differences. So if you think about the set of stimuli that are just noticeably different from that point in the center in a stimulus space, it might be some weird curve. It might be an ellipsoid in the perceptual space. We would love to have it be a circle, right, or a sphere, because we would like to be able to make predictions in that space with simple metrics. So the goal would then be to expand this flow chart and instead of measuring our distortion and realism in directly in the signal space, we first apply a transformation and then measure the measure those errors here. The one early example of this is from the 1940s, where McAdam ran an experiment and they showed humans these screens of flat colors. And then they varied the colors and tried to assess how easy is it for people to distinguish two colors? And it turns out that if you draw these just noticeable differences as ellipsoids in the chromaticity diagram, you can see that in the green area, we're much less sensitive than in the blue area, for example. And so now we have color spaces and the color, the design of the color spaces takes this into account. We're trying to design color spaces to make these ellipses into circles. For a more modern example, there was an experiment conducted at MIT in around 2016 where we were trying to design ellipses in the blue area. Where people took an object recognition network, but they trained it on a image net database or something like that. And then they took recordings from cells in the image net database and then they used it to create a new image network. So they trained it on a image net database or something like that. And then they took recordings from cells in the visual system. So they showed images to both and then looked at what the activations are in the different parts of the visual system and compared them to the activations in the neural network. And it turns out that there is a correlation. And more specifically, they found that early stages in the visual system correlated with early stages in the object recognition network and later stages correlated to later stages. So that raises the question, can we maybe use this as a perceptual space? And that's exactly what some some work does. So here one example is LP IPS, which does exactly this. So they train a neural network to do object recognition. And then they treat the activations in certain layers as their perceptual space, so they compare images in those spaces. And so that works remarkably well, even for structural distortions like these war pigs. Now, here's an experiment that. We conducted a while ago because we asked ourselves, you know, these experiments still require a lot of human ratings. We still need to have humans annotate class labels. There's actually also adaptation layers, so sometimes they then fine tune these models on human ratings of quality. So you collect all this data. You still need to ask humans for data and it can be very expensive, especially once you want to make this scale this bigger. So we asked ourselves, can we can we come up with a model that doesn't rely on data that maybe just doesn't get along without actually asking humans for ratings? And so here we relied on some concepts from computational neuroscience, in particular the idea of slowness. So this slowness idea is kind of a it's an abstraction of how the brain processes information and what information is relevant versus irrelevant. What do you tell on this? So if we we have a if you look at a video, right? So this is supposed to be a video. There's a person speaking and we think about what is the what are the activations in our brains going to be as we perceive this scene? Then if you look at a retinal image, right? So look at a particular cell in the written off. It's going to be a retinal image. Right. So look at a particular cell in the written off. It's going to be all over the place, right? Because we look at different things, things move around. There's a bit of jitter. So everything, you know, the retinal image will be very quickly fluctuating over time. If we look at something higher level, so if we look at a part of our brain where we process the facial expression of that person, then those kind of features will be fluctuating a lot slower, right? Because just because the person has a certain facial expression, independent of how their head is oriented and so on, like there's a lot of variability that is then filtered out. And so the relevant features here, the progress much slower, slower over time. And then if you think about things like object presence, so is there this mirror in the image? That's going to be even slower, right? So we have like a feature hierarchy where the less relevant nuisance features tend to fluctuate very quickly. And the important ones, the semantically relevant features tend to fluctuate slow. So we try to build. Oh, sorry. So you can appreciate that for image similarity. You compare these two images, they tend to look exactly the same, right? So there's if you just glance at them, it's very hard to see a difference. But if you take the difference image, you can see here that there is actually quite a bit of difference. So there's there has been some global motion in there. So we see a lot of the edges and also there's some motion here in these textures. So here we see they look the same, but they're actually different instances of the texture. And so these were these are two frames that were taken within a second of each other. And so that's kind of the idea here that. Within a short time frame, a lot of things change in the image, but mostly they're not perceptible. So we built a model, we did an experiment where we learned a representation of an image. Where we try to formalize this idea and we built it on top of information theoretic ideas. So we looked at videos and took some videos from YouTube. Public ones, and we extracted patches of neighboring frames in neighboring in time, right? And we tried to construct a representation that only contains the information that is present in both, but not information that's particular to one of the frames. So you can set up a learning objective. Don't want to go into too much detail here because that takes a long time to get into detail with. But essentially there is there's a neural network that takes an image and produces a representation in some other vector space. And then there is a another network that takes both images and produces a representation. So this joint encoder here, as we call it, is only necessary during training. And then once we're done training, we're going to use this marginal encoder here. So that is going to be our transform that's going from an image to a perceptual space. I have a visual explanation for how this training works in a second. So the architecture of this encoder looks like this. So we have a front end that is a multi scale filter bank. Multi scale processing is very, very helpful. And it also has its correlates in the visual system. And then we have neural networks here that sort of are stacked on top of each other and produce a representation. The multi scale representation. These are tensors. And essentially it's like a combination of a neural of a conf net with a multi scale filter bank. And this whole thing is a multi scale filter bank. And this whole thing is trained with a contrastive loss. So what that means in a nutshell is we're feeding the model with examples of frames that are from neighboring. Examples of patches that are from neighboring frames. So, for example, these despair here. And we have a loss function that encourages the representation of both of these images to get closer together. Then we take one of those images and we grab a random other frame and we combine those into a pair. And then we encourage the model to produce representations that are sort of repulsed, that are further apart. That's where the name comes from, like contrastive learning. We're giving negative and positive examples and telling the network to adjust the representation accordingly. And then so once we're done training, we take this encoder and then we can define our distortion and our realism in that space rather than directly operating on pixel values. And we find that surprisingly this actually gets competitive results on this. So we can use this BAPPS data set where we have structural distortions and we are predicting human raters with an accuracy that's very close to this other model that I showed earlier that is based on classifiers. So we can produce models. That are perceptual ratings that emulate perceptual ratings without actually ever using a human rating. And so this is where my talk ends. I think I'm good in time. You have maybe time for a couple questions. To wrap up, we need generalization. We need better metrics for optimizing compression models. And that will be an area where we will need to see a lot more work. Some of the ingredients might be that we want to trade off between distortion and realism. It's important to have a lot of data. We want to trade off between distortion and realism. It's important to ask not just point-wise questions. Do these images look the same? But we also want to ask people, do they look realistic? That is, by the way, something that has been adopted already a long time ago in audio and speech processing. We don't just ask, does this sound the same? But we actually ask people, is this understandable? Can you understand the speech? And so on. So we want to ask more sophisticated questions about the quality of the reconstruction. And I believe that some of this will be also helped by developing better perceptual spaces. And to that end, we might want to shift from things like color spaces where we model psychophysic experiments or take models of cells and then try to transfer them into a perceptual metric. Rather than that, maybe we want to abstract a bit and model brain behavior. So we want to ask the question, what is the brain learning and can we make a machine learn that? And then hopefully get a representation that is comparable. So I want to thank my collaborators. This has been a summary of a lot of work with different people. They're not even all listed here. And here are some references that you might want to look into.",
        "output": "#### Learned Image Compression Recap\n- Learned image compression models use neural networks to encode and decode images\n- They are optimized via stochastic gradient descent on a loss function that balances rate and distortion\n- MSE and MS-SSIM as optimization targets affect the rate allocation decisions within encoded images\n\n#### Improved Performance over Time\n- Many new approaches experimenting with improved entropy coding and non-linear transforms\n- Computational complexity remains an open and actively researched question\n\n#### Distortion Metrics for Image Composition\n- Good distortion metrics predict human ratings, are differentiable, and generalize across images and distorted types\n- Many popular distortion metrics have blind spots and may not optimize well for neural networks\n\n#### Realism vs Distortion\n- Realism focuses on no-reference metrics, which evaluate an image without comparing it to a reference image\n- Data sets with a wider variety of distortion types are needed for evaluating distortion metrics\n- Neural networks may cheat by exploiting blind spots in distortion metrics, so better metrics and understanding of realism are needed\n\n#### Perceptual Spaces\n- Concept that can be applied to both distortion and realism\n- Can encompass various approaches such as generalized convolutional neural networks, learned perceptual image patch similarity (LPIPS), and more\n- Realism, distortion, and perceptual spaces are interconnected, and improved understanding of these aspects can benefit image compression models"
    },
    {
        "instruction": "<|SYSTEM|>You are a scholars that is taking markdown notes about some video, you would be giving some written scripts for the task.\nYou have finished writing the notes, and you want to extract the title from the notes.\nThe title should only be the name of the technology or algorithm, no extra words needed.\nPlease write the notes in markdown format. No need to write \"Title:\" for the Title. Make sure the title is within 10 words.\nHere is an example:\n## Hidden Markov Model\n<|ASSISTANT|>## ",
        "input": "<|USER|>#### Learned Image Compression Recap\n- Learned image compression models use neural networks to encode and decode images\n- They are optimized via stochastic gradient descent on a loss function that balances rate and distortion\n- MSE and MS-SSIM as optimization targets affect the rate allocation decisions within encoded images\n\n#### Improved Performance over Time\n- Many new approaches experimenting with improved entropy coding and non-linear transforms\n- Computational complexity remains an open and actively researched question\n\n#### Distortion Metrics for Image Composition\n- Good distortion metrics predict human ratings, are differentiable, and generalize across images and distorted types\n- Many popular distortion metrics have blind spots and may not optimize well for neural networks\n\n#### Realism vs Distortion\n- Realism focuses on no-reference metrics, which evaluate an image without comparing it to a reference image\n- Data sets with a wider variety of distortion types are needed for evaluating distortion metrics\n- Neural networks may cheat by exploiting blind spots in distortion metrics, so better metrics and understanding of realism are needed\n\n#### Perceptual Spaces\n- Concept that can be applied to both distortion and realism\n- Can encompass various approaches such as generalized convolutional neural networks, learned perceptual image patch similarity (LPIPS), and more\n- Realism, distortion, and perceptual spaces are interconnected, and improved understanding of these aspects can benefit image compression models",
        "output": "## Learned Image Compression"
    },
    {
        "instruction": "<|SYSTEM|>You are a scholars that is taking markdown notes about some video with Wiki style, you would be giving some written scripts for the task.\nAnd summarize each paragraph with a title.\nRemember to keep the technology or algorithm name, very important.\nPlease write the notes in markdown format.\nHere is an example:\n### Definiation\nA deadlock is a situation in which two computer programs sharing the same resource are effectively preventing each other from accessing the resource, resulting in both programs ceasing to function. \nThe earliest computer operating systems ran only one program at a time.\n\n### What it have to do with Relationsal Database ManagementSystem\nIn a database, a deadlock is an unwanted situation in which two or more transactions are waiting indefinitely for one another to give up locks. \nDeadlock is said to be one of the most feared complications in DBMS as it brings the whole system to a Halt. \nWhen you tries to insert or update data on database, sometime your record need to depend on the row record.  \n<|ASSISTANT|>#### ",
        "input": "<|USER|> In 1961, this paper divided the whole statistics community. Prior to that, a very influential statistician, Ronald Fisher, popularised a theory called maximum likelihood estimation, which is regarded very, very highly. But this paper challenged their firm beliefs, so it was such a shock that the result is now known as Stein's paradox. Let's see what this is about. Suppose I have a set of data that follows a normal distribution, with some unknown mean mu and a variance of 1. Now I randomly pick a sample, a data point from this distribution, and let's say I picked 3.14. Your job is to estimate the value of mu. What would you estimate? On screen, it looks like it is slightly more than 3.14, but the point is that you don't know the value of mu, it could also be on the left of 3.14. The only thing you know for sure is that data point 3.14. So without much more information, you might just estimate mu to be 3.14. That's a sensible guess. After all, a normal distribution is clustered around its mean mu, so there is a good chance that the 3.14 picked is in the proximity of mu, and the error of your estimate would not be too big. In theory, that is the best estimator, in some sense of the word best. Now we play the same game, except this time we have two sets of data. They are completely unrelated to each other, but they both follow normal distribution with means mu1 and mu2, both variants 1. Again, I picked one data point from each of the distributions, and let's say they are 3.14 and 1.618. Now what would you estimate? By exactly the same logic, you might also estimate mu1 to be 3.14 and mu2 to be 1.618, and again, this is the best estimator in this situation. But things change if we have three independent sets of data. Exactly the same set of, normal distribution with unknown means and variants 1. Normally, if I have the data points x1, x2 and x3 respectively, then you would again estimate mu1 to be x1 and so on, just like the previous cases. But now this is not the best estimator anymore, because in 1961 we found a better one called the James Stein estimator. If the data points are x1, x2 and x3, then this is the James Stein estimator of the means. So in order to estimate mu1, it is better to multiply this factor in front, rather than simply saying x1. This is really surprising. Those three sets of data are completely independent from each other, but in order to estimate mu1, it is better to incorporate all three data points, because the prefactor depends on all of x1, x2 and x3. This surprising result is now known as Stein's paradox. But this is not the only surprise here. James Stein estimator generalises to p sets of data, where p is at least 3, and very similarly, the estimate is going to have this factor in front. So our ordinary estimate, which is just echoing back the data points x1 up to xp, was the best in 1 and 2 dimensions, but is overpowered by the James Stein estimator in 3 dimensions and above. What happened there? What's more, that really trusted theory of maximum likelihood estimation also supports our intuition. The theory also tells us to just use the data points given, the ordinary estimator. So the fact that the James Stein estimator beats a well-studied theory definitely caused chaos in the statistics community. But what do I really mean by an estimator being the best? To quantify how good an estimator is, we use the mean-squared error. Error is the difference between our estimator mu hat and the true, hat unknown mean mu, and then we square it to make it positive. In the case of multiple sets of data, we treat those estimators and means as vectors and consider the norm of it squared. In other words, we just consider the squared error in each component, then add them all up. Finally, we take the average. This is taken to account for the randomness in the estimator. To be clear, if we see a certain sample x, it already determines our estimate mu hat. You can't introduce your own bias here. So this process is entirely deterministic, not random, but the samples taken are random because they are taken from a distribution. So the expected value here takes all possible values of the sample into account. In other words, this is how much, on average, the estimator deviates from the true value. For example, in the one-dimensional case, what is the mean-squared error of the ordinary estimator? The estimator is exactly the sample taken, so if we have the sample being x, then the estimator will be x. In this case, the mean-squared error turns out to be the formula for the variance of x. We have assumed that the normal distribution has variance 1, so the mean-squared error is 1 in this case. But the error in general depends on mu, the true mean. To illustrate this, let's take a naive estimator 7. Whatever sample I have taken, you are just going to estimate the mean to be 7 anyway. The mean-squared error is then the expected value of 7 minus mu whole squared. However, the square inside does not depend on the sample x, and so it is not a random quantity. Therefore, the expected value is just what's inside, 7 minus mu whole squared. This clearly depends on the true mean mu. We can then plot the mean-squared error of these two estimators against the true mean mu. The ordinary estimator returns a constant 1, independent of mu, and the naive guess 7 is a parabola, centered around mu equals 7. In this case, we can't say definitively which estimator is better, because if the true mean is actually close to 7, then the naive estimator 7 will on average perform better than the sensible ordinary estimator, i.e., it has lower mean-squared error. But of course, for the other ranges, the ordinary estimator performs better. So now, let's say that one estimator A has the mean-squared error consistently below another estimator B, then we say A dominates B. We call the ordinary estimator the best estimator in the sense that no estimator can dominate it, so this blue graph entirely below the ordinary estimator will not happen. Sure, some regions can be outperformed, like near 7 for the naive estimator, but not the full parameter space. More professionally, if this will not happen to any other estimator, such an estimator is called admissible, so the ordinary estimator is admissible in this case. In higher dimensions, our ordinary estimator would have the squared error being the sum of the individual components, so we just add those means up. If we have p sets of data, because the mean-squared error of individual estimates would be 1, the sum would be p, independent of the true means mu1 up to mu p. So again, if we plot the mean-squared error, it will appear as a straight line. However, this is the graph of the James-Stein estimator, which is demonstrably below the red line. In other words, the James-Stein estimator dominates the ordinary estimator. This is only the case for p equals 3. For other values of p, we scale the graph appropriately, but we can see that the James-Stein estimator has much better performance than our ordinary estimator, especially when the means are close to 0. But why does the James-Stein estimator perform better? If you want a more rigorous explanation, including where exactly the divide on dimensions takes place, I have made another video on the second channel. I'm not doing it here because it feels more like manipulating symbols, not shedding a lot of light on the more general lessons learned. But before I talk about the intuitive explanation, I want to give thanks to this video sponsor, NordVPN. Early on, I travelled to Iceland, and it is certainly a very unique and beautiful country. But whenever you use a public wifi such as airport wifi, hackers can see your personal information if you are careless and go to a website without the HTTPS protocol. A VPN creates an encrypted tunnel that can make sure your personal information is safe. Also, any website can track your IP address and know where you are. Using a VPN means that those websites cannot easily track you. I originally thought that since a VPN adds an extra layer of encryption, it will slow down your connection, but was surprised to see that there is virtually no difference in internet speed before and after using a VPN. You can now go to nordvpn.com slash methamaniac to get a two-year plan with an exclusive deal, plus four bonus months free. It's risk-free with NordVPN's 30-day money back guarantee, and you can also support this channel by signing up. Alright, let's go to the intuitive explanation for why the shrinkage factor works. If we look at the forum of the James Stein estimator, there is this factor here, which is usually called the shrinkage factor, because normally, this factor is between 0 and 1. However, if the data points are too small, then the denominator can be very small, and this will make the supposed shrinkage factor actually negative. Even with this flaw, the James Stein estimator still performs better. Geometrically, in the two-dimensional case, we can plot the true means mu1 and mu2 as a point on the 2D plane. Variance 1 means that the data is roughly distributed like such, circularly symmetric around the red dot. The mean squared error incurred would then be first taking the distance of the sample to the red dot, square it, and then repeat this action of squaring distance to take the average. Here is how the shrinkage factor might minimise the average. For points further away from the origin, by making it closer to the origin, you also move closer to the red dot. The drawback is, of course, for points already close to the origin, the shrinkage factor will get it even closer, and further away from our red dot. Fortunately, only the points in this finite circular region will move further away from the red dot by shrinking towards the origin. For all the other points, you get closer to the red dot by shrinking towards the origin. Yes, that even includes the points on the left of the axis. So overall, we might get a reduction in distance towards the red dot. Let's call this blue region the far end, and the green region the near end. The trouble is, what if shrinkage only gives a tiny reduction on the far end, but a considerable enlargement on the near end? Then the advantage gained by having a larger area of reduction is diminished. Unfortunately, this is sort of what happens for the James Stein estimator. When we compute the distance from the James Stein estimator to the origin, we will have this shrinkage factor, as usual, times the original distance from the origin. So the absolute reduction in distance is this extra term, which can be simplified as such. Notice that the original distance is in the denominator, which means that if the original distance is large, i.e. further away from the origin, then this reduction in distance is smaller. This is the kind of stuff that we don't want to see, because it precisely means that on the far end, where the original distance from the origin is normally large, gets a smaller reduction towards the red dot, and those on the near end gets larger addition in distance from the red dot. So there is always a tension between a much larger area for potential reduction in distance versus the smaller actual distance reduction. This is where dimensions come in. In three dimensions for instance, we can similarly plot the true means as a point, and similarly, we can define the near end and the far end. In higher dimensions, there is just more stuff out there, so the effect of a larger volume is much much more pronounced, which makes this method of shrinkage really work in higher dimensions. This is already quite an important lesson in modern statistics, but there is an even more general concept at play. Let's revisit the formula for the mean squared error. We have the expected value of something squared, which might remind you of the formula for variance. Now we can simply rearrange this formula and substitute that x to be mu hat minus mu. Then we will get a formula that decomposes the mean squared error into two parts. Because mu is a constant, not random, the variance is the same as just the variance of mu hat. And the expected value of mu hat minus mu is called the bias. In words, the mean squared error is the sum of variance and bias squared. But what does that mean? Pictorially, let's say the true mean mu is represented by the vertical line here. Remember that an estimator is random, because it depends on the samples so it has a distribution, such as this one on screen. This depicts the situation where the mean of this distribution mu hat is exactly mu, the thing we are trying to estimate, i.e. the bias is zero. However, its variance is quite large, so if we perform the estimation, it is very possible to be far away from mu, and the error is quite large. Sometimes it might actually be better to introduce a little bias if the variance could be dramatically reduced. Then most of the time, it just could not go too far away from mu, and the overall mean squared error remains small. This is known as bias-variance trade-off, the idea that the mean squared error, in some cases, could be reduced by increasing the bias slightly if it means that the variance can be reduced by a lot. This is very well illustrated by the James Stein estimator. The ordinary estimator just directly copies the data points, there will be no bias in this estimation. But during the shrinkage process, clearly the centre of all these points is not exactly the red dot, so this is clearly biased. However, the points are significantly closer together than without drinking, reducing variance. Sure, an unbiased estimator like the ordinary estimator has some beautiful symmetry in it, but it could perform worse than a biased estimator that has a much lower variance. Perhaps you might be thinking that this is only a simple toy problem, which it is, but the lessons about shrinkage and bias-variance trade-off are hugely important in modern statistics. Nowadays a lot of times, we are dealing with a real-world situation where there are some inputs and outputs, and we are trying to imitate it with a mathematical model. Normally, our model might have a lot of parameters, which you can think of as adding different weights to the inputs. But then because we don't expect the real-world situation to be that mathematical, if there is any difference between the model prediction and the real-world situation, we say it is just due to random noise. Then based on observing a lot of sample-input-output pairs in the real-world situation, we try to estimate and fine-tune those parameters or weights so that we can use less noise term to explain away any disparity. Compare this to the estimation problem that started this video. Again, we try to observe some sample from the distribution, and we try to estimate the underlying parameters, the means mu1, mu2, and so on. What James Stein Estimator illustrates is that if there are many parameters we need to estimate, the shrinkage factor could really reduce the mean squared error. And much more dramatically, when the number of parameters becomes larger and larger, the growing number of parameters is exactly the situation in this century. For more complicated models, shrinkage can even help us decide which inputs are actually important and which aren't, especially if we shrink some of the weights all the way to zero. This idea of shrinkage is known as regularization in the world of machine learning. Of course, we might not use James Stein Estimator directly in the real-world situations, but the lessons of shrinkage and bias variance trade-off continue to be applied in modern-day statistics. In fact, there are way more things that we could say about Stein's paradox. For instance, James Stein Estimator is not even admissible either, i.e. there are some other estimators that dominate it. I will leave those in the description below. Thanks again to the patrons, subscribe and notifications on, like, share, and comment as well. See you next time!",
        "output": "### Stein's Paradox and its Effects on Statistics\n\n#### Introduction to Stein's Paradox\n\nStein's paradox, also known as James-Stein estimator, is a 1961 discovery that caused chaos in the statistics community by challenging the well-regarded maximum likelihood estimation theory. The paradox appears when estimating the means of multiple independent sets of data, and shows that the James-Stein estimator provides a better estimate than the traditional method of simply mirroring the data points.\n\n#### James-Stein Estimator and Dimensions\n\nThe effectiveness of the James-Stein estimator lies in its \"shrinkage factor,\" which performs better when working with three or more sets of data. In higher dimensions, this shrinkage factor has a significant effect on reducing the mean-squared error, creating a more accurate estimator when compared to the ordinary estimator.\n\n#### Bias-Variance Trade-off\n\nThis paradox also illustrates the concept of the bias-variance trade-off, which states that the mean-squared error can sometimes be reduced by slightly increasing the bias if it results in a substantial reduction of variance. Although unbiased estimators are attractive due to their symmetry, a biased estimator with lower variance might perform better in certain situations.\n\n#### Importance and Applications of Stein's Paradox in Modern Statistics\n\nThe lessons learned from Stein's paradox about shrinkage and the bias-variance trade-off have important implications in modern statistics, particularly in machine learning and dealing with complex models with many parameters. This concept of shrinkage is known as regularization in machine learning and helps statisticians to decide which inputs matter and to reduce potential noise in the data.\n\nOverall, Stein's paradox is a significant discovery that has not only challenged traditional beliefs in statistics but also provided valuable lessons and techniques for dealing with high-dimensional data and complex models, which remain relevant in today's statistical landscape."
    },
    {
        "instruction": "<|SYSTEM|>You are a scholars that is taking markdown notes about some video, you would be giving some written scripts for the task.\nYou have finished writing the notes, and you want to extract the title from the notes.\nThe title should only be the name of the technology or algorithm, no extra words needed.\nPlease write the notes in markdown format. No need to write \"Title:\" for the Title. Make sure the title is within 10 words.\nHere is an example:\n## Hidden Markov Model\n<|ASSISTANT|>## ",
        "input": "<|USER|>### Stein's Paradox and its Effects on Statistics\n\n#### Introduction to Stein's Paradox\n\nStein's paradox, also known as James-Stein estimator, is a 1961 discovery that caused chaos in the statistics community by challenging the well-regarded maximum likelihood estimation theory. The paradox appears when estimating the means of multiple independent sets of data, and shows that the James-Stein estimator provides a better estimate than the traditional method of simply mirroring the data points.\n\n#### James-Stein Estimator and Dimensions\n\nThe effectiveness of the James-Stein estimator lies in its \"shrinkage factor,\" which performs better when working with three or more sets of data. In higher dimensions, this shrinkage factor has a significant effect on reducing the mean-squared error, creating a more accurate estimator when compared to the ordinary estimator.\n\n#### Bias-Variance Trade-off\n\nThis paradox also illustrates the concept of the bias-variance trade-off, which states that the mean-squared error can sometimes be reduced by slightly increasing the bias if it results in a substantial reduction of variance. Although unbiased estimators are attractive due to their symmetry, a biased estimator with lower variance might perform better in certain situations.\n\n#### Importance and Applications of Stein's Paradox in Modern Statistics\n\nThe lessons learned from Stein's paradox about shrinkage and the bias-variance trade-off have important implications in modern statistics, particularly in machine learning and dealing with complex models with many parameters. This concept of shrinkage is known as regularization in machine learning and helps statisticians to decide which inputs matter and to reduce potential noise in the data.\n\nOverall, Stein's paradox is a significant discovery that has not only challenged traditional beliefs in statistics but also provided valuable lessons and techniques for dealing with high-dimensional data and complex models, which remain relevant in today's statistical landscape.",
        "output": "## Stein's Paradox"
    },
    {
        "instruction": "<|SYSTEM|>You are a scholars that is taking markdown notes about some video with Wiki style, you would be giving some written scripts for the task.\nAnd summarize each paragraph with a title.\nRemember to keep the technology or algorithm name, very important.\nPlease write the notes in markdown format.\nHere is an example:\n### Definiation\nA deadlock is a situation in which two computer programs sharing the same resource are effectively preventing each other from accessing the resource, resulting in both programs ceasing to function. \nThe earliest computer operating systems ran only one program at a time.\n\n### What it have to do with Relationsal Database ManagementSystem\nIn a database, a deadlock is an unwanted situation in which two or more transactions are waiting indefinitely for one another to give up locks. \nDeadlock is said to be one of the most feared complications in DBMS as it brings the whole system to a Halt. \nWhen you tries to insert or update data on database, sometime your record need to depend on the row record.  \n<|ASSISTANT|>#### ",
        "input": "<|USER|> Hello and welcome to my channel to this new video on graph embeddings. Now graph databases enjoy wide enthusiasm and adoption of analytics of interrelated data potentially from various sources. However, most machine learning algorithms they work on vector form data and therefore to unlock this powerful toolbox we need a way to map our graph structure to a vector form. And that's what I'm very excited about because I studied the NotoVEC algorithm and I'm excited about how straightforwardly and concise the approach is formulated and how well it works if you apply it to the right data. In this video we will walk through all the important details. We're going to start with the basic motivation why should we care about mapping graph structures to vectors. Secondly, we are talking about the problem. So how do we define this problem? What's the context of a node and what's the overall objective? And finally, we will walk through the math and here lies the key to the beauty and simplicity of the approach. Because once we understand the last bit of the equation, the novelty of the approach becomes obvious to us. So stay curious. It's going to be a pleasure and so rewarding to understand this. Get a coffee, lean back and let's do it. All right, part one, the motivation. Why should we care about graph embeddings? So we are trying to find a vector representation for our graph structure and we're not talking about the directly associated features of each of the nodes, but rather we want to map the structure of the data, meaning the relationships between the data points to an embedding space so that we can feed this into a machine learning algorithm. There is a strong relationship to the word to work algorithm, which is a word embedding algorithm. So it maps words to an embedding space which share a similar context. However, if we look at a sentence, we can tell that it's a one dimensional array where each word has a fixed position and two direct neighbors, meaning the word after it and the word before it. However, if we look at a graph, it's represented as an adjacency matrix. So for each node in the graph, we get an entire vector of direct neighbors. So even though the approaches are similar, as the name suggests, we need a slightly different approach for mapping nodes to vectors. Let's go ahead and formulate the general objective of our optimization problem. We want to map nodes with a similar context, meaning they connect to the same nodes close in an embedding space where they are represented as vectors. Now because you're following so attentive, two questions popped up in your head. First, what does similar context mean? And second, what does close in the embedding space mean? So you're going to be relieved as I tell you that we are going to be talking about exactly about these two questions. Let's have a look at the context of a node. So the context of a node is simply defined by nodes in the neighborhood of that node. They access two orthogonal approaches, how to explore the context of a node in a graph. And I'm pretty sure you're aware of both of them because they are tightly linked to breadth first and depth first search. The first similarity matrix is homophily. It's derived from a sociological concept stating that similar people tend to interact close with each other, meaning that similar people have direct relationships. And you can already tell why it's related to a breadth first algorithm, because this algorithm first explores the direct context of a node before it goes more into depth. The second metric is called structural equivalence, and it tries to detect the role of a node from a wider perspective. So a role of a node in a network would be, for example, like in human networks would be, for example, an influencer or someone who connects communities or something like that. So we're not only looking at the direct relationships of a node, but rather we follow up on multiple relationships, potentially multiple hops to go into depth to see which other nodes our source node is connected to. This in the graph context is closely related to a depth first algorithm, because there we also follow one relationship of our source node and then we follow up on this relationship to see, OK, whom else can we see if we follow this one relationship? And only if we have explored that one path into depth, we will move on to the next direct relationship of our source node. Now, I'm pretty sure that you are familiar with depth first and breadth first search in a graph, but I will walk you through quickly here on a slide. All right, let's start with the depth first algorithm, which is the orange annotation here on the slide. So we start in source node one, and we would explore all its relationships into depth first before we move on to the next direct relationship. So, for example, we start with the relationship one and we end up in node two and explore along the path until we cannot find any other node more into the depth. But here we would have to jump back to node two. So we don't do that, but but rather we go back in this path and see that in node two we have more relationships to explore. So we follow through on here. And from the five, we don't jump back to the node one, but rather we explore all the other relationships of five. So we would continue with node six down here. And once we have explored this entire path here into depth, we would jump back to node one and then explore its other relationships. We wouldn't take the relationship to node five because we have already explored node five, but rather we jump to node three and explore this relationship in turn into depth. OK, the second approach is a breadth first algorithm. Here we again start in node one and we explore all the direct relationships first. So we would jump to node two, five and three in the first three steps. And then we see we don't have any more direct relationships, so we explore nodes in the distance level of two. So we jump to node two and go to all the direct relationships from there. So we would end up in nodes nine and eight and seven. And then we jump to five and go to node six and then to three and go to node four. So here we have the context of node one using the depth first algorithm. And here we have the context of node one using the breadth first algorithm. Now, having these contexts, we could already calculate how much these nodes contexts are overlapping. And therefore we would use the Jaccard similarity, which I have visualized down here. So here's the context of node U, which is the red circle. And then we have the context of node V, which is the orange circle. And in the intersection there are these four nodes. And in total, we have these four plus three, so seven in total. So we have four over seven would be our similarity of these two nodes. OK, now we know what the context of a node means. However, we usually we cannot take into account the entire context of a node, because that would eventually lead to capturing the entire graph. Therefore, we have to introduce a sampling strategy, which is basically a strategy to approximate the context of a node. In the node to VAC algorithm, they introduce a sampling strategy, which basically is a fixed number of random walks starting in each of the nodes and exploring the context. Now, that context exploring is also parameterized, which is basically a trade off between the breadth first and depth first algorithm. So it's a trade off between homophily and structural equivalence, because in each step of a random walk, the algorithm decides based on the parameter whether the algorithm should move one distance level away from the source node, which would be similar to structural equivalence, or whether it should stay close to the source node, which would be related to homophily or a more breadth first algorithm. Now, with applying the sampling strategy at a source node in the graph, we can obtain the neighborhood of U given sampling strategy S, which is the context of a node, which we are going to take into account and which will be denoted like this for the rest of the video and also in the paper. Now, having the context of each of the nodes, we want to find a vector representation, such that nodes with a similar context are mapped close in the embedding space. Now, the second question was, what does close in the embedding space mean? And that's answered pretty easily because it's the dot product between our two vectors, which is basically the angle between the two. So the smaller the angle, the more similar our vectors are. Now, in that formulation, we want to map nodes with a similar context close in the embedding space is the foundation of our optimization problem. And in the next step, we are going to formulate it from a mathematical perspective and explore how Node2vec solves this optimization problem. It's getting interesting now. OK, I want to start with a single node in our graph, which would be node U. And that one we pick in our graph to be our source node, for which we want to find a graph embedding, for example. Now, if we explore the context of source node U in the graph, we can find easily the neighborhood of source node U, given the sampling strategy S. And that is the sampling strategy we talked about previously. So we have a parameter which decides how to explore the surrounding of the node U. And we end up with a set of nodes. These are the orange ones here in the context of our neighbor, of our source node U. Now, I want to define node V as being an element of the context of our node U. OK, having these things at hand, we want to find a mapping function, which we call f, which basically maps a node to a vector of the dimensions. So f of U is a vector of the dimensions. All right, having this function f at hand, which are two vectors, we can easily calculate the similarity between two nodes, because we just take the two nodes and generate the vector with a generating function f or the mapping function f. And then we take the dot product as discussed previously, which we can think of the similarity score in the embedding space. So we can think of the dot product as being the angle between the two vectors. So if node V is in the context of U, we want this similarity to be large. So the angle between the two vectors should be small. Now, having the dot product as a similarity score in the embedding space, we want to transform that score into probabilities. And that we can do by applying a softmax, because a softmax basically normalizes our score into a probability, because it squashes the numbers into an interval of 0 to 1. And all the numbers for node U, so all the combinations of V and U that we can potentially find in the graph, have a sum that equals 1. So that's the softmax. If you're not familiar with the softmax, please research that on your own. Now, having the probability, we can actually write this as a conditional probability. So we say the probability that given node U, which was our source node, what's the probability for seeing node V? And that we calculate over the function f, which makes a vector out of nodes, and then over the dot product, and we apply the softmax, and we arrive at that probability, which we want to optimize. Because as we know that node V is in the context of node U, we want this to be large, so we want to maximize this probability. And that's how the relationship is formed between contexts of nodes and similarities in the embedding space. However, having that at hand, we don't want to do that only for one node being the context of U, but rather for the entire context of U. So we start from a source node U, and we want to find the probability for all the nodes in the context of U. So the probability for having this entire context given node U is in turn a conditional probability if we apply it to the entire context of U. And if we assumed independence of the nodes in the sample of the context of U, we can write this pretty easily as simply a product over all nodes in the context of U. And we take the conditional probability which we saw earlier. So we take probability given node U, seeing one of the nodes in the context of U, and that's an entire product over all of these samples. And where we arrive is basically this up here. So we get the probability for seeing all of the nodes in the context of U of the sample context, given that we started node U. Now this formula is still very easy. And if we go ahead and apply this now to all nodes in the graph simultaneously, so we not only start from one node U, but rather we start from all nodes in the graph a fixed number of random walks. So we sample each context of each node in the graph, and then we apply this in our equation. We would add up at the following equation, which is basically the final equation for finding graph embeddings. So here we have the probability for seeing the sample context of node U, given we start in one node U, which is basically this term up here. And then we sum these probabilities up over all nodes in our graph. So every node becomes the node U. And that's the entire score for our embedding we found. And we, of course, we want to maximize this equation. So we take this equation, which is the total probability score, which we want to maximize. So we want this to be maximized, which would be our best graph embedding we could find. Because nodes which would appear in the context of a node would have a close or a high probability in the embedding space, and that's what we want to find. So we update the f function up here, which we use to generate a vector out of a node and maximize for this probability using stochastic gradient descent. And that's basically it. It's that simple. I really like how straightforwardly it is formulated and how easily it is understandable. And not to say that it's easily calculated, because there are some challenges in calculating these terms. So there are some optimizations and approximations used to calculate this equation here efficiently on large data sets. But you can look them up in the paper as well. But the basic idea is actually that simple. And that's the amazing thing, because that's all there is. It's very easily and intuitive formulated. It's a very concise description of the problem we are trying to solve, and it's very simple math, and still it works so well for many use cases. So if you like this video, hit the like button. If you have questions, leave them in the comments. And make sure you subscribe for the next video where I'm going to be talking about. I don't know yet, but we will see. See you then. Bye bye.",
        "output": "### Motivation: Graph Embeddings\nGraph databases are used for analyzing interrelated data from various sources. To use machine learning algorithms, we need to map the graph structure to a vector form, which is called graph embeddings. The goal is to find a vector representation for the graph structure, not just the features of each node.\n\n### Node2Vec Algorithm\nNode2Vec is inspired by the Word2Vec algorithm and handles the mapping of nodes to vectors in a similar way. It explores the context of nodes in the graph and finds vector representations for them. Nodes with similar contexts should be mapped close in the embedding space.\n\n### Context of Nodes: Homophily and Structural Equivalence\nThere are two ways to explore the context of nodes in graphs - homophily and structural equivalence - which are related to breadth-first and depth-first search. Homophily reveals that similar nodes have direct relationships, while structural equivalence detects the role of a node from a wider perspective.\n\n### Context Approximation and Sampling Strategy\nTo avoid capturing the entire graph, we need to approximate the context of a node using a sampling strategy. Node2Vec employs a fixed number of random walks, parameterized by breadth-first and depth-first approaches.\n\n### Using Similarity Scores and Softmax\nTo find the similarity between nodes, the dot product of their vectors is calculated, and a softmax function is applied to transform the similarity scores into probabilities. A function, f, is used to map nodes to vectors with probability maximization.\n\n### Optimizing for Embeddings\nThe goal is to maximize the total probability score for finding the best graph embeddings. The process updates the function, f, and maximizes the probability using stochastic gradient descent. The simplicity and conciseness of Node2Vec make it ideal for many use cases."
    },
    {
        "instruction": "<|SYSTEM|>You are a scholars that is taking markdown notes about some video, you would be giving some written scripts for the task.\nYou have finished writing the notes, and you want to extract the title from the notes.\nThe title should only be the name of the technology or algorithm, no extra words needed.\nPlease write the notes in markdown format. No need to write \"Title:\" for the Title. Make sure the title is within 10 words.\nHere is an example:\n## Hidden Markov Model\n<|ASSISTANT|>## ",
        "input": "<|USER|>### Motivation: Graph Embeddings\nGraph databases are used for analyzing interrelated data from various sources. To use machine learning algorithms, we need to map the graph structure to a vector form, which is called graph embeddings. The goal is to find a vector representation for the graph structure, not just the features of each node.\n\n### Node2Vec Algorithm\nNode2Vec is inspired by the Word2Vec algorithm and handles the mapping of nodes to vectors in a similar way. It explores the context of nodes in the graph and finds vector representations for them. Nodes with similar contexts should be mapped close in the embedding space.\n\n### Context of Nodes: Homophily and Structural Equivalence\nThere are two ways to explore the context of nodes in graphs - homophily and structural equivalence - which are related to breadth-first and depth-first search. Homophily reveals that similar nodes have direct relationships, while structural equivalence detects the role of a node from a wider perspective.\n\n### Context Approximation and Sampling Strategy\nTo avoid capturing the entire graph, we need to approximate the context of a node using a sampling strategy. Node2Vec employs a fixed number of random walks, parameterized by breadth-first and depth-first approaches.\n\n### Using Similarity Scores and Softmax\nTo find the similarity between nodes, the dot product of their vectors is calculated, and a softmax function is applied to transform the similarity scores into probabilities. A function, f, is used to map nodes to vectors with probability maximization.\n\n### Optimizing for Embeddings\nThe goal is to maximize the total probability score for finding the best graph embeddings. The process updates the function, f, and maximizes the probability using stochastic gradient descent. The simplicity and conciseness of Node2Vec make it ideal for many use cases.",
        "output": "## Node2Vec"
    },
    {
        "instruction": "<|SYSTEM|>You are a scholars that is taking markdown notes about some video with Wiki style, you would be giving some written scripts for the task.\nAnd summarize each paragraph with a title.\nRemember to keep the technology or algorithm name, very important.\nPlease write the notes in markdown format.\nHere is an example:\n### Definiation\nA deadlock is a situation in which two computer programs sharing the same resource are effectively preventing each other from accessing the resource, resulting in both programs ceasing to function. \nThe earliest computer operating systems ran only one program at a time.\n\n### What it have to do with Relationsal Database ManagementSystem\nIn a database, a deadlock is an unwanted situation in which two or more transactions are waiting indefinitely for one another to give up locks. \nDeadlock is said to be one of the most feared complications in DBMS as it brings the whole system to a Halt. \nWhen you tries to insert or update data on database, sometime your record need to depend on the row record.  \n<|ASSISTANT|>#### ",
        "input": "<|USER|> Hi, hello everyone. It is my great pleasure to be here and take part of such an exciting event with so many cool talks and brilliant ideas. Thanks to the organizers for putting all of this together and inviting me. My apologies for not being able to give a live presentation, but as it turns out, unfortunately, I was scheduled to give a lecture at the same time. Anyway, the title of my talk is Mark of Blankets in the Brain, How the situatedness in the environment is reflected in neural patterns. Well, the brain is not like this. It is not in a void, it does not develop and evolve as an encapsulated system or in a vat. It is situated and distributed in a body which is itself environmentally situated, all part of a multiscaled system. So when we want to investigate and understand neural activity, it should be with this in mind. My aim here today is to look into how the situatedness in the environment is reflected in neural patterns. The lineup for today is first, I'm just briefly going to talk about the distinction between two perspectives, neural representation and complex neural activity. And that will set us up to introduce Mark of Blankets as a tool to look into the different scales of neural activity, such as single neuron, cortical, microcircuitry and networks. In classic computationalism, in cognitive neuroscience, is the view that more than an analogy or a metaphor, the brain is literally a computer and cognition is information based computations outputting representations. So representations are expected to be found somewhere in the brain. As the authors of this paper tell us, some of the essential features of the cognitive scientific enterprise are, for one, the belief that in talking about human cognitive activities, it is necessary to speak about mental representations. Or two, that the central to any understanding of the human mind is the electronic computer. Or three, the decision to the end, the emphasize certain factors such as the influence of affective factors or emotions or the contribution of historical and cultural factors and the role of the background context in which particular actions or thoughts or cure. While the notion of neural representation is endorsed by many in applied and computational cognitive neuroscience, it is not held across the board. The traditional computational approach is rejected by connectionism, some neuro computational approaches, ecological psychology, situated robotics, synergetics and embodied cognitive science or complex and dynamic theory. Tim van Gelder in his dynamic hypothesis offers an alternative to the view of the mind as a special kind of computer and cognitive processes as rule governed manipulations of internal symbolic representations. It tells us that rather than computers, cognitive systems may be dynamical systems and rather than computation, cognitive processes may be state space evolution within these very different kinds of systems. Walter Freeman and Christine Skarda also note that there's something odd about looking for representations in the brain. They inform us that they found that thinking of brain function in terms of representation seriously impedes progress toward genuine understanding. Such interpretation is misleading. After years of sifting through our data, we identified the problem. It was the concept of representation. Once we stopped looking at neural patterns as representations of odorants, we began to ask a new set of questions. So in our work, we sort of take on board these insights and call for an eliminativism about neural representations. While the idea of neural representation in applied and computational neuroscience is seductive by giving the impression that it solves a problem, what we realize is that when we refrain from using it, brain activity is not affected. So our aim is to explain neural activity as interactive flows of brain dynamics whose patterns reflect the neural situatedness in the environment. The brain, seen as a complex dynamical system, is then a collection of neural populations that interact with one another in a non-trivial way, such that the collective behavior of the system is different or does not immediately follow from the aggregation of the behavior of the part. The brain, seen as a complex dynamical system, means that cognitive life is understood as a social-cultural engagement of the physiological organism that is situated in the environment, and that is the brain. The brain, seen as a complex dynamical system, means that cognitive life is understood as a social-cultural engagement of the physiological organism that is situated in the environment, in a continuous interaction with the environment. So the complexity of neural activity is not reducible to large quantities of neural connections, nor to the interactions occurring within the nervous system itself. The activity of the population of neurons embodied in brain activity inevitably occurs in the presence of noise or stochastic fluctuations due to thermal energy, ion channel chattering, and irregular synaptic inputs from other neurons. This embodiment also applies to the whole organism as extended to almost all cells, tissues, and organs of the body, such that, as Dennis Noble tells us, organisms also use stochasticity at all levels to generate new DNA sequences as in the immune system and new forms of behavior as in the nervous system. Noise in the brain corresponds to the fluctuations arising from both internal and external sources, for example, of a neuron, cortical column, region, or the environmentally situated body. Capturing these fluctuations is not trivial, and while the mathematics of stochastic differential equations provide guidance in terms of orbits, attractors, bifurcations, a comprehensive formalism that explains the interactive flows at every scale of neural activity in terms of behavior that emerges from the dynamics of the parts of the populations would be epistemically useful. Single cell spikes are highly nonlinear, but the question is, do such nonlinearities appear in microscopic neural activity, and if so, what processes underlie these nonlinear dynamics across scales? This is relevant because in the end what we want to explain is the nonlinear covariance between multi-scaled patterns of neural activity and the situatedness in the environment. That means how the situatedness in the environment is reflected in neural patterns. My colleagues and I attempted to answer this in our paper, Markov blankets in the brain, and the follow-up paper, Parcels and particles, Markov blankets in the brain. We applied Markov blankets to the brain to explain the interactive flows at every scale of brain dynamics under a common formalism. We used this statistical tool to explain how embodied systems situated in the environment maintain itself by engaging in dynamics of exchange with their environment. We turn now to the study. A Markov blanket partitions blanket states into active and sensory states. Sensory and active states influence each other directly, such that internal and external states indirectly influence one another. A Markov blanket is not necessarily a physical boundary, such as a cell membrane, but it's rather a statistical one defined by variables that are conditionally independent of each other. So a Markov blanket comprises a conditional dependency, as you can see in equation 1, and dynamics, as you can see in equation 2, by delineating observed behaviour as influences between internal and external states. This schematic illustrates the partition of states into internal states in purple and hidden states or external states in orange that are separated by a Markov blanket comprising sensory states in green and active states in blue. Specifically, it focuses on the dynamic formulation of equation 2. Direct influences are highlighted with dotted connections or connectors, and autonomous states are those states that are not influenced by external states, while particular states constitute a particle, namely autonomous and sensory states or blanket and internal states. Sensory states, active states and internal states then comprise the particular states that are constitutive of a functional neural unit. Based upon the neural dynamics used in dynamic causal modelling of canonical microcircuits, we know that synaptic dynamics conform to a Markov blanket, such that internal states correspond to the conductance of ion channels of one neuron and are distinguishable from the same states of other neurons, but they interact through presynaptic and postsynaptic voltages. This figure illustrates a Markov blanket separating the membrane conductances of a pair of neurons, or between one postsynaptic neuron and all presynaptic neurons. The A terms here are constants that act as connectivity strengths from the active state of one neuron to the external state of another, and from the sensory states of the external state to the internal state of the active state of one neuron. When many neurons are in play, this becomes a connectivity matrix. The sigma function is a sigmoid shape that can be thought of as converting potentials to firing rates. An interesting feature of this structure is that the sensory states, from the perspective of a given neuron, can arise from many different external states, while the active states, which is the membrane, the polarization, depend only on the conductance, which is an internal state, of the neuron being depolarized. Normal arrowheads here indicate an excitatory influence, while the round arrowheads show the inhibitory influences. So overall, this means that external states, such as the presynaptic conductance, cause changes in internal states, such as the postsynaptic conductance, via sensory states. These means presynaptic voltage. On the other hand, internal states couple back to the external states through active states, which correspond to the postsynaptic voltage. The Hatchkin-Axley model's equations describe how action potentials in neurons are initiated and propagated through a set of nonlinear differential equations that approximate the electrical characteristics of excitable neurons in a continuous time dynamical system. Formulating the Hatchkin-Axley in terms of Markov blankets enables us then to specify how the external states, such as electrophysiological setup, via sensory states, which is the injected current, influence internal states, which correspond to ion channels, themselves then influencing active states corresponding to the membrane potential. In order to keep track with our goal of explaining interactive flows at every scale of brain dynamics under a common formalism that should reflect the situatedness of the organism in the environment, we have to move towards dynamics exhibited by populations of connected neurons. And this means moving towards a pair of neurons or neural network where current is not coming from the experimental intervention, as we had just now with the Hatchkin-Axley model, but from another neuron, meaning a membrane potential corresponding to active states of a given neuron, which can be directly influenced by the conductance, meaning the internal states of that neuron. So on top of this scheme is the connectivity of a canonical microcircuit as developed within dynamical causal modeling. And this comprises four cell populations with a stereotyped pattern of connectivity. So you can see that from left to right, there are ascending connections and the opposite direction descending connections. The dynamics of each neural population shown here obey the equations given before specifying which populations are connected to one another. The second row illuminates how these connections map upon the cortical region. The superficial and deep pyramidal cells play the role of active and sensory states respectively. And in the final row, you can see a separation into network of regions by which the middle two regions act to mediate the far left and right regions. Now, the image in this figure takes the ideas from the previous figure one step further and shows how we could treat the connections between nodes in different networks as dependencies between states in the Markov-Lenkert system. Here the networks themselves become the active, sensory, internal and external states. This graphic is loosely structured around the kinds of networks that have been identified using resting state fMRI. Here we treat the visual networks as internal states that reciprocally influence active states correspond to the dorsal and ventral attention networks. The default mode network then plays the role of the sensory states, which then in turn mediate the influence between the above and the external corresponding to sensory motor network states. This assignment of this is equally valid if reversed, such that you could have sensory motor networks becoming internal states and visual, the visual ones becoming external states. Functional integration has been treated by formal accounts of predictive processing in the brain. This characterization rests upon the asymmetric and directed connectivity that defines cortical and sub-cortical hierarchies. The study of context-sensitive directed coupling between the nodes of neuronal networks calls for an estimate of affective connectivity and there is a model of how measured brain signals are generated. One then has to resolve the ill-posed problem of recovering the underlying connectivity parameters of the model, usually using Bayesian inference. One of the best examples out there is dynamic causal modelling. In this paper we describe a treatment of affective connectivity that speaks to the emergence of intrinsic brain networks and critical dynamics. It is predicated on the notion of Markov blankets that play a fundamental role in the self-organisation of far-from-equilibrium systems. Using the apparatus of the renormalisation group, we show that much of the phenomenology found in network neuroscience is actually an emergent property of a particular partition of neuronal states over progressively coarser scales. As such, it offers a way of linking dynamics on directed graphs to the phenomenology of intrinsic brain dynamics. With a particular parcellation scheme based on affective connectivity, we can in principle explain and quantify the emergence of large-scale intrinsic brain networks and their characteristic dynamics. But the crucial aspect of the particular parcellation or partition that we employ in this work means that it can be applied recursively in the spirit of the renormalisation group. So we use the notion of the renormalisation group to motivate the brain to recover the underlying connectivity parameters of the model. So we use the notion of the renormalisation group to motivate this recursive parcellation because there are some formal constructs in terms of RG scaling that furnish an insight into how dynamics change as we move from one scale to the next. Now, as we have seen previously, the Markov blanket allows one to distinguish a collection of vector states or simply states that belong to something, for example, a particle from states that do not. This provides an operational definition of a particle that in our present setting can be regarded as a region of interest or parcel of brain states. This means that a particular partition becomes a parcellation scheme in terms of functional anatomy. This is where the renormalisation group comes in, in a recursive application of the particular partition. In other words, if we start with some states at any level, we can partition these states into a set of particles based upon how the states are coupled to each other. We can then take the principal eigenstates of each particle's blanket states to form new states at the scale above and start again. This recursive application of a grouping or partition operator G followed by a dimension reduction R leads to the renormalisation group based upon two operators R and G. In theoretical physics, the renormalisation group refers to a transformation that characterises a system when measured at different scales. This schematic illustrates a recursive procedure by which successively coarser scale and slower dynamics arise from subordinate levels. At the bottom of the figure, the lower panel, we start with an ensemble of vector states, here nine. The conditional dependencies among these vector states, the eigenstates, define a particular partition into particles that you can see in the upper panels. Crucially, this partition equips each particle with a bi-partition into blanket and internal states where blanket states comprise active and sensory states, where active are depicted in red and sensory states are in magenta. The behaviour of each particle can now be summarised in terms of slow eigenstates or mixtures of its blanket states to produce states at the next level or scale. This constitutes an ensemble of vector states and the process starts again. Formally, one can understand this in terms of coarse graining the dynamics of a system via two operators. The first uses the particular partition to group subsets of states, while the second uses the eigenstates of the resulting blanket states to reduce dimensionality. The upper panels illustrate the bi-partition of a single particle in the left and an ensemble of particles that is the particular partition per se in the right panel. The insects on top illustrate the implicit self-similarity of particular dependencies pictorially in moving from one scale to the next. Now, this schematic illustrates a partition of eigenstates, which are depicted as the small coloured balls, into particles, where each particle has six blanket states, which are red and magenta for active and sensory states respectively, and three internal states in cyan. The upper panel summarises the operators used to create a particular partition. So we start by forming an adjacency matrix that characterises the coupling between different vector states. This is based upon the Jacobian and implicitly the flow of vector states. The resulting adjacency matrix defines a Markov blanket forming a matrix B, which identifies the children, parents and parents of the children. Now, the same agency matrix is used to form a graph Laplacian G that is used to define neighbouring or coupled internal states. So one first identifies a set of internal states using graph Laplacian. Coupled internal states are then selected from the columns of the graph Laplacian that exceed some threshold. In practice, the examples used specify the number of internal states desired for each level of the hierarchical decomposition. Now, having identified a new set of internal states that are not members of any particle that has been identified so far, its Markov blanket is recovered using the Markov blanket forming matrix. The internal and blanket states then constitute a new particle, which is added to the list of particles identified. This procedure is repeated until all vector states have been accounted for. Usually towards the end of this procedure, candidate internal states are exhausted because all remaining unassigned vector states belong to the Markov blanket of the particles identified previously. In this instance, the next particle can be an active or sensory state, depending upon whether there is a subset of active states that is not influenced by another. In the example here, we have already identified four particles and the procedure adds a fifth, which is a top particle to the list of particles that have been identified. Now, we think that Markov blanket formalism allows us to explain the interactive flows at every scale of neural activity in terms of behavior that emerges from the dynamics of the parts of a population. We thereby think that we can answer how single-cell nonlinearities of the individual particles are represented by the individual particles. In the end, by virtue of this scale-free formalism, we explain the interactive flows at every scale of the brain dynamics, which then is an explanation of how neural patterns reflect the situatedness in the environment. The formalism presented here can be further explained by the fact that Markov blanket formalism is a method of And this work is also consistent with network neuroscience and neural connectivity, which is a very important part of the work. The formalism presented here can be further applied to the whole organism and even the organism in its environment as co-varying fluctuations arising from both internal and external sources. And this work is also consistent with network neuroscience and neural connectivity, as well as dynamical causal modeling, such as models of neural mass models, networks of neural masses, and neural field models. Thanks so much for your attention. I'd like also to thank my excellent collaborators. If you have any ideas, questions or insights on this, just flick me an email. One last thing, I'd like to take the opportunity to advertise a few special issues that we have opened at the moment. For example, we have the bio-AI together that we are putting together with Adam, Saffron and Andy Clark in Frontiers of Neuro-Robotics. Or the expecting and perceiving. How does the Bayesian brain hypothesis stand, which we have open in Consciousness and Cognition with Michael Kirschhoff. And we have two coming up exciting special issues. One is going to be edited and published in Neuroscience of Consciousness together with Mark Miller and Julian Kieferstein, titled Experiencing Well-Being and Active Inference Approach to Mental Health and Subjective Well-Being. And the other one that is also in the making is with Thomas Van Ness, Guilherme Sanchez-Yoliveira and Mathew Colombo. And this one is going to be on Scientific Realism and Modeling in Computational Cognitive Neuroscience. So if you're interested in any of these, do flick me an email. Thank you so much.",
        "output": "### Introduction\n\nMark of Blankets in the Brain: How the situatedness in the environment is reflected in neural patterns.\n\n### Two perspectives: Neural representation and complex neural activity\n\n- Neural representation: The brain as a computer, cognition as computations outputting representations.\n- Complex neural activity: Rejecting the traditional computational approach, considering cognitive systems as dynamical systems, and cognitive processes as state-space evolution.\n\n### Markov Blankets in the Brain\n\n- Aim to explain neural activity as interactive flows of brain dynamics, reflecting neural situatedness in the environment.\n- Applied Markov blankets to the brain to explain interactive flows at every scale of brain dynamics.\n\n### Markov Blankets partitioning states\n\n- Active, sensory, and internal states constitute the functional neural unit.\n\n### Applying Markov Blankets to various neural activity scales\n\n- Single neuron and neural networks\n- Neural dynamics in dynamic causal modeling and canonical microcircuit connectivity\n- Recursive parcellation and renormalization group\n\n### Conclusion\n\n- Markov blanket formalism can explain interactive flows at every scale of neural activity.\n- The formalism is consistent with network neuroscience, neural connectivity, and dynamical causal modeling.\n- Upcoming special issues in various journals related to the topics discussed in the presentation."
    },
    {
        "instruction": "<|SYSTEM|>You are a scholars that is taking markdown notes about some video, you would be giving some written scripts for the task.\nYou have finished writing the notes, and you want to extract the title from the notes.\nThe title should only be the name of the technology or algorithm, no extra words needed.\nPlease write the notes in markdown format. No need to write \"Title:\" for the Title. Make sure the title is within 10 words.\nHere is an example:\n## Hidden Markov Model\n<|ASSISTANT|>## ",
        "input": "<|USER|>### Introduction\n\nMark of Blankets in the Brain: How the situatedness in the environment is reflected in neural patterns.\n\n### Two perspectives: Neural representation and complex neural activity\n\n- Neural representation: The brain as a computer, cognition as computations outputting representations.\n- Complex neural activity: Rejecting the traditional computational approach, considering cognitive systems as dynamical systems, and cognitive processes as state-space evolution.\n\n### Markov Blankets in the Brain\n\n- Aim to explain neural activity as interactive flows of brain dynamics, reflecting neural situatedness in the environment.\n- Applied Markov blankets to the brain to explain interactive flows at every scale of brain dynamics.\n\n### Markov Blankets partitioning states\n\n- Active, sensory, and internal states constitute the functional neural unit.\n\n### Applying Markov Blankets to various neural activity scales\n\n- Single neuron and neural networks\n- Neural dynamics in dynamic causal modeling and canonical microcircuit connectivity\n- Recursive parcellation and renormalization group\n\n### Conclusion\n\n- Markov blanket formalism can explain interactive flows at every scale of neural activity.\n- The formalism is consistent with network neuroscience, neural connectivity, and dynamical causal modeling.\n- Upcoming special issues in various journals related to the topics discussed in the presentation.",
        "output": "## Markov Blankets in the Brain"
    },
    {
        "instruction": "<|SYSTEM|>You are a scholars that is taking markdown notes about some video with Wiki style, you would be giving some written scripts for the task.\nAnd summarize each paragraph with a title.\nRemember to keep the technology or algorithm name, very important.\nPlease write the notes in markdown format.\nHere is an example:\n### Definiation\nA deadlock is a situation in which two computer programs sharing the same resource are effectively preventing each other from accessing the resource, resulting in both programs ceasing to function. \nThe earliest computer operating systems ran only one program at a time.\n\n### What it have to do with Relationsal Database ManagementSystem\nIn a database, a deadlock is an unwanted situation in which two or more transactions are waiting indefinitely for one another to give up locks. \nDeadlock is said to be one of the most feared complications in DBMS as it brings the whole system to a Halt. \nWhen you tries to insert or update data on database, sometime your record need to depend on the row record.  \n<|ASSISTANT|>#### ",
        "input": "<|USER|> Today, we're going to talk about BERT, so let's jump into it. This is the transformer neural network architecture that was initially created to solve the problem of language translation. This was very well received. Until this point, LSTM networks had been used to solve this problem, but they had a few problems themselves. LSTM networks are slow to train. Words are passed in sequentially and are generated sequentially. It can take a significant number of time steps for the neural net to learn. And it's not really the best at capturing the true meaning of words. Yes, even bidirectional LSTMs, because even here, they are technically learning left to right and right to left contexts separately and then concatenating them. So the true context is slightly lost. But the transformer architecture addresses some of these concerns. First, they are faster as words can be processed simultaneously. Second, the context of words is better learned, as they can learn context from both directions simultaneously. So for now, let's see the transformer in action. Say we want to train this architecture to convert English to French. The transformer consists of two key components, an encoder and a decoder. The encoder takes the English words simultaneously, and it generates embeddings for every word simultaneously. These embeddings are vectors that encapsulate the meaning of the word. Similar words have closer numbers in their vectors. The decoder takes these embeddings from the encoder and the previously generated words of the translated French sentence, and then it uses them to generate the next French word. And we keep generating the French translation one word at a time until the end of sentence is reached. What makes this conceptually so much more appealing than some LSTM cell is that we can physically see a separation in tasks. The encoder learns what is English, what is grammar, and more importantly, what is context. The decoder learns how do English words relate to French words. Both of these, even separately, have some underlying understanding of language. And it's because of this understanding that we can pick apart this architecture and build systems that understand language. We stack the decoders and we get the GPT transformer architecture. Conversely, if we stack just the encoders, we get BERT, a bidirectional encoder representation from transformer, which is exactly what it is. The OG transformer has language translation on lock, but we can use BERT to learn language translation, question answering, sentiment analysis, text summarization, and many more tasks. Turns out, all of these problems require the understanding of language. So we can train BERT to understand language and then fine tune BERT depending on the problem we want to solve. As such, the training of BERT is done in two phases. The first phase is pre-training, where the model understands what is language and context, and the second phase is fine tuning, where the model learns, I know language, but how do I solve this problem? From here, we'll go through pre-training and fine tuning, starting at the highest level, and then delving further and further into details after every pass. So let's go deeper into each phase. So pre-training. The goal of pre-training is to make BERT learn what is language and what is context. BERT learns language by training on two unsupervised tasks simultaneously. They are mass language modeling and next sentence prediction. For mass language modeling, BERT takes in a sentence with random words filled with masks. The goal is to output these mask tokens. And this is kind of like fill in the blanks. It helps BERT understand bidirectional context within a sentence. In the case of next sentence prediction, BERT takes in two sentences, and it determines if the second sentence actually follows the first, in kind of what is like a binary classification problem. This helps BERT understand context across different sentences themselves. And using both of these together, BERT gets a good understanding of language. Great, so that's pre-training. Now the fine tuning phase. So we can now further train BERT on very specific NLP tasks. For example, let's take question answering. All we need to do is replace the fully connected output layers of the network with a fresh set of output layers that can basically output the answer to the question we want. Then we can perform supervised training using a question answering data set. It won't take long since it's only the output parameters that are learned from scratch. The rest of the model parameters are just slightly fine tuned. And as a result, training time is fast. And we can do this for any NLP problem. That is, replace the output layers and then train with a specific data set. Okay, so that's pass one of the explanation on pre-training and fine tuning. Let's go on to pass two with some more details. During BERT pre-training, we train on mass language modeling and next sentence prediction. In practice, both of these problems are trained simultaneously. The input is a set of two sentences with some of the words being masked. Each token is a word. And we convert each of these words into embeddings using pre-trained embeddings. This provides a good starting point for BERT to work with. Now on the output side, C is the binary output for the next sentence prediction. So it would output one if sentence B follows sentence A in context and zero if sentence B doesn't follow sentence A. Each of the T's here are word vectors that correspond to the outputs for the mass language model problem. So the number of word vectors that we input is the same as the number of word vectors that we output. Now on the fine tuning phase though, if we wanted to perform question answering, we would train the model by modifying the inputs and the output layer. We pass in the question, followed by a passage containing the answer as inputs, and in the output layer, we would output the start and the end words that encapsulate the answer, assuming that the answer is within the same span of text. Now that's pass two of the explanation. Now for pass three, where we dive further into details. This is going to be fun. On the input side, how are we going to generate these embeddings from the word token inputs? Well the initial embedding is constructed from three vectors. The token embeddings are the pre-trained embeddings. The main paper uses word piece embeddings that have a vocabulary of 30,000 tokens. The segment embeddings is basically the sentence number that is encoded into a vector. And the position embeddings is the position of a word within that sentence that is encoded into a vector. Adding these three vectors together, we get an embedding vector that we use as input to BERT. The segment and position embeddings are required for temporal ordering, since all these vectors are fed in simultaneously into BERT and language models need this ordering preserved. Cool, the input is starting to piece together pretty well. Let's go to the output side now. The output is a binary value C and a bunch of word vectors. But with training, we need to minimize a loss. So two key details to note here. All of these word vectors have the same size and all of these word vectors are generated simultaneously. We need to take each word vector, pass it into a fully connected layered output with the same number of neurons equal to the number of tokens in the vocabulary. So that would be an output layer corresponding to 30,000 neurons in this case. And we would apply a softmax activation. This way, we would convert a word vector to a distribution. And the actual label of this distribution would be a one-hot encoded vector for the actual word. And so we compare these two distributions and then train the network using the cross entropy loss. But note that the output has all the words, even though those inputs weren't masked at all. The loss, though, only considers the prediction of the masked words and it ignores all the other words that are output by the network. This is done to ensure that more focus is given to predicting these mass values so that it gets them correct and it increases context awareness. So that was the three passes of explaining the pre-training and fine-tuning of BERT. So let's put this all together. We pre-train BERT with mass language modeling and next sentence prediction. For every word, we get the token embedding from the pre-trained word piece embeddings, add the position and segment embeddings to account for the ordering of the inputs. These are then passed into BERT, which under the hood is a stack of transformer encoders. And it outputs a bunch of word vectors for mass language modeling and a binary value for next sentence prediction. The word vectors are then converted into a distribution to train using cross entropy loss. Once training is complete, BERT has some notion of language. It's a language model. The next step is the fine-tuning phase, where we perform a supervised training depending on the task we want to solve. And this should happen fast. In fact, the BERT squad, that is the Stanford question and answer model, only takes about 30 minutes to fine-tune from a language model for a 91% performance. Of course, performance depends on how big we want BERT to be. The BERT large model, which has 340 million parameters, can achieve way higher accuracies than the BERT base model, which only has 110 parameters. There's so much more to address about the internals of BERT that I could go on forever. But for now, I hope this explanation was good to get you an idea of what BERT really does under the hood. For more details on the transformer neural network architecture, which is the foundations of BERT itself, click on this video. Subscribe and stay safe. A lot more content coming your way soon. And I'll see you soon. Bye bye!",
        "output": "#### BERT Introduction and Transformer Architecture\nBERT (Bidirectional Encoder Representations from Transformers) is a powerful architecture developed to address problems with language translation, with foundations in the transformer neural network. Transformers addressed issues with LSTM, primarily improving speed and context understanding. Transformers include two key components: the encoder, which processes words simultaneously and generates word embeddings, and the decoder, which converts embeddings to translated words in the target language.\n\n#### BERT for Different NLP Tasks\nBy stacking only encoders, BERT benefits multiple NLP tasks like language translation, question answering, sentiment analysis, and text summarization. BERT training has two phases: pre-training (language and context understanding), and fine-tuning (solving the specific NLP task).\n\n#### Pre-training Phase\nBERT learns language through mass language modeling (MLM) and next sentence prediction (NSP). MLM involves predicting masked words in a sentence, helping BERT grasp bidirectional context. NSP requires BERT to predict whether a second sentence follows the first, extending context understanding beyond a sentence.\n\n#### Fine-tuning Phase\nFine-tuning entails training BERT on specific NLP tasks. Replacing output layers and training with corresponding datasets allow BERT to quickly adapt to problems like question answering, sentiment analysis, and text summarization.\n\n#### More Phases Details\nDuring pre-training, BERT receives embeddings that include token, segment, and position embeddings. Token embeddings come from pre-trained word embeddings like WordPiece, while segment and position embeddings incorporate sentence and word ordering. The output contains word vectors to train using cross-entropy loss, focusing on masked words. Once trained, BERT can be fine-tuned for targeted tasks.\n\n#### Conclusion\nBERT revolutionized NLP tasks by effectively learning language and context through pre-training then fine-tuning for specific tasks. Its powerful transformer neural network architecture laid the foundation for understanding language and advanced question answering, sentiment analysis, text summarization, and more."
    },
    {
        "instruction": "<|SYSTEM|>You are a scholars that is taking markdown notes about some video, you would be giving some written scripts for the task.\nYou have finished writing the notes, and you want to extract the title from the notes.\nThe title should only be the name of the technology or algorithm, no extra words needed.\nPlease write the notes in markdown format. No need to write \"Title:\" for the Title. Make sure the title is within 10 words.\nHere is an example:\n## Hidden Markov Model\n<|ASSISTANT|>## ",
        "input": "<|USER|>#### BERT Introduction and Transformer Architecture\nBERT (Bidirectional Encoder Representations from Transformers) is a powerful architecture developed to address problems with language translation, with foundations in the transformer neural network. Transformers addressed issues with LSTM, primarily improving speed and context understanding. Transformers include two key components: the encoder, which processes words simultaneously and generates word embeddings, and the decoder, which converts embeddings to translated words in the target language.\n\n#### BERT for Different NLP Tasks\nBy stacking only encoders, BERT benefits multiple NLP tasks like language translation, question answering, sentiment analysis, and text summarization. BERT training has two phases: pre-training (language and context understanding), and fine-tuning (solving the specific NLP task).\n\n#### Pre-training Phase\nBERT learns language through mass language modeling (MLM) and next sentence prediction (NSP). MLM involves predicting masked words in a sentence, helping BERT grasp bidirectional context. NSP requires BERT to predict whether a second sentence follows the first, extending context understanding beyond a sentence.\n\n#### Fine-tuning Phase\nFine-tuning entails training BERT on specific NLP tasks. Replacing output layers and training with corresponding datasets allow BERT to quickly adapt to problems like question answering, sentiment analysis, and text summarization.\n\n#### More Phases Details\nDuring pre-training, BERT receives embeddings that include token, segment, and position embeddings. Token embeddings come from pre-trained word embeddings like WordPiece, while segment and position embeddings incorporate sentence and word ordering. The output contains word vectors to train using cross-entropy loss, focusing on masked words. Once trained, BERT can be fine-tuned for targeted tasks.\n\n#### Conclusion\nBERT revolutionized NLP tasks by effectively learning language and context through pre-training then fine-tuning for specific tasks. Its powerful transformer neural network architecture laid the foundation for understanding language and advanced question answering, sentiment analysis, text summarization, and more.",
        "output": "## BERT"
    },
    {
        "instruction": "<|SYSTEM|>You are a scholars that is taking markdown notes about some video with Wiki style, you would be giving some written scripts for the task.\nAnd summarize each paragraph with a title.\nRemember to keep the technology or algorithm name, very important.\nPlease write the notes in markdown format.\nHere is an example:\n### Definiation\nA deadlock is a situation in which two computer programs sharing the same resource are effectively preventing each other from accessing the resource, resulting in both programs ceasing to function. \nThe earliest computer operating systems ran only one program at a time.\n\n### What it have to do with Relationsal Database ManagementSystem\nIn a database, a deadlock is an unwanted situation in which two or more transactions are waiting indefinitely for one another to give up locks. \nDeadlock is said to be one of the most feared complications in DBMS as it brings the whole system to a Halt. \nWhen you tries to insert or update data on database, sometime your record need to depend on the row record.  \n<|ASSISTANT|>#### ",
        "input": "<|USER|> Hey YouTube, today we're going to be going over one of the most powerful tools we have in our tool set for training and fine tuning large language models, Lora's. We're going to be covering how do Lora's work, what are they, and how do we actually implement and use Lora's to fine tune our large language models using Oogaboo's text generation web UI. If you'd like to skip the conversation about how do they work, you can skip to this timestamp, but if you'd like to learn how they work under the hood, stay tuned. So let's get started. So why Lora is a big deal is because it's essentially doing a dimensionality reduction. And why this is the case is that during fine tuning tasks, we find that these generative models are really just tuning some lower dimensional set of their parameters. So why retrain the entire network or a layer when we can just create our own lower dimensional representation? So this is taking advantage of a concept in linear algebra called rank decomposition, where the most common type we're taught is singular value decomposition. And to understand why this works, we just need to understand one concept in linear algebra. And that's when can we multiply matrices. So the dimensions, if you say an M by N matrix, what we're talking about is how many rows and columns are there in the matrix. So if we have a two by two, we would have four values in the matrix. So you get the number of values by just multiplying these dimensions. So M by N equals Z, which is the dimensions or the number of elements in that matrix. So if we want to multiply two matrices, the only thing we need is that their inner dimensions match. So if we have two matrices, for example, four by one and a one by four, we can multiply them because their inner dimensions match. And we end up with a four by four. So if we take this and extend it to rank decomposition, what it says is if we have a matrix A, which is M by N, we can represent it by three different matrices that have dimension M by R for B, R and R for C and R and M for D. And if we multiply them together, we just get an M by N matrix back. And this is powerful because memory matters. So if we start with a matrix that has the elements one, two, three, four, two, four, six, eight, we can represent this exactly with these two vectors, which are four by one and one by four. And if we multiply them together, we get this four by four back. And instead of having to store 16 values, we're storing eight values. But we don't need to represent things exactly. Approximation is also okay. So if we start with a matrix A that's 500 by 100, that has 50,000 values in it. But instead of having to store all of that, we could just approximate the matrix A by having a matrix B that has dimensions five by 10 and a matrix D that has dimensions 10 by 100. The inner dimensions match. And if we ever multiply these together, we get a 500 by 100 matrix back, but instead we're just having to store 6,000 values. And these memory savings get bigger and bigger the more numbers, the bigger the dimensions of the matrices are. So how these LoRa matrices play into the generative network is we attach the LoRa matrices to the feed forward portion of our transformer model. Now they could attach other places, but we typically attach them to the feed forward portion. And we're going to hold the weights in the feed forward layer as being fixed. And instead we're going to train and update the weights inside of the matrices for these LoRa's. And all the LoRa has to do is have its outer dimensions match the dimensions of the feed forward network, which in this case are N. But the inner dimensions can be M, which can be much, much less than N. And remember, if we multiply N by N, then we get the total number of elements. And that can be much, much less, so we're not dealing with the same memory constraints. So when these two multiplications are done by the feed forward and by the LoRa's, we just add their results together and we influence the output weights. And this dimensionality reduction is done all over the place through principal component analysis or auto encoders. And we're just now applying it to our large language models or our generative models. So now let's move forward and see how we can actually use LoRa's to help fine tune your language model. Today we're gonna be using the Ugabooka text generation web UI, and we're just gonna go under the training tab. So once we go under this tab, we're gonna be presented with a lot of different options. But before we set any of these and we will go over them, we wanna make sure we have our training data. And that can be presented in two forms, as a raw text file or as a formatted data set. And the raw text file is just that. It's a splattering of text that we can use to give the network new information or teach it something new. But the formatted, we can teach the network something new about its behavior in this kind of instruction output format. In this case, I have the user asking, hello, I would like to speak to you about your car's extended warranty. And the assistant replying, what about my car's extended warranty? So these will be in the description below because I just wanna help everybody know kind of how to format these things. So now we can actually go ahead and start talking about what these hyperparameters do and how they affect your training. So the first thing we wanna do is give it a name. In my case, I'm calling it Amon's Example. And you can copy them from another set of LORAs or your own previous LORAs as well. But batch size. Now batch size plays into the epoch. And remember, for each epoch, the network will see the totality of your training set. But the batch size says how much of it do you want to see at once. And higher values can lead to overfitting or memorization issues, but lower values can lead to longer training periods. But we can also do the microbatch, which if you're having VRAM issues, a microbatch can help save memory by chunking it out a little further. Epoch. So the epoch describes how many times is the network going to see the totality of the data set during the training process. Learning rate, how quickly are the weights gonna be updated? And the default value tends to work pretty well. The learning rate scheduler updates the learning rate during the training process. And I tend to use linear or cosine, but you may experiment with that for your data set to see what performs best. The Lora rank describes the dimensionality of the Lora matrices. So higher values can lead to significantly more fine results, but use a lot more memory. Whereas smaller values will use a lot less memory, but you can also get good results out of it. It really depends on the complexity or data set and how much change in the behavior you're looking for. The Lora alpha describes how much influence does the Lora have on the feedforward portion of the network. Higher values result in much greater impacts. Lower values result in lower impacts. So if you think your Lora is having too much of an impact, you can always try backing it off. And then cutoff length has another big impact on memory. It's how much text is presented at once. And so if you are having issues with training memory, backing that off can help quite a bit. So the other thing about the training data is also, once we go to load it, we wanna make sure they're in the correct extension. Our training format or our format, it needs to be a JSON. And then our text just needs to be a .txt, our raw text. So once we have that loaded, we can go ahead and select it. We wanna have a training and a validation set. And, oh, I don't know why that one, there we go. And we can go ahead and select our validated format. And then we just need to tell it what the format we're gonna be using is. In my case, I'm gonna be using an alpaca format. And then how many times do I want to, how many steps do I wanna take before evaluating my network's performance? And there are some advanced options as well before we start. And that's namely the LoRa dropout. The rest of these options, I don't think, need to be touched very much. But the dropout can be important because it helps to prevent overfitting. So you might have to toy with this value if you notice that your network is overfitting or memorizing. But once we have that, we just click on start LoRa training. And that's all we have to do. If this was helpful, please like and subscribe and please let us know in the comments what you'd like to learn about next. And join us next time when we're gonna be continuing our conversation about how the large language models work under the hood, specifically positional encoding and embedding spaces.",
        "output": "### Introduction to LoRas\nLoRas are powerful tools for training and fine-tuning large language models. They use dimensionality reduction to efficiently update a lower-dimensional set of parameters during fine-tuning tasks. By applying linear algebra concepts like rank decomposition, they reduce the memory requirements during training.\n\n### LoRas and Linear Algebra\nLoRas employ rank decomposition, such as singular value decomposition, to represent matrices with lower-dimensional matrices that can be multiplied together. This results in memory savings, as the number of elements stored is significantly reduced.\n\n### Implementing LoRas in Language Models\nLoRas are typically attached to the feedforward portion of the transformer model in large language models. The weights in the feedforward layer are fixed, and the weights in the LoRa matrices are updated. By applying dimensionality reduction techniques like principal component analysis, the memory requirements are further reduced.\n\n### Fine-tuning Language Models with LoRas using Oogaboo's Web UI\nOogaboo's text-generation web UI can be used for training and fine-tuning language models with LoRas. Users can import a raw text file or a formatted dataset, adjust hyperparameters like batch size, epoch, learning rate, LoRa rank, LoRa alpha, cutoff length, and dropout. After setting up the training and validation sets and choosing the appropriate format, users can start LoRa training.\n\n### Conclusion\nLoRas provide an efficient way to fine-tune large language models with reduced memory requirements. By understanding the underlying linear algebra concepts and using tools like Oogaboo's text-generation web UI, researchers and developers can effectively train and customize their language models."
    },
    {
        "instruction": "<|SYSTEM|>You are a scholars that is taking markdown notes about some video, you would be giving some written scripts for the task.\nYou have finished writing the notes, and you want to extract the title from the notes.\nThe title should only be the name of the technology or algorithm, no extra words needed.\nPlease write the notes in markdown format. No need to write \"Title:\" for the Title. Make sure the title is within 10 words.\nHere is an example:\n## Hidden Markov Model\n<|ASSISTANT|>## ",
        "input": "<|USER|>### Introduction to LoRas\nLoRas are powerful tools for training and fine-tuning large language models. They use dimensionality reduction to efficiently update a lower-dimensional set of parameters during fine-tuning tasks. By applying linear algebra concepts like rank decomposition, they reduce the memory requirements during training.\n\n### LoRas and Linear Algebra\nLoRas employ rank decomposition, such as singular value decomposition, to represent matrices with lower-dimensional matrices that can be multiplied together. This results in memory savings, as the number of elements stored is significantly reduced.\n\n### Implementing LoRas in Language Models\nLoRas are typically attached to the feedforward portion of the transformer model in large language models. The weights in the feedforward layer are fixed, and the weights in the LoRa matrices are updated. By applying dimensionality reduction techniques like principal component analysis, the memory requirements are further reduced.\n\n### Fine-tuning Language Models with LoRas using Oogaboo's Web UI\nOogaboo's text-generation web UI can be used for training and fine-tuning language models with LoRas. Users can import a raw text file or a formatted dataset, adjust hyperparameters like batch size, epoch, learning rate, LoRa rank, LoRa alpha, cutoff length, and dropout. After setting up the training and validation sets and choosing the appropriate format, users can start LoRa training.\n\n### Conclusion\nLoRas provide an efficient way to fine-tune large language models with reduced memory requirements. By understanding the underlying linear algebra concepts and using tools like Oogaboo's text-generation web UI, researchers and developers can effectively train and customize their language models.",
        "output": "## LoRas"
    },
    {
        "instruction": "<|SYSTEM|>You are a scholars that is taking markdown notes about some video with Wiki style, you would be giving some written scripts for the task.\nAnd summarize each paragraph with a title.\nRemember to keep the technology or algorithm name, very important.\nPlease write the notes in markdown format.\nHere is an example:\n### Definiation\nA deadlock is a situation in which two computer programs sharing the same resource are effectively preventing each other from accessing the resource, resulting in both programs ceasing to function. \nThe earliest computer operating systems ran only one program at a time.\n\n### What it have to do with Relationsal Database ManagementSystem\nIn a database, a deadlock is an unwanted situation in which two or more transactions are waiting indefinitely for one another to give up locks. \nDeadlock is said to be one of the most feared complications in DBMS as it brings the whole system to a Halt. \nWhen you tries to insert or update data on database, sometime your record need to depend on the row record.  \n<|ASSISTANT|>#### ",
        "input": "<|USER|> Hello community. Today we talk about parameter efficient fine tuning. And one method in parameter efficient fine tuning is LoRa. Let's focus on this today. And the main idea behind this, we have huge models where we have a lot of VRAM. How can we reduce here our model that even with a less powerful GPU, we can do the calculation, for example, fine tuning an LLM or do a stable diffusion job. So here we go. Pathed LoRa, when we look at the use cases, we can tune different transformer models. For example, if you say from Big Science, the company that used Bloom, you have here the T0 model. This is the T5XL model for zero shot optimization. And you want to integrate this with hugging face, accelerate and with deep speed with a zero three offloading. And if you look at this file, I will show you the code and one of the next videos, you have some zero optimization that offloads the optimizer memory and the computation from the GPU to a CPU. Why? Because deep speed has a specific atom optimizer for CPU that is really fast. So we can do quite a lot of things here to reduce our memory footprint. Or if you go for integer eight tuning here of some models, and we want to do it in a free Google Colab notebook, we can use here hugging face pathed plus LoRa plus bits and bytes. Bits and bytes is a lightweight wrapper. There are some CUDA custom functions on your graphic code and the CUDA cores, the tensor cores. In particular, it is an 8-bit optimizer. We are talking about matrix multiplication and quantification functions. So, and if we look in those programs, the first line you will encounter is from pathed import LoRa configuration and get your pathed model. And you might ask, hey, so what is my optimal LoRa configuration for my model and what the hell is a pathed model? So here we go. Let's understand it. Three points. At first, we're going to talk about adapter transformers. This is an extension of hugging face transformers that will give us a very beautiful explanation of what is LoRa. And you will see that LoRa is integrated in a new library by hugging face. And this library is called pathed parameter efficient fine tuning. Where the hugging face library takes here a pre-trained LLM and freezes most of the parameters. Now, LoRa does not a particular job that it takes those frozen pre-trained model weights and injects trainable matrices into each layer of the transformer. So we modify the architecture of our transformer. And since we have transformer in LLM, in vision, in stable diffusion, in whisper, in vision transformers, you see this method here has a wide array of application. So let's start at the beginning. In June 2019, Holtzby published here parameter efficient transfer learning for NLP. And he more or less said fine tuning involves copying the weights from a pre-trained network or transformer network and tuning them on a downstream task, which produces a new set of weight for each task. So multitask learning requires simultaneous access to a lot of tasks. And therefore, we need a lot of memory. It is a very memory intensive fine tuning. He came up with or just came up with the idea of adapters. An adapter, they discover, yields some parameter efficient tuning for NLP. In particular, it has a very nice feature. It permits training fine tuning on task sequentially. So what are those adapters? It involves here adding a small number of new parameters to a model, which are then trained on the downstream task. So in adapter tuning, so remember we have fine tuning and we go now here for adapter tuning because the difference is all the parameters of the original pre-trained network or transformer or LLM or vision transformer are frozen. They are kept identical and therefore can be shared by many tasks. And we have only here our adapters that we are going to adapt a tune now to our downstream task. You might say, OK, can you show me a visualization? Here we are. So adapter tuning involves for the task of fine tuning injection of new architectural layers into our transformer architecture. And you say, finally, transform it was so easy. It was so trivial, so simple. Finally, we have here a little bit more that we want to understand. So here we go. So, for example, here on the decoder, you have your multi-headed attention. You have your feed forward layer. And then you see here a new layer, what's called an adapter. Or you have here now in this structure here, feed forward layers and then also an adapter. So what they adapt to do is more or less project here the maps, the feature size back to the size of the layer input. Let's have a look at this maybe in a little bit more detail. There are now for each task you have now you can train now a specific adapter. And if you have, I don't know, 100 tasks, you will get 100 adapters. So the idea was here by Pfeiffer to establish a hub where all the adapters from all the models that were adapted tuned can be accumulated and are available for everybody else. So this was the idea of adapter hub, a framework for adapting transformers. Yes, yes, yes. If you want, you read the literature, but it was very easy to implement. So from the transformer, you choose your model and your adapter type. Then you had your classification from a pre-trained here, for example, a Roberta base model. And then you just said model dot add adapter. You had here your specific training data set. You had to choose an adapter type. I'll show you in a second what I mean. And a specific configuration also show you in a second. And then you just said model dot train adapter on my specific data set for this adapter tuning of the LLM of the transformer architecture. And then the beautiful thing was, since everything else was frozen, only the adapter parameters were trained. You could then extract the adapter weight values out of the transformer architecture and said save only the adapter weights, only the trained weights of this model. And then you could upload it here, for example, to the adapter hub. And you say for a particular model Roberta base with a specific data set, as a statue with a specific configuration of my adapter. Then this is the adapter for a text task as a statue and everybody could use this adapter. So this was the adapter hub. Yes, yes, yes, trained, you know everything, you can read it in detail. Now there are two famous architecture. Very simple. We have the original Holtzby architecture with two adapter in our layer. And then we have the Pfeiffer architecture with just one adapter. Find out, yeah, although it's possible you don't need two, just one here is sufficient for the performance. If you want to have here, look at the detail, you find this on the left side. But let me show you the training process, the benefit it provided. So you had a model and you want now to have here an adapter tuning of this particular model. So you choose your adapter and you run here is adapter tuning with this particular data set on your machine. And after the adaptive tuning is done, you can extract here the adapter weights, the training adapter, let's call it this, and feed this, I don't know, one megabyte size data set back here into our hub. And then another person says, yes, I load now the same model. And now I can also download the pre-trained adapter weights here. So my inference task is now optimized for the downstream task that this adapter was trained here by this person. So on this hub here, a lot of adapter for a lot of different data set for a lot of different task accumulated. Beautiful. And you might say, is this now pathed? No. This is here adapter hub, but it is very close. It is very similar. Let me show you. So what we had is now we could add this stitching in task specific tuned adapter into our transformer models. So we injected here our adapter, our little additional layers here in our transformer network. And as I showed you, it was done simple model. If you had an adapter trained adapter, you just said model load this adapter with the data set with the configuration and you could use it. Beautiful. So now this adapter had a very particular beneficial function. Now, if we use fine tuning, the classical fine tuning, and we fine tune on one task and two tasks and 100 tasks and 300 tasks, this sequential fine tuning and multitask learnings are method aiming to incorporate more and more knowledge from multiple tasks in your model. However, they suffer here from something that's called catastrophic forgetting. If you have 100 data sets and you have 100 different tasks and you go sequential fine tuning, sequential fine tuning, sequential fine tuning, sometimes the model overrides and starts to forget what it learned 100 tasks before. So to address this shortcoming adapter were a perfect solution. And here, FIFAR again, propose here adapter fusion. This is now a new two stage learning algorithm that leverages knowledges for multiple tasks. So what it does very easily, you have one adapter tuning here of your transformer for one particular task, you extract one adapter. You have a second task with a second data set, you extract the second adapter. Let's do it here. This is magnified. And then if you have three, five, ten tasks with 10 data sets, you extracted from this model 10 adapters. And now with adaptive fusion in January 2021 was now the beauty. We could circumvent this catastrophic forgetting with adaptive fusion. We had a beautiful solution if we want to have 10 specific tasks in your company or 20 specific tasks with adaptive fusion. You could just use the adapter like a module, put it in, do an adaptive fusion and the model was trained either on three specific tasks or on 20 specific tasks. Now I know you want to know, hey, show me here the weight matrices here in our self attention layer, because this is where our value of our key and our query weight matrices. How exactly is the mechanism? And here I have, of course, the original literature for you. But if you just want to see it here, our adapter here, how it is done, you have here the multiple stacked adapters from your particular training task. And then, as you can see here, let's read it. This includes learnable weights, the weight matrix, query, key and value. Query takes the input as input, the output from the pre-trained transformer weight. So here we have a step to the query weight matrix. Both key and value weight matrix take as input the output of the respective adapters. So you see here input comes here from our adapter stack to key and value and the dot product of the query with all the keys is passed on into a softmax function, which learns to weight the adapter with respect to the context. So you see everything that we know very easy. We have the same self attention mechanism. We have the same value key query weight matrixes. We have the same softmax functionality. But we found here with the injection of these adapters a way to train the models on one particular task with one adapter faster, because all the other pre-trained weights are frozen. And we can then add, if you want the adapter 1, 3, 5, adapter 2, 114, you can configure your optimal configuration of your LLM. Now, check mark, first point adapter transformer. We understand what are adapter transformers and we understand what the adapter hub is doing and it is doing it today. And it is a beautiful framework for exchange of adapting transformers. Now, the next part and this is Laura. There are a lot of questions about the exact mathematics behind Laura. What is it really, really doing if you make a deep drive and use real read sentences like here a random projection to a small subspace and so, but we now look here at the mathematics of Laura to understand how we can have the perfect Laura configuration file for your PEVT model. And remember PEVT is the beautiful hugging face library. Now you understand that this PEVT library and here the adapter hub, those are very similar. Let's call it libraries, but on hugging face, of course, you have available, let's say within the platform, all the beautiful models and all the beautiful data sets. So this you would load to have to load extra in the adapter hub, but I'll show you in a second. So we go here to Laura. Laura, if you want to understand it, is a low rank adaptation of LLMs. And you might say, what the hell is a low rank adaptation? Now, if I tell you something, just change one character, you will immediately recognize what it is. But it was done October 2021. We have here the paper, read the paper. Laura does something that we know now from the adapters. It freezes the pre trained model weights. And injects, now we know what injects means into our layer structure of our transformer and inject some trainable elements, exactly like an adapter. They are also, of course, since all AI is matrix multiplication, they are also objects. Those are matrices. But now, and this is not an interesting part, you have now a rank decomposition. Now, if I tell you, yeah, okay. So in the research paper, they use exactly here in the self attention mechanism, they use here that the query weight tensor, the key and the value and the output projection matrices. And then they say, during the full classical fine tuning, the model is initialized to pre trained weights phi and are all updated to phi plus a delta phi. So repeatedly, we have a model that is pre trained. And we have a model that is pre trained. And we have a model that is pre trained. So, this is a model that is pre trained weights, phi and are all updated to phi plus a delta phi. So repeatedly following the gradient and you see now they start to separate this into, if you want, perturbation theory. You have here the main master term and then you have a small delta, a small increment. And this is more or less already half of the idea what they're doing. So, they say one of the main drawbacks for a full classical fine tuning is that for each downstream task, we learn a different set of parameters, delta phi. As I showed you before with the adapters, for each different task, you learn a complete set of parameters of weights of activations for this specific training data set that you have in your fine tuning. And Laura adopts now a more parameter efficient approach. You're not going to believe it, the same as the adapter where the task specific parameter increment, now the delta phi, is now further encoded in the delta phi of theta. So, what they do here is they say, okay, let's freeze here the main components, the main weights, the heavy weights that come from the pre training of the LLM, from the pre training of the vision transformer, from the training here of the stable diffusion. And then just let's look only here at delta phi. Let's only look at this little bit extra term because pre training did the main chunk of the training. And then they say fine tuning is just a little bit extra and fine tuning is specific to a task. So this here is just a little bit add on. And for this add on, they focus now on this add on and they say we use here from mathematics from linear algebra and you say, yippie, mathematics, finally. We use here the low rank representation of a matrix to encode delta phi. And you may say, hey, but how do you know the delta phi is exactly a dimensional equivalent matrix to phi? Well, of course it is because if you do the classical fine tuning, all the weights are changed accordingly and the dimension has to be the same like the master term here. So you know here we have here the same rank of this matrix here and we have a low rank representation of a matrix and there we can use our matrix. And we can use our mathematical knowledge. So if you read here in the original paper, this is now from their paper from their archive preprint, please read the paper. They now say here exactly this where they have here the pre trained weight matrix W0. So we have here now this W0 plus the delta W, this delta something, this little bit that comes now from the classical fine tuning. But now we will have now a trick for our adapter fine tuning or our lore fine tuning, however you want to call it. We freeze here W0 so we make it not trainable and the only trainable part in our architecture, in our transformer network, will be here those additional terms that we call delta V, delta W. Now what is delta W? We know it's a matrix and we know it's a matrix with a high dimension and a high rank. Let's say the rank is, I don't know, 11,000. Then we know finally from linear algebra and now I can tell you what I sort of could tell you. We have here a singular value decomposition that breaks down more or less any matrix A into this structure. You know it from linear algebra. So in linear algebra we have here a left singular vector matrix U. Find the left singular vectors, these are the columns of U, by finding an orthonormal set of eigenvectors of A transpose A. We have here at the end V transpose. V, find the right singular vectors, the columns of V, by finding an orthonormal set of eigenvectors of A transpose A. And in the middle, and this is the beauty here, we have here diagonal matrix, only a diagonal matrix S. And we have here in the diagonal this most important singular values starting from top. And those singular values, and you know what they are? They are the square roots of the eigenvalues of A transpose. So you know this from linear algebra. And this is now, yeah, if you want the Eckert-Young theorem, if you want to study it a little bit closer, if you want to have a mathematical proof that it is really applicable here in our case, Eckert-Young theorem. And now you see here, if you now decide you do not go with the full-fledged singular values over the complete rank of the matrix, but you terminate here, if you have a diagonal matrix, you just take the first, the second, and the third element of the diagonal, and you neglect the rest of the diagonal. This is exactly how we get Lora, because they truncated now the SVD of a higher-ranked matrix to get a low-rank approximation, a low-rank adaptation. And this is the mathematical access to Lora. So again, a truncated SVD, linear algebra, and we find now a reduced rank approximation to our matrix A or our matrix W of all the weights, the delta W that are now trainable weights, and the rest of the W0 are frozen weights. And as I told you, it is done by setting all, but the very first and the K largest singular value in the diagonal, so the rest here we set equal to zero. And when we do the calculation of U, matrix calculation S, matrix calculation V transpose here, we only use now the first K largest singular values, the first K columns of U and V. So in the diagonal, the first top three, the top five singular values we're going to use, and we have exactly a reduced rank approximation of the matrix. And you say, brilliant! And this is the core of Lora, replacing the diagonal matrix by zeros and the smallest singular values in the diagonal of S. And for the K, and this is now K as a factor in the Lora configuration, and now you know what K is. Yeah, the beauty is, after this fine tuning of our two matrices, the low rank matrices can be reconstructed into a full rank matrices using the same decomposition technique, and you have the same adapted LMM can be used for the downstream task. Now let me show you this in a very short program. You have here a sine pi, linear algebra, SVD, and a rank of a matrix. And if we create a matrix with 60 rows and 100 columns, I can tell you the rank is of course 60. There are no linear dependencies within this matrix. And then we just, as I told you, SVD of our matrix, we get a matrix U, this diagonal singular matrix with our singular values and the transpose. And here I show you only here the singular diagonal array. And let's say we choose rank A. So in our diagonal, we just choose the first eight elements, and the rest of the diagonal we set zero. And here you see all diagonal elements, but you see it's ranked. The first one has the highest rank, 38. The second one is already 4, 4.9, 4.8, 4.6, 4.4, 4.4. And if I take here the rank eight, so I take this, this, this, this, this, the first eight elements of our diagonal, I multiply with retranspose and with U, and I get an approximation of the matrix when I use rank A, low rank decomposition adaptation. So you might say beautiful. And now I can use this because what we achieved is kind of a compression. So where can we use it? We can use it in compression. If you have normally without Laura, a memory intensive, something array to store. Now we can use it by decomposing the matrix into two lower rank matrices. We can represent a matrix now in a more compressed form, which will save us memory and speed up our operation. And this is what we're going to use in LLMs. This is what we use in stable diffusion. So this is Laura for memory. But if you want, if you go now for image, we have a noise reduction. Because if the matrix has some level of noise or some measurement error, those low rank approximation obtained by truncating the singular value decomposition, you know, after the first eight diagonal matrix elements, you just truncate and the rest you set zero. You can help reduce the impact of this noise because you just neglect the smaller, noisier part in the image. And of course, if you have now data analysis, for example, image analysis, you can capture the most important feature of an image. Not bad for stable diffusion. Again, mathematics, you have one huge metric and you can decompose it into smaller one. Or if you want a visual interpretation of this, we have here a door. And this here is now our huge dimensional 10,000 times 20,000 matrix with all the different weights now from that fine tuning process to the matrix with all the different weights now from that fine tuning process. And let's say this is our GPU and you see this is simply too mighty the data set that it would fit on a single GPU. So what we do, we apply here our SVD, we truncate it at a certain level, we get two matrices B and A return. And these two matrices will fit neatly in our door. In our GPU. And when the calculation is done, when this has been adapter trained or a lower trained and this also has been trained, we take this two, we multiply it back together to the huge matrix. And we have achieved something beautifully. We train this huge monster here on a GPU with this simple SVD application. Again, mathematics, SVD, lower, lowering approximation. Yes, M is approximated by M, Gilde. Given the rank. I want to show you here the last time that it was really into. You have here a picture 128 to 128 pixels. If you take this now here, this matrix, and you calculate the singular values. And here on the x axis, you have to rank, of course, maximum is 128. You see the singular values, the most dominant part is here, the blue line. Look, here at let's say 10, rank 10 here, rank 20. You have here more or less all the information here. Already on this rank and the rest here, if you say 20 plus is more or less here the end tail is real where you have the noise, where you have here not the perfect gray, but a little bit washed out gray. All this is happening here. And this is exactly what we use here. You see to have here a more memory efficient model. You can have here the culminated information. You can calculate this and you see, for example, with 20 rank 20 or rank eight, you have here 70% of the whole information of an image already included. And this is the beauty of SVD. Now you know Lora. Now you know that it freezes here the pre trained model weights and injects here our trainable matrices into each layer of the transformer. And you know that this matrices here now they have this very special application of linear algebra that they have this low rank decomposition trick where you define a rank. And you have an approximation to this matrix. Now you say, what is the performance if we do this to a model? You have here the GPT-2 because GPT-3 is not available. With the adapter with the very first one I told you here at the beginning of this talk and then you have here GPT-2 with Lora. And then you have GPT-T the official the trainable parameters 770 million trainable parameters. You see with the adapter we only have 0.8 million and with Lora we have 0.77 million. Now you see with the adapter and here we have 12345 benchmarks and just look at the comparison of the benchmark data. Here with the adapter you have 69.1 here you have 70.4 here you have 8.68 here you have 8.89 here you have 46.3 and 46.8. 2.49 2.47 so you see it is relatively comparable if you have those. So just to show you that performance and if you compare this now here to here this GPT-2 of course GPT-2 is not really competitive anymore but just as a comparison you see here if you have the classical fine tuning. Numbers published in prior work okay we do not know exactly what this is but you see we are absolutely here comparable in the performance benchmark data here to a normal fine tuning. Especially if we have not enough training data if we do not have tens of thousands of training data for supervised fine tuning this is a beautiful methodology to be small fast smart and have the same performance. So as I told you we have here as an institution let's say the adapter hub the adapter transformer this here is as you can see the adapter methods you have now in the adapter methods Lora. You have prefix tuning as I showed you in my other videos already as a methodology in adapter methods so Lora is not something new exists now for close to two years already as an operational model here in the adapter transformer. But of course and as you can see here we have here again our transformer architecture and we have here our rate matrixes the query the key and the values and the beauty is now with Lora. Since it takes a matrix and it just finds a low rank adaptation of this particular matrix that it can then recompose you can take any particular tensor tensor or matrix that you have here and you can use Lora on this and this is why it's almost general. Available and you can use it almost on every model so this is here the simple trick from linear algebra you decompose here a huge matrix into eigenvector eigenvalues and you know you know and you know everything and you have here a very very good approximation of the original. Weight matrix beautiful now path path is now integrated in hugging face so you have to path library with all the other models with all the data set with everything in hugging face and now in hugging face you have also Laura in the path library so this means you do not have to go here. You have everything here now in hugging face and as I told you parameter efficient fine tuning adapter tuning to be more specific here since this is the general case with adapter including prefix including Laura. You can use it for LLMs you can use it for diffusion model stable diffusion they use it dream booth everything that is based on a transformer you can use it on vision transformer so this is not a two institution the new hugging face path library with Laura. Beautiful now if you have the configuration files of Laura config where I started today you have now in the adapter app you have here the class transformers.laura configuration and here you have this particular parameters and you have here the explanation and you have here again the configuration. You have now in the adapter app you have here the class transformers.laura configuration and here you have this particular parameters and you have here the explanation and you have here again as our the rank this is the rank of the lower. Layer I told you it defaults normally to eight maybe two yeah default to eight you can change this you can play around to find the optimal parameter you can use it only on the self attention layer or you can go it even for the MOP. Whatever you want here you can do it a drop out you have other hyper parameter for scaling the reparameterization and more or less here in path here and I have you here on the GitHub. Check it out it's a data class class Laura configuration from the path configuration and this is here all the different parameters have a look play around modify it find your optimal configuration and look a little bit around for the famous models like GPT-3 and so they are already there let's call it optimal Laura configuration available. What is the next step next step now that we know lower is of course how can we further reduce the size how can we speed up the calculation now normally we do all our calculation mathematically with a 32 bit encoding of our values. Then you remember float and brain float 16 we use now half precision for our neural network architecture and then because it is still that some people have do not have access to 320 gigabytes of rear almost 640. GPU cluster of it hopper 100 so they say okay what can we do to further reduce it and now the idea was here to have an integer eight quantization so the beautiful is now now you understand the method here of matrix decomposition in lower rank truncated SVDs and this is now compatible with the You have to be a little bit careful we'll show you in the next videos when we code this but of course by reducing now also the precision of the weights and activations now from 22 or 16 bit to 8 bit integers you can further reduce on the one hand the memory footprint but you know there is no free lunch. And increase the efficiency of operations yeah yeah yeah but you can also affect the accuracy of the model as reducing the precision of the weights and activation can lead to loss of information. So yes you can use 8 bit but at a certain value there's a certain threshold where you have to decide for your particular case if bits and bytes or 8 bit or anything else 8 bit integers for example is this the right way to do it. If you train for hours and hours and hundreds and 200 hours and then you just have an 8 bit accuracy of your values. You have to find an answer for yourself there are a lot of programs out there where you say yes we integrate everything as the integer for quantization. Wow so this is really something think about now you understand the mechanism behind Laura again summary one sheet this is to remember path the new library and hugging face implementing Laura Laura is currently implemented. As a parameter efficient fine tuning style for LLMs for diffusion models dream booth anything based more or less on a transformer goes with vision transformer you can integrate it with whisper I will show you in a later video so beautiful. But remember Laura is just one method for path you can combine Laura with other methods that are within path for example you can have Laura plus prefix tuning so you have to path methodologies you can have a path methodologies you can have a path methodologies. You can add another idea some extra trainable adapters using path this is some beautiful elegant methodology and of course if we look in one of the next videos then you can see that this is a very simple method. So you can use here path also for this particular task and I will show you in one of my upcoming videos so I hope you have now a theory at your red dot. What's going on here and the code as I showed you a simple one or two or three lines of code. This is no problem at all the confidence of the code is not a problem at all. The configuration is really something defined tuning of your hyper parameters the configuration of Laura I would put my attention over there and next videos I will show you how to do that. So I hope you have now a theory a theoretical understanding of what's going on here and the code as I showed you a simple one or two or three lines of code. So I hope you have now a theory a theoretical understanding of what's going on here and the code as I showed you a simple one or three lines of code.",
        "output": "### Introduction to Parameter-Efficient Fine Tuning and LoRa\nParameter-efficient fine tuning is a way to adapt large models to specific tasks without updating all original weights. LoRa, or Low-Rank Adaptation, is a method used for this purpose by injecting trainable matrices into each layer of the transformer, using techniques from linear algebra.\n\n### Adapter Transformers and Adapter Hub\nAdapter Transformers are an extension of Hugging Face transformers that include small amounts of new parameters, while the original pre-trained weights are frozen. Adapter Hub is a framework where adapters specific to different models and tasks are shared, allowing for minimalistic training and reduced memory requirements.\n\n### Understanding LoRa\nLoRa's main idea is to freeze the pre-trained model weights and inject trainable matrices into the transformer layers. The low-rank adaptation of weight matrix is achieved using matrix decomposition techniques like SVD, which enables the approximation of high-rank matrices with lower-rank matrices.\n\n### Applications and Performance\nLoRa provides efficient fine-tuning for various tasks and model types, including language models, vision transformers, and diffusion models. The performance of LoRa-based methods is comparable to that of traditional fine-tuning based on benchmarks.\n\n### Hugging Face Path Library and LoRa Configurations\nHugging Face Path includes LoRa and other parameter-efficient fine-tuning techniques. To use LoRa effectively, identifying the optimal configuration for rank truncation and combining with other methods like prefix tuning or integer-8 quantization may be needed."
    },
    {
        "instruction": "<|SYSTEM|>You are a scholars that is taking markdown notes about some video, you would be giving some written scripts for the task.\nYou have finished writing the notes, and you want to extract the title from the notes.\nThe title should only be the name of the technology or algorithm, no extra words needed.\nPlease write the notes in markdown format. No need to write \"Title:\" for the Title. Make sure the title is within 10 words.\nHere is an example:\n## Hidden Markov Model\n<|ASSISTANT|>## ",
        "input": "<|USER|>### Introduction to Parameter-Efficient Fine Tuning and LoRa\nParameter-efficient fine tuning is a way to adapt large models to specific tasks without updating all original weights. LoRa, or Low-Rank Adaptation, is a method used for this purpose by injecting trainable matrices into each layer of the transformer, using techniques from linear algebra.\n\n### Adapter Transformers and Adapter Hub\nAdapter Transformers are an extension of Hugging Face transformers that include small amounts of new parameters, while the original pre-trained weights are frozen. Adapter Hub is a framework where adapters specific to different models and tasks are shared, allowing for minimalistic training and reduced memory requirements.\n\n### Understanding LoRa\nLoRa's main idea is to freeze the pre-trained model weights and inject trainable matrices into the transformer layers. The low-rank adaptation of weight matrix is achieved using matrix decomposition techniques like SVD, which enables the approximation of high-rank matrices with lower-rank matrices.\n\n### Applications and Performance\nLoRa provides efficient fine-tuning for various tasks and model types, including language models, vision transformers, and diffusion models. The performance of LoRa-based methods is comparable to that of traditional fine-tuning based on benchmarks.\n\n### Hugging Face Path Library and LoRa Configurations\nHugging Face Path includes LoRa and other parameter-efficient fine-tuning techniques. To use LoRa effectively, identifying the optimal configuration for rank truncation and combining with other methods like prefix tuning or integer-8 quantization may be needed.",
        "output": "## Parameter-Efficient Fine Tuning and LoRa"
    },
    {
        "instruction": "<|SYSTEM|>You are a scholars that is taking markdown notes about some video with Wiki style, you would be giving some written scripts for the task.\nAnd summarize each paragraph with a title.\nRemember to keep the technology or algorithm name, very important.\nPlease write the notes in markdown format.\nHere is an example:\n### Definiation\nA deadlock is a situation in which two computer programs sharing the same resource are effectively preventing each other from accessing the resource, resulting in both programs ceasing to function. \nThe earliest computer operating systems ran only one program at a time.\n\n### What it have to do with Relationsal Database ManagementSystem\nIn a database, a deadlock is an unwanted situation in which two or more transactions are waiting indefinitely for one another to give up locks. \nDeadlock is said to be one of the most feared complications in DBMS as it brings the whole system to a Halt. \nWhen you tries to insert or update data on database, sometime your record need to depend on the row record.  \n<|ASSISTANT|>#### ",
        "input": "<|USER|> If you want to turn words into numbers, and you want those numbers to make sense, then use word embeddings and similar words will have similar numbers. Hooray! StackQuest! Hello, I'm Josh Starmer and welcome to StackQuest! Today we're going to talk about word embedding and word to vec and they're going to be clearly explained. Lightning makes using the cloud just as easy to use as your laptop and that's cool. This StackQuest is also brought to you by the letters A, B, and C. A always, B B, C curious, always be curious. Also, I want to give a special thanks to Alex Lavary and the students at Boston University's Spark for letting me test this StackQuest out on them. Note, this StackQuest is focused on how neural networks can be used to create word embeddings and assumes that you're already familiar with the basic ideas of neural networks and backpropagation as well as the main ideas about the softmax function and cross entropy. If not, check out the quests. Words are great. We can use them to communicate all kinds of cool ideas. For example, hey Norm, isn't StackQuest awesome? It sure is, Squatch. Unfortunately, a lot of machine learning algorithms, including neural networks, don't work well with words. Hey, neural network, isn't StackQuest awesome? So, if we want to plug words into a neural network or some other machine learning algorithm, we need a way to turn the words into numbers. 24, 0.3, 5.1. Yes, StackQuest is awesome. Bam! One super easy way to convert words into numbers is to just assign each word to a random number. For example, if someone just saw the hit movie Troll 2 and said, Troll 2 is great, we can just give each word a random number. Note, for the purposes of this StackQuest, we're going to treat Troll 2 as a single word. Anyway, if the next person said, Troll 2 is awesome, then we could reuse the random numbers that we already selected for Troll 2 and is, and assign a new random number to the new word, awesome. In theory, this is fine. But it means that even though great and awesome mean similar things and are used in similar ways, they have very different numbers associated with them, 4.2 and negative 32.1. And that means the neural network will probably need a lot more complexity and training, because learning how to correctly process the word great won't help the neural network correctly use the word awesome. So it would be nice if similar words that are used in similar ways could be given similar numbers, so that learning how to use one word will help learn how to use the other at the same time. And because the same words can be used in different contexts or made plural or used in some other way, it might be nice to assign each word more than one number so that the neural network can more easily adjust to different contexts. For example, the word great can be used in a positive way like, StatQuest is great. And it can also be used in a sarcastic negative way like, my cell phone's broken, great. And it would be nice if we had one number that could keep track of the positive ways that great is used and a different number to keep track of the negative ways. Hey, Josh, deciding what words are similar and used in similar contexts sounds like a lot of work. And using more than one number per word to account for different contexts sounds like even more work. Don't worry, Squatch. The good news is that we can get a super simple neural network to do all of the work for us. Bam! To show how we can get a super simple neural network to figure out what numbers should go with different words, let's imagine we have two phrases. Troll2 is great and Jim Cotta is great. Anyway, both phrases have two words, Troll2 and Jim Cotta, in similar context, because someone, and I'm not going to name names, thinks that these two terrible movies are great. So in order to create a neural network to figure out what numbers we should associate with each word, the first thing we do is create inputs for each unique word. In this case, we have four unique words in the training data, so we have four inputs. Now we connect each input to at least one activation function. Note, this activation function uses the identity function, and thus, the input value is the same as the output value. In other words, this activation function doesn't do anything except give us a place to do addition. The number of activation functions corresponds to how many numbers we want to associate with each word, and the weights on those connections will, ultimately, be the numbers that we associate with each word. Now, in this example, we want to associate two numbers with each word, so that means we'll use two activation functions, and the weights on the connection to the second activation function will be another number associated with each word. However, like always, these weights start out with random values that we'll optimize with backpropagation. Now, in order to do backpropagation, we have to make predictions, so we'll use the input word to predict the next word in the phrase. So, if the phrase is troll2 is great, then we can use the word troll2 to predict the word is. In other words, if the input word is troll2, and we indicate that by putting a 1 into the troll2 input and zeros into all of the other inputs, then we want the output for the next word is to have the largest value. And if the input word is is, which means that the input for is is 1 and all of the other inputs are 0, then we want the output for the next word great to have the largest value. Lastly, if the input word is gymcata, which means that the input for gymcata is 1 and all of the other inputs are 0, then we want the output for the next word is to have the largest value. In order to make these predictions, we connect the activation functions to outputs, and we add weights to those connections with random initialization values. And then we run the outputs through the softmax function because we have multiple outputs for classification, and that means we can use the cross entropy loss function for backpropagation. Again, the goal is to train this neural network so that it correctly predicts the next word in a phrase. And now, before training, if we plug the word troll2 into the input and do the math, we get lucky and correctly predict the next word is but just barely. However, when we plug the word is into the input and do the math, then we fail to correctly predict the next word great and instead predict is. Wah wah. So we need to train this neural network. However, before we optimize all of the weights, remember that I said that these weights would be the numbers associated with each word. And since in this example we have two weights for each word, we can plot each word on a graph that has the weight values to the top activation function on the x-axis and the weight values to the bottom activation function on the y-axis. For example, the word troll2 goes here because its top weight is 0.38 and its bottom weight is 0.42. Likewise, the word is goes here and the word great goes here and gemcata goes here. Now, with this graph, we see that the words troll2 and gemcata are currently no more similar to each other as they are to any of the other words. However, because both words appear in the same context in the training data, we hope that backpropagation will make their weights more similar. So we start with these weights and we end up with these new weights. The new weights on the connections from the inputs to the activation functions are the word embeddings. And when we plot the words using the new weights, which are now the embeddings, we see that troll2 and gemcata are now relatively close to each other compared to the other words in the training data. Bam! Now that we've trained the neural network, we can see how well it predicts the next word. For example, if we plug the word troll2 into the input and do the math, then the output for the next word is is one and everything else is zero. And that is exactly what we wanted. And when we plug the word is into the input and do the math, then the output for the next word in both phrases, great, is one and everything else is zero. Lastly, when we plug the word gemcata into the input and do the math, then the output for the next word is is one and everything else is zero. Bam! Now, before we talk about a popular word embedding tool, Word2Vec, let's summarize what we've learned so far. First, rather than just assign random numbers to words, we can train a relatively simple neural network to assign numbers for us. The advantage of using a neural network is that it can use the context of words in the training data set to optimize weights that can be used for the embeddings. And this can result in similar words ending up with similar embeddings. Lastly, having similar words with similar embeddings means training a neural network to process language is easier because learning how one word is used helps learn how similar words are used. Double bam! Now, so far we've shown we can train a neural network to predict the next word in each phrase. But just predicting the next word doesn't give us a lot of context to understand each one. So, now let's learn about the two strategies that Word2Vec, a popular method for creating word embeddings, uses to include more context. The first method, called continuous bag of words, increases the context by using the surrounding words to predict what occurs in the middle. For example, the continuous bag of words method could use the words troll2 and great to predict the word that occurs between them, is. The second method, called skipgram, increases the context by using the word in the middle to predict the surrounding words. For example, the skipgram method could use the word is to predict the surrounding words, troll2, great, and gymcada. Lastly, before we're done, just know that in practice, instead of using just two activation functions to create two embeddings per word, people often use 100 or more activation functions to create a lot of embeddings per word. And instead of using two sentences for training, they use the entire Wikipedia. Thus, instead of just having a vocabulary of four words and phrases, Word2Vec might have a vocabulary of about three million words and phrases. Thus, the total number of weights in this neural network that we need to optimize is three million words and phrases times at least 100. The number of weights each word has going to the activation functions times two for the weights that get us from the activation functions to the outputs for a total of 600 million weights. So training can be slow. However, one way that Word2Vec speeds things up is to use something called negative sampling. Negative sampling works by randomly selecting a subset of words we don't want to predict for optimization. For example, say like we wanted the word aardvark to predict the word a. That means that only the word aardvark has a one in it and all of the other words have zeros. And that means we can ignore the weights coming from every word but aardvark because the other words multiply their weights by zero. That alone removes close to 300 million weights from this optimization step. However, we still have 300 million weights after the activation functions. So because we want to predict the word a, then we don't want to predict aardvark, abandon, and all of the other words. So for this example, let's imagine Word2Vec randomly selects abandon as a word we don't want to predict. Note, in practice, Word2Vec would select between two and 20 words that we don't want to predict. However, in this example, we just select one, abandon. So now Word2Vec only uses the output values for a and abandon. And that means for this round of backpropagation, we can ignore the weights that lead to all of the other possible outputs. So in the end, out of the 600 million total weights in this neural network, we only optimize 300 per step. And this is one way that Word2Vec can efficiently create lots of word embeddings for each word in a large vocabulary. Triple bam! Now it's time for some shameless self-promotion. If you want to review statistics and machine learning offline, check out the StackQuest PDF study guides and my book, The StackQuest Illustrated Guide to Machine Learning at stackquest.org. There's something for everyone. Hooray! We've made it to the end of another exciting StackQuest. If you like this StackQuest and want to see more, please subscribe. And if you want to support StackQuest, consider contributing to my Patreon campaign, becoming a channel member, buying one or two of my original songs or a t-shirt or a hoodie, or just donate. The links are in the description below. All right, until next time, Quest on!",
        "output": "### Introduction to Word Embeddings and Word2Vec\nWord embeddings are a method for converting words into numbers that make sense, allowing algorithms like neural networks to work better with words. Similar words will have similar numbers which makes predicting words easier.\n\n### Simple Neural Network for Word Embeddings\nA simple neural network can be used to assign numbers to words, taking context into account. The weights on the connections become the word embeddings. This makes training a neural network easier like identifying sarcasm, as it can recognize and differentiate similar words used in different contexts.\n\n### Two Strategies used by Word2Vec for Context\nWord2Vecis a popular method for creating word embeddings which uses two strategies: Continuous Bag of Words (CBOW) and Skip-Gram. In CBOW, the surrounding words are used to predict the word in the middle. In Skip-Gram, the word in the middle is used to predict the surrounding words.\n\n### Training Word2Vec on Large Text Corpus\nIn practice, instead of just having a small number of words and phrases like in the example, Word2Vec's vocabulary often consists of about three million words and phrases from Wikipedia. This increases the number of weights to optimize which makes training relatively slow. However, Word2Vec uses negative sampling to speed up the process and ignore some weights when optimizing.\n\n### Summary\nTo summarize, word embeddings provide a useful way to represent words in numerical format which is great for machine learning algorithms. Word2Vec, a popular word embedding tool, helps to create accurate word embeddings by using two strategies: Continuous Bag of Words and Skip-Gram. Training Word2Vec on large text corpus can be slow but is made more efficient through negative sampling.\n"
    },
    {
        "instruction": "<|SYSTEM|>You are a scholars that is taking markdown notes about some video, you would be giving some written scripts for the task.\nYou have finished writing the notes, and you want to extract the title from the notes.\nThe title should only be the name of the technology or algorithm, no extra words needed.\nPlease write the notes in markdown format. No need to write \"Title:\" for the Title. Make sure the title is within 10 words.\nHere is an example:\n## Hidden Markov Model\n<|ASSISTANT|>## ",
        "input": "<|USER|>### Introduction to Word Embeddings and Word2Vec\nWord embeddings are a method for converting words into numbers that make sense, allowing algorithms like neural networks to work better with words. Similar words will have similar numbers which makes predicting words easier.\n\n### Simple Neural Network for Word Embeddings\nA simple neural network can be used to assign numbers to words, taking context into account. The weights on the connections become the word embeddings. This makes training a neural network easier like identifying sarcasm, as it can recognize and differentiate similar words used in different contexts.\n\n### Two Strategies used by Word2Vec for Context\nWord2Vecis a popular method for creating word embeddings which uses two strategies: Continuous Bag of Words (CBOW) and Skip-Gram. In CBOW, the surrounding words are used to predict the word in the middle. In Skip-Gram, the word in the middle is used to predict the surrounding words.\n\n### Training Word2Vec on Large Text Corpus\nIn practice, instead of just having a small number of words and phrases like in the example, Word2Vec's vocabulary often consists of about three million words and phrases from Wikipedia. This increases the number of weights to optimize which makes training relatively slow. However, Word2Vec uses negative sampling to speed up the process and ignore some weights when optimizing.\n\n### Summary\nTo summarize, word embeddings provide a useful way to represent words in numerical format which is great for machine learning algorithms. Word2Vec, a popular word embedding tool, helps to create accurate word embeddings by using two strategies: Continuous Bag of Words and Skip-Gram. Training Word2Vec on large text corpus can be slow but is made more efficient through negative sampling.\n",
        "output": "## Word Embeddings and Word2Vec"
    },
    {
        "instruction": "<|SYSTEM|>You are a scholars that is taking markdown notes about some video with Wiki style, you would be giving some written scripts for the task.\nAnd summarize each paragraph with a title.\nRemember to keep the technology or algorithm name, very important.\nPlease write the notes in markdown format.\nHere is an example:\n### Definiation\nA deadlock is a situation in which two computer programs sharing the same resource are effectively preventing each other from accessing the resource, resulting in both programs ceasing to function. \nThe earliest computer operating systems ran only one program at a time.\n\n### What it have to do with Relationsal Database ManagementSystem\nIn a database, a deadlock is an unwanted situation in which two or more transactions are waiting indefinitely for one another to give up locks. \nDeadlock is said to be one of the most feared complications in DBMS as it brings the whole system to a Halt. \nWhen you tries to insert or update data on database, sometime your record need to depend on the row record.  \n<|ASSISTANT|>#### ",
        "input": "<|USER|> Hello people from the future, welcome to Normalize Nerd. Today we will set up our camp in the random forest. First we will see why the random forest algorithm is better than our good old decision trees. And then I will explain how it works with visualization. If you want to see more videos like this, please subscribe to my channel and hit the bell icon because I make videos about machine learning and data science regularly. So without any further ado, let's get started. To begin our journey, we need a data set. Here I am taking a small data set with only six instances and five features. As you can see, the target variable y takes two values, 0 and 1. Hence, it's a binary classification problem. First of all, we need to understand why do we even need the random forest when we already have the decision trees? Let's draw the decision tree for this data set. Now, if you don't know what a decision tree really is or how it is trained, then I would highly recommend you to watch my previous video. In short, a decision tree splits the data set recursively using the decision nodes unless we are left with pure leaf nodes. And it finds the best split by maximizing the entropy gain. If a data sample satisfies the criteria for the data set, then the data set is not a tree. Again, if a data sample satisfies the condition at a decision node, then it moves to the left child, else it moves to the right. And finally, it reaches a leaf node where a class label is assigned to it. So what's the problem with decision trees? Let's change our training data slightly. Focus on the row with ID 1. We are changing the x0 and x1 features. Now, if we train our tree on this modified data set, we will get a completely different tree. This shows us that decision trees are highly sensitive to the training data, which could result in high variance. So our model might fail to generalize. Here comes the random forest algorithm. It is a collection of multiple random decision trees and it's much less sensitive to the training data. You can guess that we use multiple trees, hence the name forest. But why it's called random? Keep this question in the back of your mind. You will get the answer by the end of this video. Let me show you the process of creating a random forest. The first step is to build new data sets from our original data. To maintain simplicity, we will build only 4. We are gonna randomly select rows from the original data to build our new data sets. And every data set will contain the same number of rows as the original one. Here's the first data set. Due to lack of space, I am writing only the row IDs. Notice that row 2 and 5 came more than once. That's because we are performing random sampling with replacement. That means, after selecting a row, we are putting it back into the data. And here are the rest of the data sets. The process we just followed to create new data is called bootstrapping. Now we will train a decision tree on each of the bootstrap data sets independently. But here's a twist. We won't use every feature for training the trees. We will randomly select a subset of features for each tree and use only them for training. For example, in the first case, we will only use the features x0 and x1. Similarly, here are the subsets used for the remaining trees. Now that we have got the data and the feature subsets, let's build the trees. Music Just see how different the trees look from each other. And this my friend is the random forest containing four trees. But how to make a prediction using this forest? Let's take a new data point. We will pass this data point through each tree one by one and note down the predictions. Music Now we have to combine all the predictions. As it's a classification problem, we will take the majority voting. Clearly, one is the winner. Hence, the prediction from our random forest is one. This process of combining results from multiple models is called aggregation. So in the random forest, we first perform bootstrapping, then aggregation. And in the jargon, it's called bagging. Okay, so that was how we build a random forest. Now I should discuss some of the very important points related to this algorithm. First, why it's called random? Because we have used two random processes, bootstrapping and random feature selection. But what is the motivation behind bootstrapping and feature selection? Well, bootstrapping ensures that we are not using the same data for every tree. So in a way, it helps our model to be less sensitive to the original training data. The random feature selection helps to reduce the correlation between the trees. If you use every feature, then most of your trees will have the same decision nodes. And they will act very similarly. That will increase the variance. There's another benefit of the random feature selection. Some of the trees will be trained on less important features. So, they will give bad predictions. But there will also be some trees that give bad predictions in the opposite direction. So, they will balance out. Next point, what's the ideal size of the feature subset? Well, in our case, we took two features, which is close to the square root of the total number of features. That is 5. Researchers have found that values close to the log or square root of the total number of features work well. And how do you use it for a regression problem? Pretty easy. While combining the predictions, just take the average and you are all set to use it for a regression problem. So that was all about it. I hope now you have a pretty good understanding of random forest. If you enjoyed this video, please share this video and subscribe to my channel. Stay safe and thanks for watching.",
        "output": "### Random Forest Algorithm Introduction\nRandom Forest Algorithm is a collection of multiple random decision trees, which is less sensitive to training data and is more effective in generalization. In this discussion, a simple binary classification problem dataset is used to compare Random Forest with Decision Trees.\n\n### Issue with Decision Trees\nDecision trees are highly sensitive to the training data, which can result in high variance and failure to generalize. To overcome this, the Random Forest algorithm is introduced.\n\n### Bootstrapping in Random Forest\nThe first step in creating a Random Forest is building new datasets from the original data by randomly selecting rows with replacement. This process is called bootstrapping.\n\n### Feature Selection in Random Forest\nWhen training a decision tree on each bootstrap dataset, a subset of features is randomly selected for training. This reduces the correlation between trees and helps the model be less sensitive to the original training data.\n\n### Building Random Forest Trees\nRandom Forest builds trees using bootstrapping and random feature selection processes which leads to different trees in the forest. The trees are then combined to make predictions using majority voting for classification problems and averaging for regression problems.\n\n### Key Points about Random Forest Algorithm\n- It is called \"random\" due to the bootstrapping and random feature selection processes.\n- Bootstrapping helps the model be less sensitive to the original training data.\n- Random feature selection reduces correlation between trees and helps to lower variance.\n- Ideal feature subset size is usually close to the square root of the total number of features.\n- For regression problems, predictions are combined using averaging instead of majority voting."
    },
    {
        "instruction": "<|SYSTEM|>You are a scholars that is taking markdown notes about some video, you would be giving some written scripts for the task.\nYou have finished writing the notes, and you want to extract the title from the notes.\nThe title should only be the name of the technology or algorithm, no extra words needed.\nPlease write the notes in markdown format. No need to write \"Title:\" for the Title. Make sure the title is within 10 words.\nHere is an example:\n## Hidden Markov Model\n<|ASSISTANT|>## ",
        "input": "<|USER|>### Random Forest Algorithm Introduction\nRandom Forest Algorithm is a collection of multiple random decision trees, which is less sensitive to training data and is more effective in generalization. In this discussion, a simple binary classification problem dataset is used to compare Random Forest with Decision Trees.\n\n### Issue with Decision Trees\nDecision trees are highly sensitive to the training data, which can result in high variance and failure to generalize. To overcome this, the Random Forest algorithm is introduced.\n\n### Bootstrapping in Random Forest\nThe first step in creating a Random Forest is building new datasets from the original data by randomly selecting rows with replacement. This process is called bootstrapping.\n\n### Feature Selection in Random Forest\nWhen training a decision tree on each bootstrap dataset, a subset of features is randomly selected for training. This reduces the correlation between trees and helps the model be less sensitive to the original training data.\n\n### Building Random Forest Trees\nRandom Forest builds trees using bootstrapping and random feature selection processes which leads to different trees in the forest. The trees are then combined to make predictions using majority voting for classification problems and averaging for regression problems.\n\n### Key Points about Random Forest Algorithm\n- It is called \"random\" due to the bootstrapping and random feature selection processes.\n- Bootstrapping helps the model be less sensitive to the original training data.\n- Random feature selection reduces correlation between trees and helps to lower variance.\n- Ideal feature subset size is usually close to the square root of the total number of features.\n- For regression problems, predictions are combined using averaging instead of majority voting.",
        "output": "## Random Forest Algorithm"
    },
    {
        "instruction": "<|SYSTEM|>You are a scholars that is taking markdown notes about some video with Wiki style, you would be giving some written scripts for the task.\nAnd summarize each paragraph with a title.\nRemember to keep the technology or algorithm name, very important.\nPlease write the notes in markdown format.\nHere is an example:\n### Definiation\nA deadlock is a situation in which two computer programs sharing the same resource are effectively preventing each other from accessing the resource, resulting in both programs ceasing to function. \nThe earliest computer operating systems ran only one program at a time.\n\n### What it have to do with Relationsal Database ManagementSystem\nIn a database, a deadlock is an unwanted situation in which two or more transactions are waiting indefinitely for one another to give up locks. \nDeadlock is said to be one of the most feared complications in DBMS as it brings the whole system to a Halt. \nWhen you tries to insert or update data on database, sometime your record need to depend on the row record.  \n<|ASSISTANT|>#### ",
        "input": "<|USER|> Hello everyone and welcome back to the channel. My name is Jacob Hegland and this is the second video in a series about graph neural networks. If you missed the first video in this series and you want to learn more about why you might want to use graphs to model data for machine learning tasks, then feel free to check out that video. It's linked in the description below. However, if you've already seen that video or you just want to learn things in a more non-linear fashion, we're going to be diving into some of the mathematical details of graph neural networks and we're starting right now. Now before we get into the main topic of this video, I'm going to ask you to think of two or three colleagues, co-workers, or friends who might also be interested in the topic of this video. If you watch this video until the very end, and I'm not saying you will, but if you do, then please consider recommending this video to them, directly to them. Word-of-mouth communication is so important when you're talking about cutting-edge technologies like this and I really appreciate it. And if you appreciate videos like this, then please consider hitting that like button for the YouTube algorithm. The YouTube algorithm is kind of like word-of-mouth communication, but from my perspective it's a lot more mysterious in how it actually works. And with that out of the way, let's get into today's video. And now we're going to be talking about how graph neural networks actually work and we're going to be building it up from the basics of message passing. Now the equation you see on your screen right now is a popular graph neural network known as a graph convolutional network or a GCN. By the very end of the section we'll have a basic understanding of how this equation produces node embeddings for the purpose of machine learning tasks. Now the historical development of graph neural networks is actually very interesting because they were actually developed sort of independently in the world of chemistry and then later in the world of computer science. So in the mid-1990s there were people working on computational chemistry problems and they developed algorithms that were... they look a lot like graph neural networks and it's very interesting. But later on similar types of algorithms were developed in the mid-2000s sort of independently and these algorithms were based on two-step message passing. So the basic idea of two-step message passing is that you have a target node and its neighbors and you're going to look at this node's neighbors first right. You're going to look at b, c, and d and you can see this over here on the right. And you're going to take all the information from these nodes, whatever information that you consider relevant to your problem, and you're going to aggregate it in some way. Next you are going to update the representation of node a based on the message that you calculate inside of this aggregation function. So message passing algorithms can be very powerful and they can be written in a very generic form and we'll keep this image up here as a reminder of what basic message passing looks like. So we're going to write this out as an equation and very generically as well. And what we have here is h is going to be our node features or embeddings from a previous layer. K is going to be the number of neighbors or the number of hops that we'll look at actually. And we're going to talk about that detail in one second. So what we're doing here is we're taking all of these nodes in our neighbor, our first neighborhood say. This is going to be b, c, and d and we're going to somehow update and combine it with the target node representation. And then at the end we're going to update our target node representation for the next layer. So just a imagine this to be node a node a and nodes b, c, and d for this image up here. Now what's interesting is that this aggregate function operates over a set of nodes. So you could imagine that there's another node as its neighbor say node z. And this function would still have to work. It would still have to spit out a number or some embedding that we can use. So it has to have this special property that the aggregate function is going to be permutation invariant or permutation equivariant. I'm not going to go too much into detail there, but it's just something to keep in mind for later on. Now in neural message passing we're going to be using a neural network for the update and aggregate function. So if you remember this image from the last slide we're just going to put this down here and if we kind of line some things up this kind of looks like an update right. This stuff in here kind of looks like an aggregate. Very interesting. So our specific choice of aggregate function is going to be a sum over all of the neighboring nodes. We're then going to multiply that by a weighting matrix. And then we're also going to take a weighting of our target node representation. And then because it's a neural network we have to add in some non-linearity so we'll add in some activation function and this will then update our target node representation. And really this is a very intuitive idea because each node's updated value will become some type of weighting between its previous value and a weighting of its neighbor's value. And that's a very powerful idea and many neural network architectures have been very successful with some type of idea similar to this. I will have you keep in mind that this choice of sum was a choice. It was a very deliberate choice but it's a choice nonetheless. And there are some other functions that we could use instead of a sum. So you could take a mean over your neighbors, you could take the maximum, or you could concatenate all them together and then multiply by some type of weighting matrix. So there are different choices that you can make here. Now the neural message passing algorithm that we showed on the last slide can actually be simplified a little bit and this is important because it's kind of weird for us to treat the target node and the neighboring nodes asymmetrically. Like we have an entire weight matrix that's just on one node and then another separate weight matrix that's being used for the neighbors. Now if we do a little bit of a trick what we're going to do is we're going to collapse these two weight matrices into a single weight matrix and we're going to do that by adding self loops to the adjacency matrix. Now I didn't talk about this before but an adjacency matrix defines the topology of the graph. It's based on the topology of the graph basically and what this does is it will make the target node its first neighbor as well. You're basically adding in a loop from a node to itself. So what this does is it reduces message passing to a relatively simple matrix multiplication. Now the historical development of graph neural networks is extremely interesting because there are several different communities that sort of converged on the same idea. So one way to think about it is message passing. Another way to think about it is a convolutional perspective where you're generalizing a convolution on an image to an operation that works on a graph or graph structured data. Now if we look at the original GNN algorithm I believe this was based on a message passing idea but this looks almost exactly the same. These equations look very very similar right and I believe that when Kip and Welling did this work in 2016 for the graph convolutional network they were coming at it from a convolutional perspective and really the only small difference is in how you define the adjacency matrix where you're simply normalizing by the number of nodes in the neighborhood for each target node. Now the development and application of graph neural network models is a new and active area of research. If you look at any of the major conferences in machine learning or even some engineering journals now you will see that people are taking graph neural network models and applying them to new and novel problems and they are doing better than previous models have ever done and I think that is an absolutely fascinating thing to be a part of. So if you're in research or you're considering doing a PhD or a masters or whatever that involves research I would highly suggest that you can look into graph neural networks and see what new things you can do with them. So as a sort of historical development in 2018 the graph attention networks model was proposed and this is one of the first models to use attention as a graph convolutional operation. So you don't have to worry about the mathematical details here but basically they are learning end to end the weighting between every node in a graph or every node and its neighboring nodes. So in that way it is fundamentally different from a graph convolutional network where you're simply sort of stacking layers and you're multiplying by a fixed or a weighting matrix that changes based on the layers but you have like this fixed weighting. Then you also have sort of simplifying directions of convolutional operations. So this paper is very very interesting and you should all take a look at it. It came out ICML 2019 and basically they proposed the idea that you don't need all of these layers in the graph neural network. All you need to do to sort of pass information between nodes is to multiply some adjacency matrix by itself k times. So if you want to look at the second neighbors of every target node you simply pre-compute this and then you can do this very easy matrix multiplication. Very computationally simple and I think that is a very interesting method. So if you want to learn more about graph neural networks in general there are some important names to know. So Jure Leskovec is a professor at Stanford and I believe he is a chief scientist at Pinterest right now and then William Hamilton he literally wrote the book on GNNs. A lot of the material from this presentation is based off of some chapters in that book and there are also some key readings. So obviously the textbook by Hamilton if you look at chapters five through seven this will basically get you caught up on cutting edge stuff and you can actually go and do research based on that information. There are some good surveys out there and of course when you're learning anything you want to learn from primary sources as much as possible. So you want to watch talks on youtube from the people who created these algorithms whenever possible because often they'll give some cool anecdotes and some stories that just you don't learn that stuff from a paper right. So in summary graph neural networks are a very exciting field of opportunity right now and they are already making impact in several important ways. Hopefully I've answered the question of why you might want to use graphs in the first place, why you might want to use graph neural networks to learn on your graph structured data, and the very basics of how graph neural networks work. But wait there's more. So if you're like me you're probably already thinking about different systems that you could model with graphs and graph neural networks and all these different things. And graph neural networks alone are good at modeling spatial interactions between entities but not so much at temporal interactions between entities. So basic graph neural network models as I showed in this talk are good at modeling static graphs where your features and your graph topology don't change over time. But they need a little bit of extra work to work with systems that change over time right. So in a future video we'll be talking a little bit more about how you might actually accomplish that and I'll just leave it at that. If you enjoyed this video and want to see more like it in the future then please hit that like button and consider subscribing to the channel. If you want to start a discussion about the things that we talked about here today then please feel free to do so down below in the comment section. If you made it this far in the video then it obviously brought you some value and it'll probably bring your friends, co-workers, and colleagues some value as well. So please at least consider taking an extra minute out of your day and sharing it with people that you think would get some value out of it as well. I really appreciate it. Thank you so much for watching and we'll see you in the next video. Bye.",
        "output": "### Introduction\nVideo about the mathematical details of Graph Neural Networks (GNNs) and their applications.\n\n### Basics of Message Passing\nGraph Neural Networks operate using the concept of message passing, where information is aggregated from a node's neighbors and used to update the node's representation.\n\n#### Neural Message Passing Algorithm\nThe Neural Message Passing algorithm uses a neural network for both update and aggregate functions, with an aggregate function such as sum, mean, maximum, or concatenation over the node neighbors.\n\n### Simplification of Neural Message Passing\nBy adding self loops to the adjacency matrix, the Neural Message Passing algorithm is simplified to matrix multiplication.\n\n### Convolutional Perspective\nAnother perspective on GNNs is generalizing convolutions on images to operations on graph structures.\n\n### Recent Advancements in Graph Neural Networks\n- Graph Attention Networks (2018): Use of attention as graph convolutional operation\n- Simplifying Directions of Convolutional Operations: Reducing the need for multiple layers in GNNs, utilizing matrix multiplication\n\n### Important Names and Resources\n- Jure Leskovec: Professor at Stanford and Chief Scientist at Pinterest\n- William Hamilton: Author of a GNN textbook\n- Hamilton's Textbook: Chapters 5-7 for a comprehensive overview of GNNs\n- YouTube talks from GNN creators for primary source information\n\n### Summary\nGraph Neural Networks are a promising field with potential applications in many areas. The basics of GNNs involve aggregating information from neighboring nodes to update a target node's representation through message passing. Additionally, recent advancements in this area, such as Graph Attention Networks and simplifying the convolutional operation, provide alternative methods for processing graph data. To learn more, it is essential to study primary sources, such as textbooks, research papers, and talks from creators in the field."
    },
    {
        "instruction": "<|SYSTEM|>You are a scholars that is taking markdown notes about some video, you would be giving some written scripts for the task.\nYou have finished writing the notes, and you want to extract the title from the notes.\nThe title should only be the name of the technology or algorithm, no extra words needed.\nPlease write the notes in markdown format. No need to write \"Title:\" for the Title. Make sure the title is within 10 words.\nHere is an example:\n## Hidden Markov Model\n<|ASSISTANT|>## ",
        "input": "<|USER|>### Introduction\nVideo about the mathematical details of Graph Neural Networks (GNNs) and their applications.\n\n### Basics of Message Passing\nGraph Neural Networks operate using the concept of message passing, where information is aggregated from a node's neighbors and used to update the node's representation.\n\n#### Neural Message Passing Algorithm\nThe Neural Message Passing algorithm uses a neural network for both update and aggregate functions, with an aggregate function such as sum, mean, maximum, or concatenation over the node neighbors.\n\n### Simplification of Neural Message Passing\nBy adding self loops to the adjacency matrix, the Neural Message Passing algorithm is simplified to matrix multiplication.\n\n### Convolutional Perspective\nAnother perspective on GNNs is generalizing convolutions on images to operations on graph structures.\n\n### Recent Advancements in Graph Neural Networks\n- Graph Attention Networks (2018): Use of attention as graph convolutional operation\n- Simplifying Directions of Convolutional Operations: Reducing the need for multiple layers in GNNs, utilizing matrix multiplication\n\n### Important Names and Resources\n- Jure Leskovec: Professor at Stanford and Chief Scientist at Pinterest\n- William Hamilton: Author of a GNN textbook\n- Hamilton's Textbook: Chapters 5-7 for a comprehensive overview of GNNs\n- YouTube talks from GNN creators for primary source information\n\n### Summary\nGraph Neural Networks are a promising field with potential applications in many areas. The basics of GNNs involve aggregating information from neighboring nodes to update a target node's representation through message passing. Additionally, recent advancements in this area, such as Graph Attention Networks and simplifying the convolutional operation, provide alternative methods for processing graph data. To learn more, it is essential to study primary sources, such as textbooks, research papers, and talks from creators in the field.",
        "output": "## Graph Neural Networks"
    },
    {
        "instruction": "<|SYSTEM|>You are a scholars that is taking markdown notes about some video with Wiki style, you would be giving some written scripts for the task.\nAnd summarize each paragraph with a title.\nRemember to keep the technology or algorithm name, very important.\nPlease write the notes in markdown format.\nHere is an example:\n### Definiation\nA deadlock is a situation in which two computer programs sharing the same resource are effectively preventing each other from accessing the resource, resulting in both programs ceasing to function. \nThe earliest computer operating systems ran only one program at a time.\n\n### What it have to do with Relationsal Database ManagementSystem\nIn a database, a deadlock is an unwanted situation in which two or more transactions are waiting indefinitely for one another to give up locks. \nDeadlock is said to be one of the most feared complications in DBMS as it brings the whole system to a Halt. \nWhen you tries to insert or update data on database, sometime your record need to depend on the row record.  \n<|ASSISTANT|>#### ",
        "input": "<|USER|> So what's the problem with training large language models and fine tuning them? The key thing here is that we end up with really big weights. This raises a whole bunch of problems here. These problems are two main things. One, you need a lot more compute to train for this. And as the models are getting larger and larger, you're finding that you need much bigger GPUs, multiple GPUs, just to be able to fine tune some of these models. The second problem is that in addition to basically needing the compute, the file sizes become huge. So the T5 XXL checkpoint is around about 40 gigabytes in size. Not to mention the sort of 20 billion parameter models that we've got coming out now are getting bigger and bigger all the time. So this is where this idea of parameter efficient fine tuning comes in. So I'm just going to talk about this as PEFT going forward. So PEFT uses a variety of different techniques. The one we're going to be looking at today is LAURA, which stands for low rank adaption, and it comes from a paper all about doing this for large language models. But PEFT also has some other cool techniques like prefix tuning, P tuning, and prompt tuning that we'll look at in the future and when to use those and how they can be really useful. And there are some of the techniques that are actually being used by companies like Nvidia to allow people to fine tune these models in the cloud. So that's something really interesting to look at. So what PEFT does and with LAURA in particular is that it's just allowing you to fine tune only a small number of extra weights in the model while you freeze most of the parameters of the pre-trained network. So the idea here is that we're not actually training the original weights. We're adding some extra weights and we're going to fine tune those. One of the advantages of this is that we've still got the original weights. So this also tends to help with stopping catastrophic forgetting. If you don't know, catastrophic forgetting is where models tend to forget what they were originally trained on when you do a fine tuning, if you do the fine tuning too much, you end up then causing it to forget some of the things from the original data that it was trained on. But PEFT doesn't have that problem because it's just adding extra weights and it's tuning those as it freezes the original ones. So PEFT also allows you to get really good fine tuning when you've only got a small amount of data and also it allows this to generalize better to other scenarios as well. So in all this sort of thing is a huge win for fine tuning large language models and even models like stable diffusion, a lot of the AI models that we're seeing currently are starting to use this as well. One of the best things is that you end up at the end with just tiny checkpoints. In one of my recent videos, I showed fine tuning the Llama model to create the Alpaca model. And I think the final checkpoint for just the add-on part was something around 12 megabytes. So it's tiny. Now you still need the original weights. So it's not like you're getting away totally from everything, but you've got something that's much smaller. So in general, the PEFT approaches allow you to basically get similar performance to fine tuning a full model just by fine tuning and tuning these add-on weights that you're going to put into it. Hugging Face has released a whole sort of library around this. And this is what, where this PEFT comes in. If they've taken a number of papers and implemented them to work with the transformers library and the accelerate library. So this allows us to basically take off the shelf Hugging Face pre-trained models that have been done by Google, done by Meta, done by a variety of different companies and put them into something where we can use them with this and fine tune them really well. So we're going to jump into the code and we're going to look at how to basically use PEFT to do a LoRa fine tuning of a model. All right. In this notebook, we're going to go through and look at training up a model fine tuning a model using PEFT, bits and bytes and doing a LoRa checkpoint for this. So this is a LoRa fine tuning. So if you remember the idea with LoRa is that we're training sort of adapters that go on. We're not training the actual weights. We're adding weights to the model at various points in the model. And we're fine tuning those to get the results out that we want. So you just come up at, you install your libraries here. I always like to set up the Hugging Face Hub early because if you're going to leave this running and it gets to the end of the training, you want to basically save your model, your weights up to Hugging Face Hub as quickly as possible so that your Colab doesn't stop. And then you lose all your work in there. I tend to put this up the front. This is basically just get your Hugging Face token. You'll need a right token obviously to do this. So this Colab I've run on an A100, but you can certainly, you should be able to do it with a T4 if you change the model to be a smaller version of the Bloom model. So the model that I'm training here or fine tuning here is the Bloom 7 billion parameter model. And there's also like a 760 version. I think there's also a 1.3 billion version, et cetera, that you could try out. So we're loading in the model. So you'll see, we've just got an order. We've just from transformers, we're bringing in bits and bytes, which is going to handle the 8-bit turning our model into 8-bit, which means that it won't take up so much GPU RAM, makes it easier, makes it quicker, makes it easier to store things later on too. And we've got our auto tokenizer and we've got this auto model for causal language modeling. So when we just bring in from pre-trained, we can pass in the name for the Bloom 7 billion. And all we have to do here is pass in load in 8-bit equals true. And transformers will take care of the 8-bit conversion using the bits and bytes library for doing this. If you're using a GPU at home where you've perhaps got a 30 90 or something like that, and you want to try it on there, if you've got multiple GPUs, you can do a device map to basically map parts of the model across, but in this case, we're just using auto and I suggest you try out auto at the start anyway. So we've got our model in, we've got our tokenizer in here. The next thing we want to do is basically go through and freeze the original weights. So you can see here that we're basically just going through and freezing these weights with a few exceptions, the layer norm layers, we want to keep them. And we want to actually keep them in float 32 and also the outputs we want to keep as being float 32. So what this is just doing, this is some standard code for you for doing that. Next up is setting up the actual LoRa adapters. So this all comes down to the config here. So we're going to basically get the config. So we've remember up here, we've got our model here and this is the full size model, but there's no LoRa added to that yet. In here, we're going to make this config and then we're going to basically pass in the model that we had and then get the PEPF model, which is going to have the original model and the LoRa adapters on this, so the config here is key. You're basically setting the number of attention head that you want, the alpha scaling, if you know that your model's got certain target modules. I don't find a lot of documentation about this in the library at the moment, but my guess is that going forward, people will work out, are these are the best modules in large module to basically have LoRa adapters on there. Setting your dropout for LoRa and another key one is just setting the task type. So is it a causal language model, meaning that it's a decoder only model, a GPT style model, or is it going to be a seek to seek model more like the T5 models, the FLAN models, et cetera. And I'll perhaps make another video of going through training, fine tuning a seek to seek model. So you can see the differences in here. So by playing around with these two settings up here, this will determine the size of the trainable amount quite a lot. So you can try out some different ideas here, but you'll see that, okay, we've got this 7 billion parameter for all the parameters, but the trainable parameters is just tiny, really tiny in here. So this gives us the total trainable parameters that we can see what's going on there. All right. In this case for data. So I've just picked a really simple little task in here. There's this data set of English quotes rather than what most people seem to do is use that to finish a quote so that if someone starts a quote and it can finish it, looking at the data set, I saw that there are actually a bunch of tags about the quotes and what I thought would be cool is let's try and make a model where you can input your own quote and it will then generate tags for that quote. So you can see here, what I've done is basically just merge some of the columns to make it so we've got this quote and then we've got these three characters here now, those three characters are chosen because they're probably not going to appear in that order very often in the pre-training and stuff like that, so we're trying to teach the model that anytime you see these three characters, we're going to condition on the input before that. And we're going to generate the tags out after that. So you can see here by looking at the data set that we've made, we've got this be yourself, everyone else has taken and the tags, so we've got that there. And then the tags are going to be this be yourself, honesty, inspirational, misattributed to Oscar Wilde, these kinds of things. Now, some of them are probably being able to predict whether a quote was misattributed to someone is probably not going to be easy for the model to learn to do, especially if you're making up the quotes, but certainly elements about what the sort of keywords in the quote should be appearing up here, let's just see here. Things like so many books, so little time books, humor, right? That's a good one to try out. Let me just take that and we can try that later on. So we've got the data there. We're just running it through to basically to get the input IDs, the attention masks, all for that. Now we want to set up our training. The training is just going to use the Hugging Face, the sort of the transformers trainer. We pass in the model here. We then pass in the train data set. So you can see here, we've got this train data set, and then we've got to pass in the arguments, so let's go through some of the arguments, the first ones are this. We're going to have gradient accumulation steps, meaning, and this is, these are the things that you would change if you're trying to run on a smaller GPU. So here we've got, we're going to do four examples for forward passes, and then we're going to do four of those before we calculate the gradients. So normally if you think of a batch, if you were training this with a lot of GPUs, you would just do a batch size of 128 or a lot more. In the Llama paper, they're using batch sizes of 4 million, right? They're using so many GPUs. Unfortunately, we don't have that budget. So here, what I'm trying to show you is that you could use, and this is probably under utilizing it for the A100. We could actually make the batches bigger here because see here, we're basically saying that we're going to do four examples at a time. We're going to collect those gradients. We're going to accumulate them for four steps, and then that will be one batch. So it's the equivalent of doing a batch of 16 here. Next up, we want to set up the warmup steps. So we don't want to just go in there and start with our learning rate at the full amount and shake everything around. We start with the learning rate being extremely low and then building up to the learning rate that we've set, and that will take a certain amount of time. And then we can set the max steps here. So the max steps here I've said is very small. This is more just a toy project to show you getting something loading. We're using floating point 16. We're sitting to this in here. We've got the outputs where we're going to be checking things. And then we just kick off our training. You can see here that it's going to tell us, okay, how long it's going to train. In this case, it's trained very quickly, but you might find for your particular one is going to train for a lot longer. And then we can see like over time that yes, sure enough, our loss is going down. So the model is starting to learn something. You could go through an experiment doing this with lot training than what I've done here. Then the next part is sharing this on to the Hugging Face Hub. So here you can see, I've basically just put my Hugging Face Hub username slash then the model name that I'm going to call it. So this is the Bloom 7 billion Laura tagger is what I've called this here. And I could put some info in for the commit message. I can set this to be private or to be public. I will come and make this checkpoint public afterwards so that you can play with this. But that will then basically upload it. And it's just going to upload the Laura weights too. It's not uploading the full bloom model plus the Laura weights. So you'll find on the Hugging Face Hub. This is going to be a tiny, tiny file. We're talking about megabytes here, not gigabytes here. In fact, you can see here that this is going to be 31 Meg or something when it's fully up uploaded. The next thing is if you want to just do inference, you can just basically, this is how you would bring it in. So you can basically load this in and then this will basically put together the one that you've trained, but it'll also bring in the actual full model as well. So you can see that this is basically bringing in, it's going to work out from this, okay, I need the Bloom 7 billion model, I'll bring that in. I need this tokenizer for that. And I'll bring those in and it will go off and download those. Then finally you're left with this. You can basically do some inference and here we're basically passing in a quote. And we've got our sort of magic three characters that we're going to put out. And then it's going to predict something. Now you can see that, okay, I haven't trained it that long. So it does seem to go into a loop. We could even look at putting a end of sentence tag or something like that in there as well in the data. But we can see, okay, the world is your oyster. It's worked out the keywords there. World and oyster. Let's see. I think I put in this one, so many books, so little time. And we could change this obviously here. We could change that the max tokens, et cetera. Okay. So many books, so little time it's generated books, reading time, reading, writing time, writing gone. Again, you can see that, okay, it's going into sort of repeat mode. This would help probably help if we did this a lot more. Let's put in just something. Okay. Training with PEP and Laura is cool. Let's see. Okay. What will it pick out for that? And you'll find that some of them, it will obviously could pick out keywords, but for some of them too, it will pick out other things. Now it's interesting. Okay. So it's got training and teaching here. Hasn't really worked out PEPF and Laura, which is to be expected. And you can see here that it's got some of its previous training still in there. So you would probably want to, it looks like that there's some things related to training models in there that it bouncing off. You'd want to train this for, for longer. And if you really wanted to use this as a model. But this gives you just a good example of how to make a causal language model with PEPF, fine tuning a bigger causal language model with Laura. And then you can use that for something that you particularly want. It's very easy to play with your dataset, put the whole thing together in here. As always, if there's any questions, please put them in the comments. If you found this useful, please click like and subscribe and feel free to let me know what you would like to see videos going forward. Bye for now.",
        "output": "### Training Large Language Models with Parameter Efficient Fine-Tuning (PEFT)\n\n#### Problems with Training Large Language Models\n- Need a lot more compute to train.\n- File sizes of checkpoints become huge.\n\n#### PEFT and LAURA\n- PEFT (Parameter Efficient Fine-Tuning) uses different techniques, including LAURA (Low Rank Adaptation).\n- PEFT allows for fine-tuning only a small number of extra weights while freezing most of the parameters.\n- Helps prevent catastrophic forgetting.\n- Good for fine-tuning with a small amount of data and generalizing better to other scenarios.\n- Produces tiny checkpoints compared to the original large file sizes.\n\n#### Fine-Tuning a Model with PEFT and LAURA\n1. Load the model and tokenizer (e.g., Bloom 7 billion parameter model).\n2. Set up the LAURA adapters using config (attention heads, alpha scaling, task type, etc.).\n3. Freeze the original weights and add the LAURA adapters.\n4. Train the model using the Hugging Face Trainer.\n5. Save the model and upload the weights to Hugging Face Hub.\n6. Load the model for inference and test it with custom inputs.\n\nNote: Fine-tuning models with PEFT and LAURA requires proper configuration settings, and adapting the dataset to the intended task is crucial. More training iterations could also help with better performance."
    },
    {
        "instruction": "<|SYSTEM|>You are a scholars that is taking markdown notes about some video, you would be giving some written scripts for the task.\nYou have finished writing the notes, and you want to extract the title from the notes.\nThe title should only be the name of the technology or algorithm, no extra words needed.\nPlease write the notes in markdown format. No need to write \"Title:\" for the Title. Make sure the title is within 10 words.\nHere is an example:\n## Hidden Markov Model\n<|ASSISTANT|>## ",
        "input": "<|USER|>### Training Large Language Models with Parameter Efficient Fine-Tuning (PEFT)\n\n#### Problems with Training Large Language Models\n- Need a lot more compute to train.\n- File sizes of checkpoints become huge.\n\n#### PEFT and LAURA\n- PEFT (Parameter Efficient Fine-Tuning) uses different techniques, including LAURA (Low Rank Adaptation).\n- PEFT allows for fine-tuning only a small number of extra weights while freezing most of the parameters.\n- Helps prevent catastrophic forgetting.\n- Good for fine-tuning with a small amount of data and generalizing better to other scenarios.\n- Produces tiny checkpoints compared to the original large file sizes.\n\n#### Fine-Tuning a Model with PEFT and LAURA\n1. Load the model and tokenizer (e.g., Bloom 7 billion parameter model).\n2. Set up the LAURA adapters using config (attention heads, alpha scaling, task type, etc.).\n3. Freeze the original weights and add the LAURA adapters.\n4. Train the model using the Hugging Face Trainer.\n5. Save the model and upload the weights to Hugging Face Hub.\n6. Load the model for inference and test it with custom inputs.\n\nNote: Fine-tuning models with PEFT and LAURA requires proper configuration settings, and adapting the dataset to the intended task is crucial. More training iterations could also help with better performance.",
        "output": "## Parameter Efficient Fine-Tuning (PEFT)"
    },
    {
        "instruction": "<|SYSTEM|>You are a scholars that is taking markdown notes about some video with Wiki style, you would be giving some written scripts for the task.\nAnd summarize each paragraph with a title.\nRemember to keep the technology or algorithm name, very important.\nPlease write the notes in markdown format.\nHere is an example:\n### Definiation\nA deadlock is a situation in which two computer programs sharing the same resource are effectively preventing each other from accessing the resource, resulting in both programs ceasing to function. \nThe earliest computer operating systems ran only one program at a time.\n\n### What it have to do with Relationsal Database ManagementSystem\nIn a database, a deadlock is an unwanted situation in which two or more transactions are waiting indefinitely for one another to give up locks. \nDeadlock is said to be one of the most feared complications in DBMS as it brings the whole system to a Halt. \nWhen you tries to insert or update data on database, sometime your record need to depend on the row record.  \n<|ASSISTANT|>#### ",
        "input": "<|USER|> We present NOPNERV, a new approach for optimizing neural readings field with no pulse prior. Compared to existing approaches, we are able to handle complex trajectories with large rotations and thus obtain high-quality view synthesis. Given the seconds of images, our goal is to estimate the camera pulses and reconstruct the 3D scene with an end-to-end differentiable model. We first predict the depths for each frame using an off-the-shelf monocular depth estimation network. As monocular depth maps have unknown distortions, we parameterize the distortion for each frame and optimize the distortions during training. We also reconstruct the point cloud by backprojecting the recovered depth maps to 3D. The point cloud will be used in the lost times. During training, we optimize the NERF parameters, camera pulses, and depth distortions jointly. Along with the NERF model being reconstructed, the distortion parameters are able to align to a globally consistent space. After training, we obtain the optimized camera pulses and the NERF model which can be rendered from novel viewpoints. This is the training pipeline. For each input view, we render the RGB image and depth maps through a NERF model, and compute losses against the input image and mono depth maps with distortion parameters, which allows us to recover globally consistent distortions. At the same time, we take another reference frame and apply inter-frame losses between two views using the point cloud and RGB images. In this way, we provide constraints to relative camera pulses in addition to the optimization of each individual camera pulse and achieve better optimization performance. Now we show some qualitative results. For novel view synthesis, we visualize two types of trajectories. The first one is a Bezel curve. The trajectories are generated by fitting a Bezel curve to the learned training pulses. Our model is able to produce high-quality rendering results, while other baselines have artifacts. The second one is a Bezel curve. The next one is a Bezel curve. Secondly, we visualize the novel view synthesis results of a spiral path generated from learned pulses. Next, we visualize the rendered images, depth maps, and the pulse trajectories of the training set. The Bezel method can overfit to training images, but fail to estimate correct depths or pulses. The next one is a Bezel curve. The next one is a Bezel curve. The next one is a Bezel curve. The next one is a Bezel curve. The next one is a Bezel curve. The baseline method can overfit to training images, but fail to estimate correct depths or pulses. Compared to the baseline process, our model achieves better synthesis quality and more consistent depth estimation. The top right plots show the camera trajectories, where the red dots represent combat pulses and green dots are the learned pulses. Our method achieves more accurate pulse estimation. Thanks for watching.",
        "output": "### Introduction to NOPNERV\nNOPNERV is a new approach for optimizing neural readings field without pulse prior, capable of handling complex trajectories with large rotations to obtain high-quality view synthesis. The goal is to estimate camera pulses and reconstruct the 3D scene using an end-to-end differentiable model.\n\n### Depth Estimation and Distortion Parameters\nThe depths for each frame are predicted using an off-the-shelf monocular depth estimation network. Due to the unknown distortions in monocular depth maps, the model parameterizes the distortion for each frame and optimizes it during training, resulting in the reconstruction of the point cloud.\n\n### Joint Optimization of NERF Parameters, Camera Pulses, and Depth Distortions\nDuring training, the model optimizes the NERF parameters, camera pulses, and depth distortions simultaneously, aligning distortion parameters to a globally consistent space. The optimized camera pulses and NERF model can then be rendered from novel viewpoints after training.\n\n### Training Pipeline with Input Views and Inter-Frame Losses\nThe training pipeline involves rendering the RGB image and depth maps using a NERF model while computing losses against input images and mono depth maps with distortion parameters. This allows for the recovery of globally consistent distortions. Additionally, inter-frame losses between two views are applied using the point cloud and RGB images, providing constraints to relative camera pulses and achieving better optimization performance.\n\n### Qualitative Results and Comparisons\nThe model produces high-quality rendering results in various types of trajectories, significantly surpassing other baseline methods that often have artifacts or fail to estimate correct depths or pulses accurately. The method also exhibits more accurate pulse estimation.\n\n### Conclusion\nNOPNERV is a novel approach to optimize neural readings field without pulse prior, excelling in handling complex trajectories and obtaining high-quality view synthesis. The joint optimization of NERF parameters, camera pulses, and depth distortions allows for accurate pulse estimation and superior synthesis quality compared to existing methods."
    },
    {
        "instruction": "<|SYSTEM|>You are a scholars that is taking markdown notes about some video, you would be giving some written scripts for the task.\nYou have finished writing the notes, and you want to extract the title from the notes.\nThe title should only be the name of the technology or algorithm, no extra words needed.\nPlease write the notes in markdown format. No need to write \"Title:\" for the Title. Make sure the title is within 10 words.\nHere is an example:\n## Hidden Markov Model\n<|ASSISTANT|>## ",
        "input": "<|USER|>### Introduction to NOPNERV\nNOPNERV is a new approach for optimizing neural readings field without pulse prior, capable of handling complex trajectories with large rotations to obtain high-quality view synthesis. The goal is to estimate camera pulses and reconstruct the 3D scene using an end-to-end differentiable model.\n\n### Depth Estimation and Distortion Parameters\nThe depths for each frame are predicted using an off-the-shelf monocular depth estimation network. Due to the unknown distortions in monocular depth maps, the model parameterizes the distortion for each frame and optimizes it during training, resulting in the reconstruction of the point cloud.\n\n### Joint Optimization of NERF Parameters, Camera Pulses, and Depth Distortions\nDuring training, the model optimizes the NERF parameters, camera pulses, and depth distortions simultaneously, aligning distortion parameters to a globally consistent space. The optimized camera pulses and NERF model can then be rendered from novel viewpoints after training.\n\n### Training Pipeline with Input Views and Inter-Frame Losses\nThe training pipeline involves rendering the RGB image and depth maps using a NERF model while computing losses against input images and mono depth maps with distortion parameters. This allows for the recovery of globally consistent distortions. Additionally, inter-frame losses between two views are applied using the point cloud and RGB images, providing constraints to relative camera pulses and achieving better optimization performance.\n\n### Qualitative Results and Comparisons\nThe model produces high-quality rendering results in various types of trajectories, significantly surpassing other baseline methods that often have artifacts or fail to estimate correct depths or pulses accurately. The method also exhibits more accurate pulse estimation.\n\n### Conclusion\nNOPNERV is a novel approach to optimize neural readings field without pulse prior, excelling in handling complex trajectories and obtaining high-quality view synthesis. The joint optimization of NERF parameters, camera pulses, and depth distortions allows for accurate pulse estimation and superior synthesis quality compared to existing methods.",
        "output": "## NOPNERV"
    },
    {
        "instruction": "<|SYSTEM|>You are a scholars that is taking markdown notes about some video with Wiki style, you would be giving some written scripts for the task.\nAnd summarize each paragraph with a title.\nRemember to keep the technology or algorithm name, very important.\nPlease write the notes in markdown format.\nHere is an example:\n### Definiation\nA deadlock is a situation in which two computer programs sharing the same resource are effectively preventing each other from accessing the resource, resulting in both programs ceasing to function. \nThe earliest computer operating systems ran only one program at a time.\n\n### What it have to do with Relationsal Database ManagementSystem\nIn a database, a deadlock is an unwanted situation in which two or more transactions are waiting indefinitely for one another to give up locks. \nDeadlock is said to be one of the most feared complications in DBMS as it brings the whole system to a Halt. \nWhen you tries to insert or update data on database, sometime your record need to depend on the row record.  \n<|ASSISTANT|>#### ",
        "input": "<|USER|> Hi there, welcome to the new video. Today we'll be going through this very interesting paper, which is titled as Node2Vec, Scalable Feature Learning for Networks. It's from authors from Stanford University. So this paper is also one of the early works in the domain of machine learning with graphs. Just as one of the papers I have explained earlier, which was titled as DeepWalk, that uses random walks to generate embeddings for every node in that graph. This work, Node2Vec, uses a similar idea of generating node embeddings in a graph using random walks. So the main contribution this paper adds over DeepWalk is by introducing a controllable factor to how do you generate random walks. So they talk about introducing a search strategy that does a trade-off between how deep do you want to go in a graph while doing your random walk, or if you want to sample just from the neighbors. Okay, so we'll see to all of this in a little detail. Let's start with the abstract first. We propose Node2Vec, an algorithmic framework for learning continuous feature representation of nodes in a network. In Node2Vec, we learn a mapping of nodes to a low dimensional space of features that maximizes the likelihood of preserving networks' neighborhoods of nodes. Okay, so let's say you have a network that has five nodes in this and those are connected in some format. So Node2Vec learns a dense representation for every node in the network. Let's consider this representation to be let's say 100 dimensions. Now when you try to plot every node of this network in a 100 dimensional space, let's say, or maybe let's say just two dimensional space after you applied PC or TSNE. So the nodes that are near to each other in that space will also be near in the actual structural formation. So this is what Node2Vec tries to preserve. That's what they have written. We try to maximize the likelihood of preserving node network neighborhoods even if you represent them in some low dimensional space. So yeah, our algorithm generalizes prior work which is fixed on rigid notation of network neighborhoods and we argue that the added flexibility in exploring neighborhood is the key for learning rich representations. So yeah, as I already discussed, the way DeepWalk would work at every node it would give equal probability to how it would propagate to the next node and that we would essentially sample multiple random walks from every node and then you would pass all of that to a Word2Vec algorithm. Maybe let's say SkipGram and post that you get density representation for every node in that graph. The contribution of this paper essentially comes by adding a flexibility in terms of how do you explore the neighborhood and create those linear chains of graph. So there is a parameter that they define that kind of encourages to go more in depth in a certain network which gives a feel of how a DFS would work whereas you could also tweak that parameter to just search around in a certain neighborhood, let's say one hop neighbors of it which gives a sense of breadth first search or level search. So after you play around with how do you want to sample these nodes using random walk, you come up with linear chains of all those nodes. Again you pass it to Word2Vec algorithm and you get density representation of every node in that graph. So this paper claims adding this flexibility of if you want to go in a breadth first or depth first fashion is the key to learn rich representation compared to the previous approaches. We demonstrate the efficacy of Node2Vec over existing state-of-the-art techniques on multi-level classification and link prediction on several real-world networks and diverse domains. So this is how they test how good is a network's representation. They do a multi-level classification for every node in a graph. So this can be seen let's say in social media network you have every node as one of the users and the edges that you have kind of signify if they're friends or not. So you could do a multi-level classification for every node in terms of their interest maybe or maybe if they belong to a certain community. So that could be one of the possible applications and talking about link prediction. Link prediction is a task so you must have seen in Facebook network for example the system would recommend you certain friends at certain point of time. So which is based on some common interest and common friends that two persons share. So that use case is essentially an example of link prediction. We have a system that takes two nodes as an input and predicts if there can be an edge link between them or not. So let's move forward. So this diagram shows how BFS and DFS search strategies would work for a node U. So consider you are a user and you are at node U at any point of your time during random walk. So all the red color arrow essentially tell that you are doing a breadth first search which means you are traversing more or less within the same community. Whereas if we see the blue colored arrow you are essentially leaving out of the community and going somewhere in depth within the network structure. So there are two main properties that a network usually shows. The first is called the homophily and the second one is called the structural symmetry. So if you see the depth first search essentially mimics the structural symmetry which means S6 is more or less similar to the node U because it has like four node attached to it at a one hop distance and similar is the case if you see the neighbors of S6. So more or less the structure is little preserved. So DFS lets you handle that kind of symmetrical features whereas if you see the homophily symmetries. So in this case the representation for S1, S3 and S2 will be close to U in some n-dimensional representation space. So yeah this way you can kind of trade off between the type of symmetry you would want your model to learn and encode in a feature vector format for every node. Now talking about the feature learning framework. So let's say we have graph G that has V vertices and E edges and let's say our graph can be undirected or directed it could be weighted or unweighted. We wish to find a function f that takes in a vertex V and represents it in a D dimensional space. So D is the parameter specifying number of dimensions of a feature representation. So these are the number of parameters that we wish to learn. So f is a matrix of V number of rows and D number of columns. So if D is equal to 100 and V is equal to let's say 200. So we have 200 vertices in that graph and each of the node is represented in 200 dimension. So 200 cross 100 are the number of parameters that we wish to learn. They also define NSU as subset of V where NSU is nothing but the neighborhood of node U under sampling strategy S that is some subset from entire set of nodes V. Then they wish to maximize the log probability of predicting the neighborhood of the node U given you have the feature representation for that node and you iterate it for all the number of nodes in that graph. So yeah this is pretty clear because let's say if this is the graph that you have and this is the node U. So given some representation for this node you want to maximize if this this and this are the neighborhood for this node. So this is the probability that you wish to maximize. So to solve this log likelihood we assume conditional independence where we factorize the likelihood into independent segments for every neighbor of the node U. So Ni is all the neighbors for node U under sampling strategy S. So what is the probability that you get neighbor n1 for example if this was the representation for the central node what is the probability that you get let's say n2. So if this was the representation for the central node. So if the representation is good enough you'll get this probability as high so will the product. So that's how you calculate the log likelihood. So probability of observing Ni given the f U which is the feature representation for the central node U is given by the softmax function over the dot product of the neighbors. So you do a dot product of the neighbor and the central node to observe the similarity and you normalize by all the nodes that you have in the network and considering its similarity with the central node U. So now if you plug all of this in the equation one which was this equation of log likelihood you can see we have a term log over there. So now if you take a log of this thing you have log of a minus log of b which is exactly what you can see is written over here. The a comes at this position b comes at this position with the negative sign. So this is the objective that you want to maximize. Given the linear nature of the text the notion of neighborhood can be naturally defined by a sliding window or consecutive words. Networks however are not linear thus a richer notation of the neighborhood is needed. So what they are saying is in text a sentence could be represented as a linear chain of the words that propagates forward as you go in time and to get vector representation of this word let's say you can define a sliding window that has four words in it and the representation for this word depends on what are the words that occur in that context. So under skip gram given this word you want to predict what are the nearby words that occur to be in that window. So that is how you do in context to natural language but networks are not linear in nature so you need some mechanism to kind of convert that non-linear structure that might have hierarchy as well into some linear format of this sort. That is where the sampling strategies come into the picture and how you do it again defines the richness of the representation that you are trying to learn which again what we saw as in the case of DFS and BFS in terms of homophily and structural symmetry. Okay moving forward. Okay so as we discussed there is a bias term what they define for controlling the random walks they call it as search bias alpha so they define a second order random walk with two parameters p and q which guide the walk. So second order random walk which means at any instant t you will also consider the previous state where you have come from. So pi vx is the final transition probability what they define if you're going from node v to node x it's a multiplication of wvx which is the initial probability for that edge between the vertex v and x. Now this is the tweakable parameter which is defined under these three conditions. So let's see the figure number two first. Let's say your node started from point t then after one hop now you are at point v so this is your current location of your random walker object so they define alpha between the vertex v and any of the possibilities from where you can go from v to any of the xi's so which means if you are at v t also become one of the xi's is equal to one by q if the shortest distance between the original node which was t to any of the xi's is equal to two then you put the edge weight as one by q so if you go from t to x1 the distance is one so that doesn't meet a criteria from t to x2 you have a distance of two that's why you have this alpha is equal to one by q over here and similarly from t to x3 you have a distance of two so again you have a alpha of one by q over here so this is the factor that you multiply with the weight that was initialized between each of these edges and finally you have the pi with which you traverse from v to xi's so this is when you have a distance of two similarly if you have a distance of t to any of the xi's as one then alpha is equal to one which is the case from t to x1 if you see the distance is one that's why the alpha is also defined as one between v and x1 the last condition is if t to xi is equal to zero then they define alpha is equal to one by p so as i already mentioned t is also one of the xi's if you are at v so distance is equal to zero which means from t to t itself the distance is zero that's why the alpha between v and t is put as one by p so exactly this is all what they have mentioned over here okay so talking about p and q over here i'm giving a bit of an intuition around it so we see if p is greater than max of either one which is the edge weight from v to x1 or q which is the edge weight from v to x2 and x3 then the overall alpha value for the edge weight from v to t will be very less compared to other edges so the chances of going back from v to t will be really low which means you are kind of promoting a depth first exploration rather than visiting the revisited nodes and similarly for q if it is less than the minimum of maybe one and p then the edge weights between v and x2 and v and x3 will be relatively more so again you will be promoting a depth first exploration so yeah that is the basic intuition to how do you select p and q so finally talking about the entire algorithm you have a graph g that has v vertices e edges and some initial weights w for each of the edges d is the dimension r is the number of walks per node l is the walk length k is the context size p is the return parameter which we just saw and q is the inode parameter so you set the value of p and q and you get a distribution of pi over all the edges that you have in the graph by modifying the initial priors that you had for the edge weights you construct a graph g dash with that and you initialize all the walks to be in an empty list and let's say we define we want to go r number of walks per node so you do an iteration over that then for every node in the vertex that you had you do a node to vec walk you generate a random walk and append all of those walks to the list that you had created earlier so after you are done with generating r number of random walks from every node in that graph you do the back to adjust the feature representation of every node so now this function talks about how you sample the random walks so this was one of the input that comes to this method which is the entire graph then the starting node where you will start the random walk and the length of the walk that you want to traverse so you start your random walk with this initial node in an empty list you insert just the starting node you iterate from 1 to l you choose the last node in that array to be the current node you get the neighbors of that so vcur essentially holds all the neighbors to the current node you sample a node from there based on the pi probability distribution that we have got which is nothing but a function of p and q parameters multiplied by the initial weight distribution and then at last you append that node to the walk list and finally you return that walk so yeah this is the entire pseudo algorithm to how node to vec algorithm works so i guess now we are done with the paper we have experiments and all so yeah if you like such content and appreciate what i do make sure you hit that like button and subscribe to the channel also do share it across with your friends whosoever is interested in such content i'll meet you in the next one bye",
        "output": "### Introduction to Node2Vec\n\nNode2Vec is an algorithm from Stanford University that generates embeddings for nodes in a graph using random walks. Similar to DeepWalk, it uses random walks to create node embeddings but introduces a controllable factor in generating random walks.\n\n### Random Walks and Search Strategy\n\nNode2Vec uses a search strategy that does a trade-off between going deep into the graph or sampling from the neighbors. The main contribution of the paper is by introducing this flexibility in exploring the neighborhood to generate random walks, which is the key to learning richer representations.\n\n### Homophily and Structural Symmetry\n\nThe algorithm tries to preserve the likelihood of maintaining network neighborhoods of nodes even when represented in a lower-dimensional space. It has two main properties, homophily and structural symmetry. Breadth-first search (BFS) captures homophily, while depth-first search (DFS) captures structural symmetry.\n\n### Feature Learning Framework\n\nNode2Vec learns a mapping of nodes to a lower-dimensional space by maximizing the log probability of predicting the network neighborhood of a node given its feature representation. The neighbors are obtained using second-order random walk with parameters p and q, which guide the walk.\n\n### Algorithm\n\nThe algorithm has the following steps:\n1. Set the value of parameters p and q.\n2. Get the distribution of pi over the edges using initial edge weights.\n3. Construct graph with updated edge weights.\n4. Initialize walks as an empty list.\n5. Generate random walks from every node and append to the list of walks.\n6. Use Word2Vec to get the feature representation of every node.\n\nThe pseudo algorithm to Node2Vec works as follows:\n1. Start with the graph, starting node, and walk length.\n2. Start the random walk with the initial node.\n3. Iterate from 1 to the walk length.\n4. Obtain the neighbors of the current node.\n5. Sample a node from the neighbors based on the pi probability distribution.\n6. Append the sampled node to the walk list and return the walk.\n\n### Experiments\n\nNode2Vec was tested on multilabel classification and link prediction tasks on various real-world networks to demonstrate its efficacy over existing state-of-the-art techniques."
    },
    {
        "instruction": "<|SYSTEM|>You are a scholars that is taking markdown notes about some video, you would be giving some written scripts for the task.\nYou have finished writing the notes, and you want to extract the title from the notes.\nThe title should only be the name of the technology or algorithm, no extra words needed.\nPlease write the notes in markdown format. No need to write \"Title:\" for the Title. Make sure the title is within 10 words.\nHere is an example:\n## Hidden Markov Model\n<|ASSISTANT|>## ",
        "input": "<|USER|>### Introduction to Node2Vec\n\nNode2Vec is an algorithm from Stanford University that generates embeddings for nodes in a graph using random walks. Similar to DeepWalk, it uses random walks to create node embeddings but introduces a controllable factor in generating random walks.\n\n### Random Walks and Search Strategy\n\nNode2Vec uses a search strategy that does a trade-off between going deep into the graph or sampling from the neighbors. The main contribution of the paper is by introducing this flexibility in exploring the neighborhood to generate random walks, which is the key to learning richer representations.\n\n### Homophily and Structural Symmetry\n\nThe algorithm tries to preserve the likelihood of maintaining network neighborhoods of nodes even when represented in a lower-dimensional space. It has two main properties, homophily and structural symmetry. Breadth-first search (BFS) captures homophily, while depth-first search (DFS) captures structural symmetry.\n\n### Feature Learning Framework\n\nNode2Vec learns a mapping of nodes to a lower-dimensional space by maximizing the log probability of predicting the network neighborhood of a node given its feature representation. The neighbors are obtained using second-order random walk with parameters p and q, which guide the walk.\n\n### Algorithm\n\nThe algorithm has the following steps:\n1. Set the value of parameters p and q.\n2. Get the distribution of pi over the edges using initial edge weights.\n3. Construct graph with updated edge weights.\n4. Initialize walks as an empty list.\n5. Generate random walks from every node and append to the list of walks.\n6. Use Word2Vec to get the feature representation of every node.\n\nThe pseudo algorithm to Node2Vec works as follows:\n1. Start with the graph, starting node, and walk length.\n2. Start the random walk with the initial node.\n3. Iterate from 1 to the walk length.\n4. Obtain the neighbors of the current node.\n5. Sample a node from the neighbors based on the pi probability distribution.\n6. Append the sampled node to the walk list and return the walk.\n\n### Experiments\n\nNode2Vec was tested on multilabel classification and link prediction tasks on various real-world networks to demonstrate its efficacy over existing state-of-the-art techniques.",
        "output": "## Node2Vec"
    },
    {
        "instruction": "<|SYSTEM|>You are a scholars that is taking markdown notes about some video with Wiki style, you would be giving some written scripts for the task.\nAnd summarize each paragraph with a title.\nRemember to keep the technology or algorithm name, very important.\nPlease write the notes in markdown format.\nHere is an example:\n### Definiation\nA deadlock is a situation in which two computer programs sharing the same resource are effectively preventing each other from accessing the resource, resulting in both programs ceasing to function. \nThe earliest computer operating systems ran only one program at a time.\n\n### What it have to do with Relationsal Database ManagementSystem\nIn a database, a deadlock is an unwanted situation in which two or more transactions are waiting indefinitely for one another to give up locks. \nDeadlock is said to be one of the most feared complications in DBMS as it brings the whole system to a Halt. \nWhen you tries to insert or update data on database, sometime your record need to depend on the row record.  \n<|ASSISTANT|>#### ",
        "input": "<|USER|> So, here you see a classifier that takes a look at this image and assigns one of many, many labels, actually one of 101 labels, as you can see here. And one of the labels is a photo of guacamole, a type of food, and it assigns a really high probability to that, as opposed to like the second prediction, which is ceviche. So you know, classifier, pretty good. Okay, take a look at this classifier. Out of 397 labels, it correctly identifies that this is a television studio. You can go on right here. And so this is a photo of an airplane. Whenever there's a green bar at the top, it means that the respective classifier has this correctly, whenever there is an orange bar, it's an incorrect label with the green bar being the correct label. So you can see here, these classifiers perform sometimes pretty well on these examples and sometimes not. But what you can distinctly see is that these are all from different data sets. So different tasks, there is a satellite image, there is a car, and you're supposed to classify which car it is not only that it is a car. So very diverse set of tasks. And the interesting thing is that this is all the same classifier. So this classifier is it's not even fine tuned. It is a zero shot classifier that handles all of these different training data sets, sorry, not training data sets, all of these different test data sets in one go. So that's already pretty cool. But what you may have noticed is that the labels aren't labels that you would usually see in a classifier. So you know, these these 101 labels here, they are, it says it here, guacamole, that's the label. Interestingly, the label the classifier assigns is not just the word, it's the a photo of guacamole, a type of food. Okay, that's the label the classifier assigns. And the second highest label is a photo of ceviche, a type of food. It's not always a photo, though it is often a photo. But here, you can see, for example, the label that the classifier assigns is a centered satellite photo of permanent cropland, where the the correct label here is the annual cropland, which is down here, again, the label is longer. So there's something interesting going on here. It's the same classifier, it's zero shot. So that means the classifier is not trained on these data sets. It's not trained to fulfill these tasks yet. Still it seems to perform okay. And the labels are quite weird. So this is, this is a new paper by OpenAI, which we're going to look at today, you can see it's a pretty long paper, but we'll cut it short, I promise. And it's called learning transferable visual modes from natural language supervision. And the model colloquially or even also in this paper is referred to as clip. So this is the model has been released along with the DALI model, which you know, can do the chair made of avocado and so on. The DALI model is a generative model that generates images. Clip is a more of a, I won't I want to say discriminative model, but clip is a model that takes in images and text and connects them in a in a in a non generative way. So we're going to see what that entails. It's by Alec Radford and Jungwoo Kim and others, as I said, of OpenAI. So the idea here is to connect text and images. And this has been done in a in a number of ways previously, even in this way, it has been done in one fashion or another, I find the introduction and discussion of related works in this paper to be very, very thorough and superb. So they do assign a lot of credit to people who have had the various ideas. So the goal here is that we want to get a model that can represent images and text really, really well. Okay, so how do we connect images and text? First of all, what what if what if we have a data set of images and text? Okay, so they construct a new data set where there's an image, something like this cat, and a text, a little piece of text to it, like my, my cute cat, images and text like this, you'll find on, you know, for example, social media, you can scrape that Pinterest, whatnot Flickr, people write descriptions along with their pictures. So it's pretty easy to get these pairs of images and text from the internet without having to label them, right. So one motivation of doing this kind of work is, if we train a image classifier model, we always need labeled examples into, you know, into a very predefined set of classes. So in image net, we have 1000 classes or 22,000 respectively, in MNIST, we have 10. However, if we could just somehow learn to connect images with the text that comes along, we wouldn't be bound by the classifier labels, and we could get very good representations. So the original idea or one of the original idea is we take the image and we predict, predict the text from the image. Of course, Dali goes the other way. So Dali somehow goes the other way, taking the text and predicting the image. But the idea is, if we can take an image and from it predict the text, what we get out of it is not only a model that can label images, but what we hope to get out of it is this process right here, maybe very, very good representer. So if this is like the image goes into a neural network with a bunch of layers, and then out comes you know, the text, my cat and so on, then somewhere in here in the intermediate representation of the neural network, there must be a pretty, pretty good representation of what is in the image. So not not only you know, the pixel values, but there must be actually some kind of representation of the concept of cat, because otherwise it could not predict the word cat at the end. Okay. The idea is to get a really good representer. And then you could take that representation and fine tune it to other tasks and so on. So that's one of the ideas that we're going to work off here. And it turns out, this is pretty useful. There have been papers before predicting the simply predicting the caption of images, but it doesn't work too well. So what this model here is going for, and we'll simply, we'll simply, let's look at this graph right here. So they tried first to predict the text. And you can see that zero shot and we're going to look at what exactly zero shot ImageNet accuracy means in this context. But you can see here that they had some success with using a transformer language model to predict the text and images and evaluating that on ImageNet. However, they seem to have more success by using just a bag of words prediction. So what that means is you're not trying to predict the exact words, you're simply trying to predict which words occur in the description. So you see the photo, if you predict cat and my and cute in, you know, any non ordered, you're already correct. And that already gives a sort of a better efficiency. You can see the models here, they tend to go up. But it's questionable if that will ever reach the orange line. And with their new objective with what this paper suggests, you can see right here, the contrastive method, you get a way bigger performance. So we'll look at what this zero shot accuracy means. And why it might be that these simply predicting the text from an image might not be a good enough idea. So let's say we have a model that can do this, we have a model that can take an image, and it can predict the text that appears in it, right? Most of the time, this model right here is also going to give you something like a probability, okay, like a likelihood. So if this is a transformer, you can, you can ask, you know, for its logits, and then you can compute the likelihood of a given label. So if you have such a model, what you can do is exactly what, what they allude to right here. If you have an image task, right, and you have a, you have a model that can predict the the text of an image, you can take that image. And you can run this sort of through your image and through your encoding pipeline. And then you can ask the model, instead of you know, predicting a text, you can ask the model how likely is the text dog? How likely is the text cat for this image? How likely is the text mouse? And then you can, you get some sort of likelihood, right? So maybe it says dog is this likely cat is this likely mouse is this likely. And immediately you have built a classifier. So I hope you can see if, if I have a model that can predict how likely a piece of text goes with an image, I can, by simply asking my model for each of the for each of the classes that are possible in the task, I immediately get a classifier out of that. I mean, I'd have to normalize or something by that, but I immediately get a classifier. And now you can already see why we might want to phrase the things a bit. So I don't want to just put dog and cat right here, even though those are the labels in that task, right? If if I had an image net classifier, I would put here I would put all of the 1000 possible classes and ask the model for each, how likely is that label to go with this image and the model, you know, can produce text. But the model can not only produce, you know, the single word dog, the model can also tell me how likely is the phrase, a photo of a dog, a photo of a dog, or how likely is the phrase, a photo of a cat, and so on, right. So and you can you can see that this result here, the classifier result, it might change actually depending on how you phrase. So here you can use the exact same classes as you use the above. But by rephrasing the prompt, so to say, you might get a better quality classifier or a worse quality classifier. If you already know that your images are all photographs, and you will get a better accuracy because simply, you know, the model if you, you might get a better accuracy by asking the model, hey, how likely is the phrase a photo of a dog going with this image versus the phrase a photo of a cat that might give you a better signal, so less noise in whatever you get as an output than simply going with the single word. Because again, this model is trained to predict this just from a data set scraped from the internet. So how often do people post something, I don't know, an Instagram of their cat and simply write cat with it, whereas, you know, maybe they, they write, here's a photo of my cat, right. So the first phrase a photo of a cat is, or they do like hashtag photo, hashtag cat or something like this. So that's why these classifiers at the bottom, they were constructed from the labels of the data set, but with a prompt that has been adapted by humans to work, you know, find to work particularly well on that data set. So we're sort of back to prompt engineering here. So this is how we go from a model that can assess predict text to a classifier, and that's a zero shot classifier, we don't need to train this classifier on the actual task, we simply need to restrict its possible outputs to the classes at hand, right. This is a bit, it's a bit like a tiny bit like, like, you know, in in Q learning in Q learning, where for in, in each step, you ask your model, well, what if I do action one, and then the model tells you what that's five good, probably that your Q value is five, and then you ask, well, what if I do action two, and then your model says, well, that's seven good, and so on. So it's, it's sort of a similar concept in except, you know, Q learning, we usually train end to end with an actual classifier. But I said, simply predicting text objective might not be good enough, right. So we're going to retain this property of being able to zero shot classifier. But we're going to now switch out our task of how we get to such a model. So instead of predicting text, what does clip do? Clip does the following. So what we're going to do is we're going to take the image right here, and we're going to pass it through an image encoder. And that gives us an image representation. So a vector in some latent space. So this is image one, and then image two right here would be image two here. Okay, so we have a mini batch of images. And that's important. Then we're going to take the text and feed it to the text encoder, also obtaining a representation for the text, right, a single vector for this entire text right here. And then of course, if we go to the second sample in the mini batch, we get the second representation. And the batches, of course, in the training data set, we know that the first, the first text goes with the first image, the second text goes with the second image, the third text goes with the third image, because that's how we scraped it from the internet. And then what we ask the model to do is simply to tell us not so previously, we tried to predict from the image, the text, right, we went through the image encoder. And from this representation here, we try to predict the text. So we no longer do that. What we're trying to do is simply ask, ask the model, which for so for this representation, which of these texts is most appropriate to that particular image? Okay, so this is why it's called a contrastive objective. We know because this is training data, we of course know that image one goes with description one, and image two goes with description two. But we're going to train this in the way that you know, we feed in this image and we ask it to which of all of these texts right here to which of all of these is this image the closest, and we're going to train it such that it is maximally close to the correct one and minimally and far away from all the other. So this, this is why it's contrastive. It contrasts what we know goes together, right, the diagonal elements in this matrix with what we know doesn't go together. And actually, we don't know if a different description wouldn't fit the same image, but we can safely assume that a random piece of text since we do the mini batches randomly, a random piece of text will probably not go with this particular image, at least not as well as the piece of text that we found it with on the internet. Okay, so you get, what you get is effectively for each input, you get a classification task in this direction, you can see right here for image three, there is one correct text that it goes with. And for each text, you get a classification task in this direction. By the way, this is simply an inner product right here, right? You're simply trying to maximize the inner product of things that go together, and minimize the inner product of things that don't go together. So you you multiply the two, put the inner product, you interpret that as a logit. And then you do a softmax classification in this direction, and the softmax classification in this direction. So this is a symmetric loss from the text and image perspective. And yeah, so so it's a classification problem, it's like a classification problem viewed from two different angles. So you can immediately see that this relies on having large enough mini batches, right? So the larger your mini batch, as your mini batch size approximates the entire data set, your representations are going to be more and more detailed, right? So so you wanna, so Pepper the Aussie pup, being close together to this particular image means that in the ideal case, it is close to this image and far away from anything else in in the data set. And as an approximation far away from anything in this particular mini batch. And at inference time, you do very much what we did so far. So you take, if you want to build an image classifier, and the interesting thing is you can also build a text classifier, right? If you have multiple images to go with a text, then you, you can do that it's entirely symmetric. But in this case, you take an image, you put it through the image encoder, you get a representation here, you get all the labels of your classification tasks, right? So this is the label is this right here, you engineer a prompt and that you do as a human, right? This is heuristic, this you as a human think, aha, okay, I'm going to put whatever this is here, you encode all of these labels in their prompt context, through the text encoder, you get the representations here, and you simply ask to which of these labels is it closest, right? So the is the inner product, the highest, and then and that's how you obtain the label zero training needed on the actual task, right? So the tape, the data set that you do this with can be an entirely different data set that then you do this with. And this is extremely, extremely interesting. I've actually seen some some posts on Twitter and Reddit, where people use this to guide a style GAN to produce given pictures with given descriptions and so on. So the possibilities for this are pretty, pretty huge. Okay, so that's, that's the model. The model, it encodes images, encodes text, it does this contrastive objective, what goes together, what needs a part. And now you see why this might be a better representer than, for example, simply pre training a model on an image classification task. Because if you pre train a model on an image classification task, it is going to simply lump together every all the dogs, you know, if this is if this is your classification task, it's going to lump together all the dogs, because there's no need to differentiate the individual dogs from each other, right? It's going to lump all of them together, and forget that they are actually different, right? It's also going to forget everything that doesn't concern the immediate classification problem. Whereas this model here, this model is specific, as it gets better and better, it will pick up at more of the text, right? So in this case, maybe if the model is pretty weak, still it will focus on this pup. And that's about the same as saying, okay, it's a classifier of a dog. But then we can also all see pup if it incorporates that if it gets better, well, it can differentiate it from other dogs. And by the way, it's a pup. So it's a young dog. I can also learn eventually learn its actual name, right? And and so on. So you can see this as the model gets stronger, can pick up more and more nuances of the data set. So they test this, and they tested fairly, fairly, fairly extensively. And I don't think we'll have to go through all of it for me to convince you that this is a good idea, you're going to maybe see it approximately, or immediately. So yes, so they use different, different types of Yes, that's what I wanted to say. They use different types of encoders for the image encoder. So for the text encoder, this is a transformer. So transformer, it's not a particularly big transformer even. And they simply take the end of sentence token, the representation of that at the end, and that's their vector. If you don't know what a transformer is, I've done many, many videos on transformers, find one of them, any of them. For the image encoder, they test out a bunch of different things. So they test out a bunch of variants of ResNet. I've done a video on that. And they also test out a bunch of variants of the visual transformer, the VIT that has recently been popularized. I've also made a video on that. So that's why their model shows up in sort of different flavors and sort of different, different points here. They scale the amount of data, I believe, with the model. So they scale everything together, compute data and model size. And that's why you see different variants of the same model. They also do ensembling. So you know, you have to engineer these prompts. And what you can do is you can engineer better prompts, and that will gain performance. And you can also ensemble over prompts. And you can see right here that that gets you both an efficiency gain if you want to stay at the same performance and also, sorry, yeah, and also it gives you a performance improvement for the same compute with the same model, right? So here, the corresponding dots are the same model. That's why they have the same compute. So that's just one of the fun things you can do. And again, I think prompt engineering will become quite a bit more relevant. So here, you can see, you can see the comparison zero shot clip is competitive with a fully supervised baseline, right? So the baseline here isn't too good. So it's a fully supervised linear classifier fitted on ResNet-50 features on 16 datasets, including ImageNet. So the ResNet-50 is a popular architecture, it's not nowhere near the absolute best we have, but it's a popular architecture. So this ResNet-50, what it has been trained on is that's been trained on ImageNet, right? So you get so and that results in a neural network with a bunch of layers, including a classification layer at the end, right into 1000 classes. So what you do is you pre train this on ImageNet. And then you simply take this part right here, up until the last layer, and you take it. So that's this part right here. And you assume that this has a sort of a good representational power, since it can do ImageNet. And then you simply train a new linear classifier on top that does the classification into whatever new task you want. So this is called, it's called linear probing. So linear probing, you can also do it in the middle, sort of, but in this case, they mean linear probing at the second to last layer, like before the classification layer. So you assume that whatever this is, is a good representation function, you keep it constant, and then you train a linear probe on top of it. This is compared to fine tuning where you would fine tune the entire network on your new task. But they elect to do most of their experiments with linear probing, since it gives you a better indication of the representational power of the bases. So here they compare to ImageNet, right? So on 16 data including ImageNet, so for ImageNet, you would expect ResNet-50 to perform quite well because it's been, its representational base has been trained on ImageNet, and training a linear classifier on top, it should simply give you back the performance that it had on ImageNet. And here you can see how zero shot clip compares to linear probing ResNet-50, right? Zero shot clip compared to an actual trained thing, not the best, but a trained thing. And you can see that on many, many, many datasets, clip outperforms the ResNet-50, zero shot, right? So no training required beyond the pre training. That being said, the pre training is huge. But it's similar to GPT-3, right? You train it once, huge training, but then you can do lots of things. ImageNet interestingly, you see right here, only it's actually improving ImageNet over ResNet-50, crazy, right? Whereas so ResNet-50 still better in various other tasks. So this is not to say that this is the new state of the art or anything, except in STL 10, where it actually appears to be the new state of the art against all the previously including all the supervised whatever, it's the new state of the art on this data set. And the reason is, this STL 10 data set, it has very few training examples per class only. So supervised is very difficult. Transfer learning is kind of difficult, as I understand it, it's not that similar to ImageNet. So that transfer learning is kind of different. So this really seems to be this zero shot clip objective seems to be good, if you have images that are sort of natural, that happen a lot on the internet, but are not really like ImageNet. And there exists quite a number of those, and that you have few labeled examples of if any, right? So that's a, that's a good application domain. However, on more specialized things, they say things like, you know, tumor classification and so on satellite images, this clip objective still does pretty poorly, probably because, you know that that's not the type of images you find on the internet with a piece of text. Super interesting MNIST, one of the easiest tasks in deep learning, it also quite underperforms in this in this thing. So that they do, they do an analysis of these different data sets. So they, they compare to ResNet-50 and also to Visual N-grams right here, and they discuss the the importance of the different data sets. Oh, I find I found this to, I found this to be very interesting. Most standard image classification data sets treat the information naming or describing classes, which enables natural language based zero shot transfer as an afterthought. The vast majority of data sets annotate images with just a numeric ID of the label and contain a file mapping these IDs back to their names in English. Some data sets such as flowers, and the GTSRB, that's a German transport street sign or data set, I don't exactly know, don't appear to include this mapping at all in their released versions, preventing zero shot transfer entirely. So what these authors had to do is they had to like, look at the classes and then sort of label them themselves because their model works on language, whereas this street sign data set probably just came with, this is sign type one, this is sign type two, they have a footnote here. Alec learned much more about flower species and German traffic signs over the course of this project than he originally anticipated. I love that. I love a bit of humor in the papers. And so I made this meme where the street sign is specifically tractors and trucks with an authorized loaded weight of more than 3.5 tons prohibited. I wonder actually how the model does on exactly this sign. But yeah, we'll find out. By the way, the clip model is available in not the big one, but a small one is available actually trained. So you can test it out. And maybe we'll do a video on it where we actually do something with it. So here you can see that if they compare their model to few shot linear probes, so here they compare zero shot clip with few shot linear probes. So before we compare to linear probe, which means means we just trained this linear classifier, but we did it on the whole data set. Okay, so here, we simulate only having very few examples per class, which is where pre training really comes in. And you can see that zero shot clip outperforms a lot of models if you only give them very few labeled examples per class. In fact, it is comparative to a 16. It is comparative to a 16 label bit em. So this is one of the best models that is currently in the public. And that is doing this transfer learning. If you transfer learn with a linear probe, again, this is not fine tuning with a linear probe on 16 samples per class. With this model, you are still only as good as the zero shot, no training at all of the clip model. That is pretty, pretty interesting and pretty cool. The other noteworthy thing is that if you linearly probe the clip model, you way outperform the the largest models there. And also, what is also interesting is that when you do labeled examples for clip when you do linear probe on clip, the performance decreases first and only increases once you get to like four labeled examples per class. And that, you know, is, is pretty intuitive when you think about it. So what you're doing is, so in clip, the zero shot classifier is actually a different one than the linear classifier. So the zero shot classifier is in a way already trained. So it has already trained this sort of last layer, whereas if you do linear probing, you throw that away, you know, the whole part where you encode the text and you blah, blah, blah, blah, you throw that away, and you simply do the old school. So the linear probe here, this is no more that is which text is close, this is simply, I take this, I throw away the last layer, I put in a new last layer, and I do my original classification task. And of course, this layer right here is initialized randomly, it's going to require some training. And maybe you know, one example per class isn't enough, it's just going to pick up on some spurious correlation in the feature. And it's going, that's why it's getting worse initially. But it recovers at four examples per class, and it severely outperforms the other models. So we'll forgive it. They do discover in various experiments here, that it is very, very different from data set to data set, how this model performs zero shot, how it performs versus linear probing, they they find that they find that very often in, in, in some data sets that are far away from sort of natural images, they perform worse in, again, in some data sets, they require lots of labels to match zero shot performance. So it is really a study into sort of, I want to say it's a study into what kind of images appear on the internet. They do. Interestingly, there is a trend in machine learning that if you give more data and compute, then your error goes down, even with the same type of models. And that seems to hold pretty well here, as you can see here, as they scale up, this is the same, this is a ResNet backbone. As you scale that up, zero shot clip performance scales smoothly as a function of model compute. However, they do note that there is a whole bunch of variations of the curve you're seeing is the average. But for the individual tasks in their task data sets, it varies wildly. So there's a lot of noise here. This could be because of how the data sets are selected. This could be because of how the prompts are engineered, there are still a lot unknown right here. They compare various other things like linear probe, linear probe performance of clip models in comparison with state of the art computer vision models. And they do outperform all of these other models, as you can see here. So there is 12 data sets in previous experiments, but the 12 are still sort of similar to ImageNet. But if you include more data sets, of course, that's sort of a selection bias or whatnot. But then this model severely outperforms all of the other models. So the red models here are the red ones are the clip models compared to the other ones. So yeah, this seems to be a step forward in the sort of in the sort of building classifiers for the average user, right? So I can now go ahead take this model and build my own classifier pretty, pretty easily. They also make some interesting discoveries in terms of robustness, robustness to perturbations. So previously, all these models, they sort of pre trained on ImageNet and so on. And people have discovered that as soon as you go away from ImageNet, these, the performance of these models decreases heavily. So if for example, ImageNet v2 is just ImageNet, but is it they try to collect, I've made a video about that, by the way, they try to collect ImageNet as closely as possible to the original test set. And they try to collect a new test set. And immediately, the performance of all the classifiers dropped in the light of this just slightly data shifted data set. And if you, if you sort of try to go away a little bit further, so you just have sketches of these objects, you sort of have this this adversarial placement of objects, you can see right here. It's yeah, it's pretty, it's pretty mean, but still a human could do this, right. You see right here, these are just variations on the themes of ImageNet, they have the same classes. So a classifier trained on ImageNet should be able to also classify these images, right. So here, they compare zero shot clip to models that have been trained on ImageNet. And they find that zero shot clip, even though it matches, so this zero shot clip matches the performance of ImageNet, by the way, huge achievement, right? This is a fully trained model on ImageNet. And this is a not the state of the art, but respectable top one performance on ImageNet. And zero shot classifier matches that performance. This is crazy. Okay. You can see as this classifier degrades, degrades, degrades, degrades, degrades, as you go to harder and harder data sets that are all technically ImageNet images, like in the same classes, this classifier, it sometimes even, you know, gets better, but it, you know, it keeps up its performance, which, and see here, the difference between it gets just larger and larger. The clip is way more robust. And of course, this model right here is trained to predict these specific types of images. So it knows very well, like how to keep them apart. The only thing it has to do as a classifier of ImageNet is keep apart the individual instances of exactly those classes in exactly this data set. So it forgets about everything else, right. And as a result, it has never seen a sketch. It like a banana is yellow, what are you talking about? So it heavily degrades, right. Whereas clip, it simply knows how to sort of connect images to text. So while clip realizes that, of course, both are described as banana, it somehow has to account for the fact that there are also lemons in here, right, it has to somehow represent that it has to represent that this is a bunch of fruit. And that this is here, maybe a, you know, high grade picture, like on a magazine, where this here might be more of a sort of random GoPro fallen into some bunch of bananas, it has to somehow represent all of this if it, you know, performs well on its task, and thereby its representation will be nuanced enough, such that it can transfer more easily, it picks up on different features than only distinguishing banana from, you know, other classes in the ImageNet data set. And that results. So here is the curve in that if you had the ideally robust model, you'd have this right here. So, depending on the performance on the natural distortions than on ImageNet, in the original ImageNet, you can see that all of the standard ImageNet training examples, including all the robustness techniques that barely lift away from this curve are massively outperformed by a zero, again, a zero shot classifier that hasn't even been trained on ImageNet. And the fact that it hasn't been trained on ImageNet might be one of the, you know, things that it actually is very helpful. So they do, they do some investigation into it in including that you can in fact, adapt to ImageNet. So you can in, I think that's the that's a linear probe, if you linear probe clip, you can improve the performance on ImageNet. Where interestingly, you can improve the performance on ImageNet by doing a linear probe on top of clip. So this is logistic regression clip, while only mildly degrading your performance on these other data sets. So there seems to be a value to only to just having the representation. So the representation itself seems to be more stable. Okay, so you can see as you adapt to ImageNet, this performance improves massively, but it only degrades a little bit across the other data sets. So that means Yeah, as I said, the representation itself is more nuanced, such that even if you train a linear classifier on pure classification, you'll still keep up the performance on the other tasks. You can also adapt to class shift. So by better prompt, sort of prompt engineering for some of these sub tasks, but I think that's a sort of a minor thing. All right. Yeah, I don't want to go, you know, too much. They also compare to humans, which is very interesting. And they discovered that, you know, samples that are hard for the clip model are also hard for the human model. They do some sort of duplicate detection from their training data set, because their training data set is 400 million images together with text, right? So it's conceivable that there's some duplicates, but they find, even if there is, there's generally not a problem. And they have like a three or four page broader impact section, as you can see right here, which you know is. So if you read it, it reads sort of like, yeah, there are problems with these models, we are better than other models, but we're still not good enough, or things like this, or they always they're like, yeah, this is, of course, we're better, like, they're better at everything. But then again, you know, this is only preliminary, more study is needed, and so on. But I, so they have some fairly interesting, interesting results. So they, what they do is, since there is such a focus on prompt engineering, right, it actually matters what you give to the model as possible labels. So this is no longer fixed labels, you can give any labels. So they have these data sets where you, you know, for example, this fair face, fair face race, where you try to categorize faces into different ethnicities, ethnicities or races, these seven things that are given here. They also include some non human categories. Or is it so they include, they include categories such as here, such as animal, chimpanzee, gorilla, orangutan. And they also include sort of crime categories like thief, suspicious person, criminal. And then they research how how the model misbehaves. And these models, they do do a fair bit of, you know, kind of misclassification right here, as you can see, they also, so they notice that the misclassification is especially there for younger people. So these are the ages of people. And here are the misclassification rates, you can see the misclassifications are mostly for younger people, then they simply add a child category. And then the misclassification for young people all of a sudden drops because the model now has the option to classify them as a child. So this, I think the result of the paper and especially of the broader impact section, one of the results is that it matters a lot how you engineer the prompts, which is something we already knew. But of course, this can be particularly, particularly crucial in some applications, in some concerning applications, that's kind of one of their points right here, you can see that the paper is huge. And it also has a huge appendix. And they do, as I said, a lot more experiments right here. But all in all, this is a very, very cool approach, I feel. And it's, as I said, a step towards making it easier for the, you know, the everyday person to build their own classifier for, you know, you can do quite niche tasks, as long as they're sort of natural images, this will work fairly, fairly well. I think it's pretty cool. It gives, it gives a little bit of more freedom in how you work with these models. And I'm excited for people to come up with ideas of how to use this, how to connect this to other models such as you can connect it, as we already saw with Dolly, you can connect it with StyleGAN as some people are doing. And sure you can connect it to something like GPT-3. And it's going to be an exciting world. Alright, that was it for me. Thanks.",
        "output": "### Introduction\n\nClip is an OpenAI model that aims to connect text and images for various tasks, and it comes with a new contrastive objective that results in better performance compared to previous methods.\n\n### Method\n\nThe Clip model takes an image, passes it through an image encoder, and obtains an image representation in a latent space. Similarly, it takes the related text and passes it through a text encoder and gets a text representation. Then, the model is trained to maximize the inner product of the image representation and its corresponding text representation while minimizing the inner products of non-corresponding pairs. \n\n### Inference\n\nAt inference time, given a new image, we obtain its representation using the image encoder and then find the closest text representation in the latent space, and that text is assigned to the image. This allows us to perform zero-shot classification on never-seen-before datasets.\n\n### Experiments\n\nClip is tested on various datasets, and its performance is compared to previous methods like predicting text or using bag of words prediction. It shows a better accuracy compared to those methods. The model can also be fine-tuned to other related tasks using prompt engineering.\n\n### Summary\n\nClip is a new model by OpenAI that can associate images and text for numerous tasks. It employs a contrastive method that results in greater accuracy than earlier techniques. Clip can be employed for zero-shot classification without the need for retraining on unseen datasets, and it can also be fine-tuned to other tasks as needed."
    },
    {
        "instruction": "<|SYSTEM|>You are a scholars that is taking markdown notes about some video, you would be giving some written scripts for the task.\nYou have finished writing the notes, and you want to extract the title from the notes.\nThe title should only be the name of the technology or algorithm, no extra words needed.\nPlease write the notes in markdown format. No need to write \"Title:\" for the Title. Make sure the title is within 10 words.\nHere is an example:\n## Hidden Markov Model\n<|ASSISTANT|>## ",
        "input": "<|USER|>### Introduction\n\nClip is an OpenAI model that aims to connect text and images for various tasks, and it comes with a new contrastive objective that results in better performance compared to previous methods.\n\n### Method\n\nThe Clip model takes an image, passes it through an image encoder, and obtains an image representation in a latent space. Similarly, it takes the related text and passes it through a text encoder and gets a text representation. Then, the model is trained to maximize the inner product of the image representation and its corresponding text representation while minimizing the inner products of non-corresponding pairs. \n\n### Inference\n\nAt inference time, given a new image, we obtain its representation using the image encoder and then find the closest text representation in the latent space, and that text is assigned to the image. This allows us to perform zero-shot classification on never-seen-before datasets.\n\n### Experiments\n\nClip is tested on various datasets, and its performance is compared to previous methods like predicting text or using bag of words prediction. It shows a better accuracy compared to those methods. The model can also be fine-tuned to other related tasks using prompt engineering.\n\n### Summary\n\nClip is a new model by OpenAI that can associate images and text for numerous tasks. It employs a contrastive method that results in greater accuracy than earlier techniques. Clip can be employed for zero-shot classification without the need for retraining on unseen datasets, and it can also be fine-tuned to other tasks as needed.",
        "output": "## Clip"
    },
    {
        "instruction": "<|SYSTEM|>You are a scholars that is taking markdown notes about some video with Wiki style, you would be giving some written scripts for the task.\nAnd summarize each paragraph with a title.\nRemember to keep the technology or algorithm name, very important.\nPlease write the notes in markdown format.\nHere is an example:\n### Definiation\nA deadlock is a situation in which two computer programs sharing the same resource are effectively preventing each other from accessing the resource, resulting in both programs ceasing to function. \nThe earliest computer operating systems ran only one program at a time.\n\n### What it have to do with Relationsal Database ManagementSystem\nIn a database, a deadlock is an unwanted situation in which two or more transactions are waiting indefinitely for one another to give up locks. \nDeadlock is said to be one of the most feared complications in DBMS as it brings the whole system to a Halt. \nWhen you tries to insert or update data on database, sometime your record need to depend on the row record.  \n<|ASSISTANT|>#### ",
        "input": "<|USER|> Welcome to our tutorial on Denoising Diffusion-Based Geotectic Modeling Foundations and Applications. My name is Arash Vatat. I'm a principal research scientist with NVIDIA Research and today I'm very excited to share this tutorial with you along with my dear friends and collaborators, including Carson Krauss, who is a senior research scientist with NVIDIA, as well as Ruqi Gao, who is a research scientist with Google Brain. Before starting the tutorial, I'd like to mention that the earlier version of this tutorial was originally presented at CVPR 2022 in New Orleans. This tutorial received a lot of interest from the research community and given this interest, we decided to record our presentation again after the conference and we'd like to share this project with the research community through YouTube. If you happen to watch this video and you enjoy this video, we would like to encourage you to share this with your friends and collaborators and hopefully together we can create more interest around Noising Diffusion-Based Geotectic Models. If you're watching this video, most likely you're familiar with deep genitive learning. In deep genitive learning, we assume that we have access to collects of samples drawn from unknown distribution. We use this collection of training data to train a deep neural network. If everything goes smoothly, at the test time we can use this deep neural network to draw new samples that would hopefully mimic the training data distribution. For example, if we use these images of cats to train our deep neural network at the test time, we do hope that we can also generate images of cute handsome cats as you can see in the bottom. Deep genitive learning have many applications. Mostly the main application is content generation that have use cases in different industries, including for example, entertainment industry. Deep genitive learning can be used for representation learning as well. If we have a deep genitive model that generates very high quality images, most likely the internal representation in that model can be used also for downstream applications, such as semantic image segmentation as you can see in this slide. Deep genitive models can be also used for building artistic tools. In this example that you can see on this slide, we have a tool that can be used by an artist who happens to be just a six-year-old kid to draw a semantic layout of a scene, and this tool can take the semantic layout and generate a corresponding high quality image that has the same semantic layout. If you're a researcher and you've watched the landscape of deep genitive learning, you're going to see that this landscape is filled with various frameworks, ranging from genitive adversarial networks to various autoencoders, energy-based models, and autoregressive models and normalizing flows. Historically, the computer vision community has been using genitive adversarial networks as one of their main tools for training genitive models. In this talk, we would like to argue that there is a new and powerful genitive framework called the noise and diffusion models. These models obtain very strong results in generation, and we hopefully want to convince you that these models are very strong and can be used for various applications. Hopefully in this tutorial, we're going to provide you with some foundational knowledge that requires for using these models in practice, and we're going to even talk about how these models are currently used for tackling some of the interesting applications that exist in the computer vision community. As I mentioned earlier, the noise and diffusion models have recently emerged as a powerful genitive model outperforming GANs. In these two papers shown on this slide, one from OpenAI on the left side and one from Google on the right side, researchers observed that you can use the noise and diffusion models to train genitive models on challenging datasets such as ImageNet, and the results generated by these models is often very high quality and very diverse, something that we haven't seen previously with other genitive models such as GANs. The noise and diffusion-based ones have already been applied to interesting problems such as super-resolution. In this slide, you can see a super-resolution framework that takes a low-resolution image, 64 by 64 dimension, and generates high-resolution image in 1024 by 1024 pixels. And this result showed that actually super-resolution models built on top of the noise and diffusion models can generate very high quality diagrams. If you're on social media, you've been probably having a hard time not noticing a lot of excitement that was created around Dolly 2 and Imagine. These two frameworks are examples of text-to-image genitive models that take text as input and they generate a corresponding image that can be described using that text. In their core, these models use the noise and diffusion genitive models, and on the left side, you can see for some Dolly 2 built by OpenAI, can actually create this image of teddy bears skateboarding in Times Square. And on the right side, you can see Imagine can generate images of multiple teddy bears celebrating their colleagues' birthday while sitting behind the cake that looks like pizza. This is impressive. These models can generate very high quality diverse images and they only take text as input. So today, not only are we going to talk about diffusion models, we're going to even talk about how you can use diffusion models to create such models. Towards the end of video, Rucci will talk about these applications. So today's program consists of six parts, six main parts, besides introduction and conclusion. The first three parts that are shown in green are mostly the technical components of the program. I'm going to start with part one. I will talk about the noise and diffusion probabilistic models. And after me, Carson will talk about the score-based genitive modeling with differential equations. And after us, Rucci will talk about advanced techniques, mostly around accelerated sampling and condition generation. These parts, each one of them will be roughly around 35 minutes to 45 minutes. And after these parts, we then have three short parts around applications and mostly computer vision applications that have been recently used diffusion models in their core to tackle various deep gen to learning related applications. And finally, we're going to have a very short segment where I will talk about conclusions, open problems and final remarks. Before starting, I'd like to mention that our slides, videos and some content will be available on this website. So I'd like to encourage you to bookmark this website. We're hoping to add more content in future to this website. Before starting the presentation, I'd like to just mention that we did our best to include as many papers that we could. And we thought that could be interesting to the community. However, due to limited time and capacity, of course we could not include every paper. So if there's a particular paper that you are passionate about, you're very excited about it, and you would like to be included in this tutorial, we apologize that we couldn't do that. But we encourage you to send us an email, let us know that there was a paper that would be interesting to have in our program. And hopefully in the future versions of this tutorial, we will try to include those papers as well. With that in mind, I will start the first segment, the Nervous and Diffusion Probabilistic Models. So part one, the Nervous and Diffusion Probabilistic Models. Here you can see an image of three cute dogs who are trying to understand the Nervous and Diffusion Probabilistic Models. And as you can see, they're a little bit lost. So let's go over these models and discuss how these models can be an alternative model. So Nervous and Diffusion models officially consist of two processes, a forward diffusion process that gradually adds noise to input. This process is shown on this figure from left to right. It starts from the image of this cute cat. His name is Peanut, he's my cat. We can start from him and we're going to add noise to this image one step at a time. The forward diffusion process does this in so many steps such that eventually on the right side, we converge to white noise distribution. The second process is the reverse denoising process that learns to generate data by denoising. This process is shown on this slide from right to left and it starts from white noise and it learns to generate data on the left side by denoising. So this process will take a noisy image and it will generate a less noisy version of it and it will repeat this process such that it can convert noise to data. So we're going to dig deeper into these two processes and we will see how we can define this process formally. The forward diffusion process, as I said, it starts from data and generates these intermediate noisy images by just simply adding noise one step at a time. At every step, we're going to assume that we're going to use the normal distribution to generate this noisy image condition on the previous image. This normal distribution will have a very simple form. We're going to represent this normal distribution using Q that takes this X at previous step and generates X at the current step. So it takes, for example, X1 and it generates X2. As I said, it's a normal distribution over the current step Xt where the mean is denoted by this square root of one minus beta t times the image at the previous step and this beta t representing the variance. For the moment, assume that this beta t is just simply very small positive scalar value. It can be like 0.001, something like this. And here, this normal distribution basically takes the image at the previous steps. It rescales this image, the values, pixel values in this image by the square root of one minus beta t and it adds tiny bit of noise where the variance of noise is beta t. So this is just a diffusion cannon we can call per time step because we had this very simple form per time step, like per step in order to generate Xt given Xt minus one. Now we can also define the joint distribution for all the samples that will be generated in this trajectory starting from X1 all the way going to X capital T. Capital T represents the number of steps in our model and the joint distribution of all these samples conditioned on this input image of P naught will be the product of conditionals that are formed at each step. So this just represents the joint distribution of all the samples that will be generated on this trajectory using this Markov process. This is a Markov process that generates one step, that generates examples one step at a time given the previous examples. Now that we know how we can generate samples one step at a time, you may ask me how can I now just take this input image and jump to particular time step? Do I need to generate samples one step at a time or can I just take this X0 and generate X cap, Xt or X4 for example here just directly? Because in the forward process we're using a very simple Gaussian kernel to diffuse the data, we can actually show that because of this simple form, we can first define this scalar. This scalar alpha bar t is the product of one minus betas from time step one all the way up to current step t. So this is just defined based on the parameters of the diffusion kernel and having defined this alpha bar t, now we can define a Gaussian kernel or the diffusion kernel that would generate Xt given X0. So for example we can now generate using this Gaussian kernel, we can sample from X4 given X0. This would be again a normal distribution where mean is same as the input image really scaled with this square root of alpha bar t defined here alpha bar and then the variance is also one minus alpha bar t. So just remember that because betas are just parameters of the diffusion process, we can just compute this alpha bar t very easily and then we can sample from Xt given X0 using this normal distribution and this we're going to call this diffusion kernel that diffuses input at time step 0 to time step Xt. Recall that if you want a sample from just a simple normal distribution you can use the reparameterization trick. So if you want to draw samples from Xt you can just set Xt to mean plus some white nose epsilon that is drawn from standard normal distribution rescaled with this square root of one minus alpha bar t which represents just a standard deviation of this normal distribution. So using this we can just simply generate samples at time step t given samples at time step 0. So given X0 we can just diffuse it easily to time step t. Beta t values are important. These are basically we're going to call beta t values as the noise schedule that defines how we're diffusing the data and this noise schedule is designed such that alpha bar t, this alpha bar at the very last step, would converge to 0. And when this converges to 0, if you just set alpha bar t to 0 here you're going to see that because the way that the forward diffusion process is defined this diffusion channel at last step given the X capital T given X0 would be just can be approximately using normal distribution, standard normal distribution. This basically means that at the end of this process diffused data will have just standard normal distribution. This is something that we will need later when we want to define the generative part. Now that I've talked about this forward process, like how we can diffuse the diffusion channel that diffuses data, let's talk about the marginal diffuse data distribution. Let's talk about what happens to data distribution as we go forward in the diffusion process. So have in mind that diffusion channel that generates Xt given X0 is different than the diffuse data distribution. So we can use Qxt to represent diffuse data distribution. And in order to obtain this diffuse data distribution, we first need to form the joint over clean data input data X0 and diffuse data Xt. This joint simply can be defined as product of input data distribution Qx0 times this diffusion channel, which is just a simple normal distribution. And now we can marginalize at X0, we can just integrate at X0 and this will give us marginal diffused data distribution at time step t. If we just consider a very simple one dimensional data, and we here I'm visualizing the diffuse data distribution at different time steps. On the left side, we have data distribution at time step 0. Y-axis represents this just one dimensional random variable and this X-axis represents the PDF, probability density function of this random variable at time step 0. This is just the data distribution visualized for one toy example, one dimensional toy example. Here you can see the visualization of diffuse data distributions. And as you can see, as we go in the forward process, we just take this data distribution, we're making kind of this distribution smoother and smoother as we go forward in time. And eventually it becomes so smooth that we can just represent this distribution using standard normal distribution, zero mean unit variance normal distribution. As you see this smoothing process, we can actually show mathematically this diffusion channel in the forward process, this diffusion channel here is kind of applying a Gaussian convolution to the data distribution. So this smoothing process can be just represented as Gaussian convolution, in the sense of signal processing convolution that takes input data distribution and makes it smoother and smoother. However, we should have in mind that we actually don't have access to these, to input data distribution and intermediate diffuse data distribution practice. Usually we only have samples from data distribution. We don't have access to the explicit form of the probability density function of data distribution. So even though we don't have access to this distribution or the intermediate diffuse distributions, we don't have to draw samples from diffuse data distribution. So in order to generate data from diffuse data distribution, we can just simply sample from training data X0, and then we can just follow the form of diffusion process, sample Xt from X0 in order to draw samples from Xt. And this is basically a principle of ancestral sampling that you can basically use in order to generate data, for example, at time step t. You can just use the training data, sample from training data, diffuse it and sample at that Xt using diffusion cameras. And that will give you samples from marginal data distribution. Okay, so, so far we talked about forward process and how this forward process just smoothens the data distribution. Now let's talk about how we can define a generative model out of this. In order to generate data in diffusion models, what we can do, we can start from this base distribution at the end, we know that the diffuse data distribution would converge to zero mean unit values, standard normal distribution. So we can sample from standard normal distribution and we can follow the reverse process, where at every step we can now sample from the denoising model that generates the less noisy version of data given current step. So this is the reverse model where we now use the true denoising distribution to sample from Xt minus one given Xt. So we just need to start from Xt and just use this true denoising model to generate samples at time step zero. So the algorithm will be something like this. We sample X capital T from standard normal distribution and we sample iteratively from Xt minus one using this true denoising distribution. The problem here is that in general the denoising distribution Xt minus one given Xt is intractable. We don't have access to this distribution. We can use Bayes' rule to show that this distribution is proportional to the product of the marginal data distribution of Xt minus one times this diffusion kernel at step t. This is simple, it's just Gaussian distribution. However, this distribution marginal data distribution is intractable and in general because of this, this product is also intractable. So we don't have it in closed form. Now that we don't have it in closed form, you may say, can I approximate this denoising distribution from data? And if yes, what is the best form I can use to represent that denoising model? So the good news is, yes, we can try to train a model to mimic this true denoising distribution Xt minus one given Xt. And if you want to model that, the best model that you can use, this testicle model you can use to represent this one is actually a normal distribution if in the full-world process we use very small noise or the variance of the noise that we're adding at each step is very small. If we know the full-world process adds a very small amount of noise, we know theoretically that actually the reverse process can be also approximated using a normal distribution. So this denoising model can be represented using denoising model. So now that we know this, we can actually define a parametric model to mimic or to train this true denoising model. So this formally defines our parametric reverse denoising process. Remember that the reverse denoising process starts from noise and generates data by denoising once at a time. We're going to assume that the distribution of data at time step capital T at the end of the forward process, so it's the beginning of the reverse process, will be standard normal distribution here. We assume this data has a standard normal distribution. We define this denoising distribution, this is a parametric denoising distribution that samples from Xt minus one given Xt. We're going to assume that it also can be represented using normal distribution, where now mean here is parametric. It's a deep neural network that takes noisy image Xt and predicts the mean of the less noisy version, less noisy image. So this neural network is the training component. Then we have also the variance per time step. This is just, think of this sigma t squared as just some scalar value that represents the variance per time step. And for that, just assume that it's just some parameter or we have access to it. We're going to talk about it later. But the most important part is this mean parameter. Remember, this is a trainable network and it takes a noisy image at Xt and predicts the mean of less noisy image at Xt minus one. Because it takes an image and predicts an image, we can actually model this using a unit that has the same input and output shape. Or you can think of it as just a denoising autoencoder that noises the input image to less noisy level, basically. And we're going to talk about the architecture of this denoising model, this mean factor model. Now that we have these definitions, we can define the joint distribution on the full trajectory. This joint distribution is the product of the base distribution at step Xt and the product of conditionals at all these t steps, where the condition comes from our denoising model. This is again just a Markov process and we can define the joint distribution of the full trajectory by the product of base distribution and the product of these individual conditionals on each denoising step. Okay, so now, so far we talked about the forward process, the reverse process. Now let's talk about how we can train these models, how we can train the denoising model. For training denoising model, we're going to use variation upper bound that is mostly or commonly used for training via short encoders. In the variation upper bound, we ideally want to optimize marginal likelihood of the data under our parametric model. So this, here we have this expectation over training data distribution where we are computing the expected value of the log-to-log-to-value that our trainable model gives to data distribution. Unfortunately, this is interactive, we don't have access to this quantity here, but we can define variation upper bound. Right now we have this expectation over training data, we have expectation over samples drawn from the forward process. This is forward process, you can think of it as encoder in VAE that samples from latent space, so you can think of these as latent variable x1 to capital T. And we have this log ratio here where the denominator is the joint distribution of the samples on the full trajectory that is given by our denoising model p theta. And in the denominator, we have the likelihood of the samples generated by encoder. This is basically the same objective or, yeah, same training object you would see variation bound in variation autoencoder. This is exactly the same. The assumption is that the forward process is kind of like encoder and the reverse process is the genitive model at the VAE. These two papers, by the way, our papers, the links on our slides are clickable, so if you want to check these papers, just find our slides and click on these references and you're going to find the paper. So these two papers showed that this variation bound can be decomposed to several terms that we're going to go one by one. The first term is just simply the KL divergence from the diffusion kernel in the last system x capital T given x0 to this base distribution xT. Remember by definition, the diffusion kernel for x capital T converges to standard normal distribution, which is the same distribution we assume for xT. So we can completely ignore this term because by definition, forward process defines such that at the end of process, samples converge to zero mean unit variance distribution. So we can ignore this term. We have this KL term that I'm going to go into details. And then we have this term that can be considered as the reconstruction term in VAEs. This just measures the likelihood of input clean image, even the noisy image at first step under our denoising model. This term is also very simple and basically is structured very similar to this other term. So for a moment, just assume that we can compute this very easily and we just ignore for a moment. And mostly we just need to focus on this term, which is the most important term here. This is the KL divergence between this Q xT minus one given xT and x0 to our denoising model. This is our parametric denoising model, which is xT minus one given xT. This Q distribution is a particular new distribution. We're going to call it the tractable posterior distribution. This samples from xT minus one, less noisy image, condition on noisy image xT and clean image at time step zero. So it's kind of like you have clean image, you have noisy image, and you want to predict what would be the less noisy variation of this image if you knew what was the starting point, what was the x0 for generating this noisy image. It turns out that this distribution is also very simple because the forward process is just Gaussian distribution. The posterior distribution here is also very simple. This is a particular posterior distribution, it's condition on x0. We know what is the starting point. So this distribution is also normal distribution with mean expressed here. This mean is just simply weighted average of the clean image and the noisy image at xT. So it's actually very interesting if you have clean image and you have noisy image, if you want to predict what is the distribution of xT minus one, the mean, that distribution would be normal. This expression is just normal. And the mean of that normal is just the weighted average of these two images where the weights come from our diffusion process, parameters of diffusion process. So very simple expression. And the variance of this distribution is also defined based on the parameters of the forward process. So you can think of it like this beta tilde t can be computed very easily from the parameters of the diffusion process. So that we know none of this here, it's interesting. So we have the scale divergence from this distribution which is normal. And then this denoising model which we assume that is normal as well. So now that we have two normal distributions, the scale divergence there, so I'm just writing down the same scale divergence again, can we, the scale divergence can be computed analytically for two normal distributions. And we can show that the scale divergence simply just boils down to the squared output distance between the mean of this tractable posterior and the mean of the denoising model. Plus some constant terms as we can ignore these constant terms do not depend on any trainable parameters. So this just basically scale divergence is really interesting, it just boils down to the difference between the mean of this denoising, sorry, this mean of the tractable posterior and our denoising model, parameter of the denoising model, which is represented by the mean of that time. And this weight is 1 over 2 sigma t squared, it's just the invariance used in the denoising distribution. So you can ignore for a moment this coefficient, so we're going to focus on these two terms. Recall that if you want to generate x t, if you want to generate a sample at time step t, you can use this parameterization, which we discussed earlier. And in this paper, Ho et al. in NeurIPS 2020, we observed that you can also express the mean of the tractable posterior distribution I discussed as x t, the input noise image, minus the noise that was used to generate that data. To get this expression, it's very simple, you just need to do some arithmetic operations on the on this equation and just plug the definition of x t from the parameterization. With some arithmetic operations, you will see that if you basically have a noisy image and you want to predict the less noisy version, right, if you knew the noise that was used to generate that noisy image, you can just subtract some rescaled version. So this is the scaled version of those. So you can take noise, subtract just some scaled version of that noise from x t to get the mean of x t minus one. This is kind of very interesting information. So you basically can represent this mean in a very simple form, the expression of x t and epsilon, the noise that was used to generate x t. So this actually is the same noise that was used to generate x t. Knowing that knowledge, it means that now if we want to parameterize this network, we can use this knowledge in the parameterization of this model. So we can say that in order to predict this mean of less noisy images, we're going to just take x t and subtract it from a neural network that predicts the noise that was used to generate this x t. So basically we train a neural network to predict the noise that was used in order to generate x t in order to represent this denoising model. This is just a parameterization trick. Instead of just representing the mean of the noisy model directly, what we can do is that we can just represent the noise that was used to generate x t. And if you have this, you can just subtract this from x t in order to get the mean of the denoising model. So if you assume this parameterization for the denoising model, and now we also know that this is true for mu theta t, if you plug these two expressions into this, you can actually get a very simple expression here. So we have this 18 minus 1, this is the same term, 18 minus 1 becomes just you need to draw samples from training data distribution, you draw some noise vector from standard normal distribution, and using this noise vector you just generate this x t using diffuse sample, using the parameterization trick. You can now pass this diffuse sample to your epsilon prediction network, the network that is trained to predict the noise that was used in order to generate x t. So it's basically a simple algorithm. You draw samples from data distribution, you draw noise, you generate x t from that noise and input data, and you train the network to predict the noise that was used in order to generate that x t. And these grids here are just some scalar parameters that comes from this basic 1 over 2 sigma t here, and this 1 over square root of beta t. And these terms that we know, we can compute very easily based on the parameters of the diffusion process. So you can ignore them for a moment. Just think of them as scalar parameters that we can compute from the diffusion parameters. So 18 minus 1 can be represented as this weighted objective. So we're going to just summarize these weights as lambda t. We're going to define a new scalar lambda t. This lambda t ensures that your training objective is weighted properly for maximum data likelihood training. So by using this lambda t weights, you're actually maximizing data likelihood. But however, what happens is that this lambda t ends up being very large for small t's, and it is small for large t's. So it kind of monotonically decreases. For small t's, it's very high, and for large t's, it's very small. And this is basically how the maximum data likelihood training is formulated. However, in this paper by Ho et al. in Europe 2020, they observed that if you simply drop these weights, or equally just if you simply set these weights to one and train the model using this unweighted version, like if you drop this weight, you're going to get very high quality sample generation using the fusion models. So they introduced this very simple objective that does not have this weighting anymore. This weighting is one. So again, this objective simply draws samples from data distribution, draws white noise, and randomly samples from one of the time steps from one to capital T. It generates a diffused sample using the parameterization trick, and it trains a model to predict the noise injected. So it's exactly the same objective without any weighting, and it can be done very easily. And the authors showed that actually with this weighting, you will get very high quality sample generation using diffusion models. This objective weighting actually plays a key role in getting high quality sample generation in diffusion models. So if you're interested in checking this area, I would encourage you to check this paper by Choy et al. published at CU 2020, too, that discuss how you can potentially change this weighting over time to get even better high quality images from diffusion models. So let's summarize the training and sampling from diffusion models and what we've learned so far. In order to train diffusion models, the algorithm is extremely simple. You draw a batch of samples from your training data distribution. You uniformly sample from these time steps from one to capital T. You draw some random noise that has same dimension as your input data, and you use the parameterization trick to generate sample at time step T. You give the sample to your noise prediction network, and you train this noise prediction network to predict the noise that was used to generate that diffuse sample. And to train this, you just simply use squared L2 loss to train this noise prediction network. After training, if you like to draw samples from your diffusion model, you can use the reverse diffusion process to generate data. So we're going to start from last step, X capital T. We're going to draw samples from standard normal distribution. And then here we have a for loop that walks back in diffusion process, starting from capital T all the way to T equals to one. At every step, we just draw white noise, Z, from standard normal distribution. Here we're forming the mean of the noisy model. Remember, this is the parameterization we use for the noisy model. And then we add noise rescale with the standard deviation of the noisy model signal T to generate Xt minus one. And we can repeat this T times in order to generate X0. So very simple training and very simple generation process. So far we talked about training and sampling. So let's talk about the implementation details of off-to-form neural networks for the noisy model. In practice, most diffusion models use unit architecture to represent this noisy model. This unit architecture often has residual blocks. So here different rectangles represent residual blocks at different scales. And these residual blocks often have also self-attention layers in them. And some layers usually produce self-attention layers. Remember, this unit takes this diffused image, diffused pinot Xt, and it predicts the noise that was used in order to generate this diffuse image. So this maximum prediction would be trained to produce the predicted noise that was used to generate this Xt. This network is also conditioned on time. It's shared across different time steps. So it also takes time using some time embedding. This time embedding can be done using, for example, sinusoidal positional embeddings that are often used in transformers or random free features to represent this time embedding. This time embedding will be a vector that will be fed to a small, fully connected network. A network consists of a few full-connected layers to access some time representation. This time representation is usually fed to all the residual blocks. In order to combine this time embedding with all the residual blocks, you have a few options. For example, you can just take this time embedding and do arithmetic sum with all the spatial features in the residual blocks. Or you can use, for example, adaptive group normalization in order to do this, like to add time embedding into residual blocks. So I would encourage you to check this paper that discusses the potential traits between adaptive group normalization and spatial recognition. So far, we talked about forward process, reverse process, training, as well as the network's design for diffusion models. Let's also talk about some of these hyperparameters that we have in diffusion models, mostly beta t schedule, the variance of the forward process, and the variance used in the reverse process, sigma t squared. So one common assumption is that we can, in most papers, follow John Tonho at our NeurIPS 2020 paper, where they use just simply beta t's that are defined using just a linear function. Just these beta t's are from small values and they gradually go to some larger value through some linear schedule. And it's also common to assume that sigma t squared is just equal, set equal to beta t. This works also usually well in practice, especially when the number of diffusive steps is large. But you may ask me, how can I train these problems? Is there any way I can also train beta t and sigma t squared? So there are a couple of papers that discuss this. One of them is this paper by King W et al. at NeurIPS 2022. This paper introduces a new parameterization of diffusion models using a concept called signal-to-noise ratio. And they show how you can actually train this noise schedule using some training objective. So they actually propose a method for training these beta t values. There are also a couple of papers that discuss how you can train sigma t squared, the variance of the reverse process. This first paper here shows how you can use a variational bond that we use for training diffusion models to also train sigma t, the variance of the noise models. And this only paper here, Analytic DPM by Bao et al. in ICELR 2022. This paper actually got an outstanding paper award this year at ICELR and they showed how you can actually post-train it, as we're training diffusion models, how you can use this to compute the variance of the noising model analytically post-training. So far, we talked about how we can train and how we can also pick up these hyperparameter and diffusion models. Let's look at diffusion process and look into what happens to, for example, images in the forward process. We call that in order to sample from time step t, we can use this diffusion canon and we can use this parameterization trick to generate x t from input image x 0. Here, x t, in order to diffuse sample, in order to analyze what happens to the image, what we're going to do, we're going to use Fourier transform to convert x t to the frequency domain. So this f x t, when you think of just Fourier transform applied to x t, is just a representation of the image in the signal, in the frequency domain. And we know from Fourier transform that this x t can be just represented as Fourier transform of input image plus Fourier transform of the noise, summed together with some weights corresponds to the basis we use actually in this parameterization trick. This is just simple rules in Fourier transform. You should have in mind that most images actually in the frequency domain have a very high response for low frequency and they have very low response for high frequency content. And this is because most images are very smooth and in general, even if they're not like super smooth, when you apply Fourier transform to them, you see usually that most images have very high concentration in low frequency and their high frequency response is very low. This is very common in most images. So one thing you should also know that if you have a white noise or Gaussian noise and you apply Fourier transform on top of it, in the frequency domain, actually this Gaussian noise can be also represented as Gaussian noise in the frequency domain as well. So Fourier transform of Gaussian noise is itself Gaussian noise, which we can use now here for analysis. So remember for the small t's alpha bar t is almost one. So as a result, we actually did the perturbation we applied is very small in the frequency domain as well. So in frequency domain, because most of our input signal for input image is constant at the small t's and because alpha bar is almost one, we actually don't perturb the low frequency content of the image that much. And we mostly perturb when we kind of wash out this high frequency content of the image for small t's. And then for large t's, because alpha bar t, this coefficient here is almost zero. Right. So what happens is that you're not pushed on all the frequency content. So you also pushed on the low frequency response of the image. And you wash away all the kind of frequency content of the image. This basically shows that there is kind of a trade off in the forward process. In the forward process, what happens is that the high frequency content is perturbed faster than the low frequency content. So at small t's, most of the low frequency content is not perturbed. It's mostly that high frequency content that is being perturbed. But eventually at the end of process is a time when we also completely get rid of the low frequency content of the image. This is very important to also understand what happens in the reverse process. So there's this kind of trade off between content and detail. We can think of low frequency response as the main content of image and the high frequency response is just the detail in that generation. Because these diffusible kind of trade off between these in different steps. So you can think of when you're training a generative model, the reverse denoising model, for in the large teams, denoising model is becoming specialized at generating low frequency content of image. So it's mostly the coarse content of the image is being generated in large t's. In the small t's, then the noisy model is becoming specialized in generating the high frequency content of the image. So most of the low level details are generated in the low small t's. This is also why the weighting of training objective becomes important. So because you have a model that is shared in different time steps and this model is responsible for generating coarse content versus low level details, by rating this training objective, now we can kind of keep balance between how much we want to generate this coarse content that is visually very appealing usually versus how much we want to generate the high frequency details that usually we ignore. We cannot necessarily observe that. And the weighting plays a key role in keeping this trade off. So, so far I talked about diffusion models in general. Now let's talk about the connections between diffusion models and VAEs, especially hierarchical VAEs. In hierarchical VAEs, which one of the examples can be like NVAE, a work I did a few years ago. In hierarchical VAEs, we have this deterministic path. You can think of just a ResNet that generates data at x at the end. This is just a generative model. In hierarchical VAEs, we use sample from nodes and we inject to this deterministic path. And then we go to second group with sample from second group conditional to first group. And after generating this noise, we feed it to the deterministic path. And we keep doing this by just walking down in the hierarchical model. So diffusion models can be considered as hierarchical models as well, where these diffused steps are just latent variables in this hierarchical model. The conditional dependency is of course different. And here we're going to discuss like what are the main differences between diffusion models and hierarchical VAEs. One major difference is that the encoder in VAEs is often trained, whereas encoder, which is the forward diffusion in diffusion models, is fixed. We're not training the forward diffusion. We're just using a fixed diffusion process as encoder. That's one major difference. The second difference is that latent variables in hierarchical VAEs can have a different shape and different dimensionality compared to input images. Whereas in diffusion models, we assume that all the intermediate latent variables have the same dimension as input data. The third major difference is that in diffusion models, if you think of the noisy model as a generative model, this is actually shared across different steps. Whereas in hierarchical VAEs, we actually don't make an assumption. We don't share any component in this hierarchical structure, usually. And also in hierarchical VAEs, we usually train these models using the variational model. Whereas when we're training diffusion models, we're using some different reweighting of the variational model in order to drive the training objective of diffusion. So even though these two are related, they're not exactly the same. There are some trade-offs that occur when we are reweighting the variational model. So this brings me to the last slide. So in this part, I reviewed the noisy diffusion probabilistic models. I showed how these models are just simply trained by sampling from forward diffusion process and training a noisy model that simply predicts the noise that was used in order to generate diffuse samples. We discussed these models from different perspectives. We saw what happens to data as you go in there, like what happens to images as you go forward in the diffusion process. We also discussed what happens to data distribution in the forward process. We saw how data distribution becomes smoother and smoother in forward diffusion. But of course, like any other deep learning framework, the devil is in the details. The network architecture, objective reweighting, or even diffusion parameters play a key role in getting good high quality results with diffusion models. So if you're interested in knowing more about important design decisions that actually play a role in getting good diffusion models, I would encourage you to check this paper by our colleagues. It's called elucidating the design space of diffusion-based models by Karas et al. And this paper discusses important design decisions and how they play a role in getting good diffusion models. So with that in mind, I'd like to pass the mic and meet you to my dear friend and colleague Karsten to talk about score-based genitive modeling with differential equations. Hello everyone, I'm Karsten and I will now talk about score-based genitive modeling with differential equations. So to get started, let us actually consider the diffusion process that Arash already introduced in his part. This diffusion process is defined with this Gaussian transition kernel of this form. But now, let us consider the limit of many, many small steps and each step being very, very tiny. So how does sampling from this diffusion process and practice look like? To sample from this Gaussian transition kernel, we can just do essentially the parameterization trick and we take the xt-1, we scale it down by this 1-beta t square root term, and we add a little bit of noise from the standard normal distribution, scaled by this square root beta t term. So beta t, we can actually interpret it as a step size essentially. So if beta t is zero, nothing happens, this term drops out, also no rescaling of xt-1 happens. So let's make this a little bit more explicit and write beta t as this delta t times this function beta of t. So now, delta t is explicitly our step size and beta of t is now this time dependent function that allows us to have different step sizes and along the diffusion process t. And now in this limit of many, many small tiny steps, it is delta t that goes to zero. But if delta t goes towards zero, or tiny, we can actually Taylor expand this square root expression here and obtain this equation at the bottom. So I just copied that over here. And it turns out that this equation has a particular form. We can interpret this as some iterative update, like the new xt is given by the old xt plus some term that depends on xt itself. So this is just a small correction and some noise added. Turns out that this iterative update rule corresponds to a certain solution or a certain discretization of a stochastic differential equation. And in particular, this stochastic differential equation. If I wanted to iteratively numerically solve this stochastic differential equation, for instance with an Euler-Moyer-Rama solver, then this is exactly the iterative scheme I would end up with. But let us not get ahead of ourselves. I'm not sure if everybody who's listening here is an expert in differential equations. So let us do a one slide crash course in differential equations. And let us start with ordinary differential equations, which are a little bit simpler than stochastic ones. Here is an ordinary differential equation that can be written in that form. X is now a state that we're interested in. This could, for example, the value of a pixel in an image. And t is some continuous time variable that captures the time along which this state changes or evolves. Ultimately one is often interested in the evolution of this state x or this pixel value x of t. However, that is not what we're given. An ordinary differential equation, what we're given is an expression for the time derivative for dx to dt in the form of this function f. So this could, for instance, now be a neural network. So what does this mean? This f essentially describes not x itself, but the change of x. So if you now look here in this graph at the bottom. So for point x for a given time t, this f of x now describes the change. So in other words, we could look. So this f essentially corresponds to an arrow in this graph. And if I now wanted to get x of t, I would just follow the arrows in this thing here. So basically, you just have to integrate up this differential equation following the arrows to get my final expression x of t. Yeah, so that's what I would have to do. However, in practice, this f is often like highly complex nonlinear function, for instance, like a neural network and solving this integral here analytically and following these field lines exactly is often not possible. So instead, one can solve this whole thing iteratively numerically in a stepwise fashion. So in that case, when we add some point x, we evaluate our neural network or our nonlinear function f or function f, yeah, at this x and time t. And then we do like a small linear step in this direction and access to our old state x. And now we just continue doing that. Again, evaluate f and again update and so on and so forth. So we have an approximation essentially to this analytical solution. So, yeah, and now there are also stochastic differential equations. This is a little bit more complex. The stochastic differential equations now have an additional term, this term sigma of x and t times this term, omega. So what is all this. First of all, omega. This is called a wiener process. And what this is in practice is really just Gaussian white noise. What this means is that our dx of t, our ODE equation now has this additional term, which this corresponds to noise injection. And this noise is scaled by this standard deviation term essentially sigma of x t. And this has a name, this is diffusion coefficient. And in that context, also the other term, this term here is called the drift coefficient. It is noteworthy that these equations are sometimes written like in this explicit form with derivative here. Sometimes also like, like here this dt is written on the other side, and just becomes a bit of a special expression for the wiener process. Sometimes this differential equations or stochastic differential equations here also written like this, but this essentially means the same thing for the sake of this talk. So, yeah, importantly, keep in mind this omega of t is essentially a Gaussian random variable that is drawn independently for each t. Looks like this. So how does this now look like if I want to solve this stochastic differential equation. So for example, numerically. So it's a little bit similar to the iterative solution we had here. So if I'm given a state, I first updated corresponding to the drift coefficient, I evaluate that. So update a little bit with that direction. But then I also evaluate this diffusion coefficient and add a little bit of noise that is proportional, the strength of the noise is proportional to the diffusion coefficient. And additionally, also to the, to the timestamp. And now when I do this, so each time I do this, I make more different noise variables. So this means there is not a unique solution, like there was in the ordinary differential equation case, but there is a lot of noise injected. So if I do this multiple times, I may get slightly different trajectories. So overall these trajectories still approximately follow this deterministic f function, but we have additional noise injection. So this is how it may look like. You may ask, is there now also like an analytical framework to instead, you know, describe this analytically like it was for the ordinary differential equation case. So there is, but this is a little bit more involved, because now we are not talking about one deterministic solution that we need to describe. Rather, given one state, at the beginning, we now have a probability distribution of a possible state where we could land. And the evolution of this probability distribution, that is described by the Fokker-Planck equation. And that is beyond the scope of this tutorial here. Anyway, I think now you should also have some intuitions for not only ordinary differential equations, but also stochastic differential equations. So now we can go back to our stochastic differential equation that we had here, and that actually describes the forward diffusion process of diffusion models. So this is again just a copy over the equation from the last slide here at the bottom. And this is now a visualization of how this whole thing looks like in practice, more or less. So let's go through that one by one. On the left hand side, we have some distribution Q of x0, this might be like defined through an empirical data distribution, or here we have this one dimensional toy distribution. In a more realistic setting, this may represent a distribution of images, like images of these cats here. So now if we simulate this stochastic differential equation forward, we get these green trajectories and they evolve towards this standard normal prior distribution. And here the images become progressively noisier and noisier as we do this. We can also look at the form of this equation and it's kind of intuitive. We see that this update, or in the x, this update direction is actually proportional to the negative of the state x square. And so this means if we have a large pixel value, for instance, this will pull us back towards zero. So direction is always negative direction corresponding to our x. On the other hand, while all our states are being pulled towards zero, as I've just explained, at the same time we're also injecting noise. So this makes it intuitive that after a while of simulating this whole process, we end up with this distribution. Or like it means that every single point basically completely inverts itself, converts itself to just plain noise. Where, yeah, with mean zero and the certain variance. How does this look like in practice? Here's another animation of that. Note that throughout this talk we will make a lot of use of this image of this cat, a washes cat, a peanut. We do hope that it will become a little bit famous after this talk, but let's see how that goes. All right. So, yeah, we have this forward diffusion FDE, that's a diffusion term, one of them pulls towards the mode of the distribution, the other one injects noise. It may be worth mentioning that in the diffusion model, which are also other differential equations have been used to define other types of diffusion processes. Often just take a more general form like this. Yeah, I don't want to go into too much detail and in this talk we will stick to this equation for simplicity, but all the concepts that we described in this tutorial also hold for other types of FDEs. The only thing that is important is that these drift kernels and diffusion terms here, these are basically linear function of x. Otherwise, we couldn't solve for the probability distributions here, which reminds me the background, this reddish background in these, in these pictures here, these animations, this actually defines the marginal probability distribution of the diffused data, which is a multimodal and then it becomes unimodal. So great. We have not talked about the forward diffusion process, but what about the reverse direction, so which is necessary for generation. Can we also have like some differential equations that describe that? It turns out yes. There is this result described in Anderson 1982 and then used in Yang-Songs IKEA paper last year, which says that if there is a forward differential equation of this form forward FDE, then there is a corresponding reverse stochastic differential equation that runs in the other direction, but tracks the exact same probability distribution. So we like the reverse direction, which generates data from noise and not noise from data. So, and since we've got generative diffusion FDE, it looks like this. It again has a drift term and a diffusion term. It's a diffusion term and the drift term look somewhat similar. So the diffusion term is the same thing. So this is a ovena process. But again, inject noise. This drift term also has this same minus x minus one half beta t of x term. But then there was this additional term here. This is right term and that is very important. So this is the gradient of the logarithm of the marginal diffused density, probability density of the data. And this is known as the score function, actually. So this means, if you now have access to this object here, right, this score function, we could do data generation from random noise by just sampling or simulating this reverse diffusion FDE. So this in practice and what looked like this, like I said, this reverse diffusion FDE, it's really a diffusion process running in the other direction. So yeah, simulating this generative modeling, essentially. So, are we done? So we can simulate this and generate data? Well, not quite. The question is, how do we actually get this score function? We may have the naive idea, why not learn a neural network for this score function to approximate it, and then we can take this neural network, we call this neural network S theta, this parameter theta, and then we can take this network, insert it in our reverse diffusion FDE. And yes, then we can simulate it and generate data. Something we could do, for instance, is draw a diffusion time along this diffusion process. Now take that data, diffused data, from this point in the diffusion process, sample this Q of t of x t. And now we take the neural network that takes us as input, maybe we also additionally give it the time, and train this, maybe with some simple squared L2 term, which we minimize. We train it to predict this score of this diffused data, this marginal score of diffused data. Well, great idea, but unfortunately it doesn't work, because the score of this marginal diffused data is not tracked a bit, or more explicitly this diffused data density itself, Q t of x t is not tracked a bit. We don't have an analytic expression for that, which makes intuitively sense, because, well, if we had, we could just put N t equals zero and get our data distribution, but this is what we are interested in modeling in the first place. So, too bad. But what we can do is something different, which is now known as denoising score matching. So what we had on the previous slide was just general score matching. What we can do is, instead of considering this marginal distribution Q of t given x, which corresponds to the full diffused density, let us consider like individual data points x zero and diffuse those. So instead, we consider the conditional density Q of t of x t given x zero. That distribution actually is tracked a bit. And for the variance preserving stochastic differential equation, which is precisely this SDE that defines our forward diffusion process. For that case, this conditional density Q of t of x t given one particular data point x zero, it has this expression, this form. So it is this normal distribution where the mean is given by the initial point x zero that is now simply scaled down by some gamma of t, which is this form. So this is a function that starts at one and decays towards zero. And then we also have some variance, which is the other way around, which starts at zero and grows towards one. So now we can define our denoising score matching objective. So again, we have a redraw diffusion time t, we have an expectation over those. Then we draw a data sample x zero, one particular sample. Overall, we again have an expectation. Then we diffuse that particular data sample. So towards some noisy sample x t. Now we give that to our network. This noisy sample is time t. And now we train it to predict the score of this one particular diffused data sample x zero. And yeah, now this is tractable. This is just a normal distribution. So we can take the logarithm of this density, the expression for the density and also calculate the gradient. Great. So now there is one very beautiful result. So after this expectation over the data, when we consider that, it turns out that this neural network then will still learn to approximate the score of the marginal data, of the marginal diffused data distribution, which is exactly what we need. And this sort of makes intuitively sense. Because this x t that we're feeding to the neural network, this could correspond to, this is noisy, right? Noisy data. So this could correspond to many possible different x zeros and many different possible like grand truth scores that we will be blessed with. So in practice, this neural network has to kind of average over those. And it turns out that, yeah, after this averaging, considering the expectation, this neural network will still model the marginal as the score of the marginal diffused data distribution. And this is exactly what we need. What we need in our model, and more specifically in the reverse generative SDE for generation. So this is great. So yeah, in practice, diffusion modeling basically boils down to learning this score function. Let us now talk about a few implementation details here and what people do in practice because that's also crucial. This is again just a bit of the denoising score matching formula. So how do we actually sample this diffused data points? So this is really just the parameter sampling again from this normal distribution here. So we have gamma times our input x zero, and then we add noise to this that is scaled by the standard deviation sigma t. Now let us actually explicitly write down the score function, how it now looks like. So it's a gradient, the logarithm of this conditional distribution, SDE given x zero. So this is a normal distribution, so this is basically some exponential times some stuff. So we have the logarithm and the exponential drop out. There's also, by the way, a normalization constant, but this does not depend on x, so it's not important for the derivative. Anyway, so you will write this term. So now we can take the gradient of this term here, we write that. Now here for x t, this is like the diffused data point that we sampled. We can actually insert this expression here. Then we get that. But now it turns out all these terms here, they cancel out this gamma t x zeros and also one of those sigmas. And what we left with is that. So this is interesting, because this means that the score function. It's basically just the noise values that we introduced during the reparmeterized sampling of our diffused data, like minus that noise value and scaled with inverse standard deviation from diffusion. But still, this is cool. So this maybe also suggests us how we should choose our neural network and how we should parameterize this. More specifically, for instance, we can take this new network and define it by like some, yeah, some other neural networks times minus one and divided by the standard deviation, which is inspired by this. So now if we insert both of this, the expression for the ground to a score, which is really just noise value, and also this neural network parameterization. What we're left with is this objective here at the bottom. And notice interesting. So this means if I choose this parameterization our neural network epsilon theta now is basically tasked with predicting noise values epsilon, which are really just the noise values that were used to perturb our data. So, this also makes it kind of intuitive why this is called denoising scratching, because if our new network can be nice can predict those noise values that were used for perturbation, then yeah, we can be nice and reconstruct original data point next to. So this is another implementation detail here. So, I have kind of arbitrarily motivated that we can use this script to turn to perform denoising scratching and to regress this function. Maybe we want to give different weights to this L2 term to this L2 loss for different points along the diffusion process. Keep in mind that this is one neural network that just as input gets a noisy state entity, it's the same neural network for all the t's along the diffusion process. So, maybe we want to specialize the network, a little bit more for large times along the diffusion process, or small times, something like this, and give like different weights to this objective for different times along the diffusion process. And this is a loss weighting lambda t. So we introduce this loss weighting lambda t that it does this. And it turns out that different loss weightings trade off between like models is different good perceptual quality, like the images, and the pretty that we can generate the set and sharp versus no high log likelihood. So for instance, if we choose lambda of t to be exactly this variance of the forward diffusion process here to cancel out the variance in the denominator, then this is an objective that is actually that leads to good high quality perceptual quality results. On the other hand, if we choose for lambda, for instance, beta of t, which is hyper parameter of forward diffusion process. Then, this whole objective that we have that corresponds to training our model towards maximum likelihood. More specifically, it's kind of a negative elbow. So this is interesting, and it turns out that, yeah, this is exactly the same objectives that we do right with a variation approach and part one presented by us. And, yeah, so there are this means that there are some deep connections between this variation of the revision and actually the excom matching and noisy scratching in particular. I would also like to point out that there are like much more sophisticated model parameterizations and less weighting possible. I would in particular refer you to this recent paper by Carol covers at all, who discusses in quite some detail. There's another implementation detail I would like to talk about. So, we know that this variance sigma t squared, and the office forward diffusion process for like using individual data points that actually goes to zero, as t goes to zero. But this means that if I sample the t here, close to zero, then this loss might be heavily amplified when sampling. Yeah, t close to zero. And this is also the case when we do not choose lambda or waiting function to cancel out the sigma spread. So for some of these for these reasons we sometimes see some tricks and implementations, where we train the small time cut off so we prevent sampling teeth that are extremely close to zero. And until way how this can be fixed. Like I said, this is especially relevant training models towards high block likelihood versus lambda t function maybe something like beta t and sigma squared is not canceled out. In that case, we can perform important sampling with respect to this. Yeah, weight of loss. So we have the oversampled teeth, close to zero. And, yeah. So the objective thing looks like this. So we oversampled small teeth, according to the important sampling distribution that has most of its weight, or small teeth. And then we weigh down the competition of source to the overall loss, and this one over our teeth and points in distribution. I don't want to go into too much detail but this is a technique you see in several papers. So here's a visualization of what happens. So this is not a lot of value, the wet is the last value without sampling here. So, yeah, the first sample, t, close to zero than I have this heavily amplified loss values, but with important sampling the blue line, the variance is significantly reduced. Before moving on, it makes sense to briefly recapitulate what we have been doing so far. So, we have been introducing this diffusion modeling framework based on continuous times now, in contrast to the first part, presented by our rush, where each diffusion step had a finite size, and we overall had a finite number of discrete forward diffusion process steps, and also the noisy steps. In this section, we have continued, and so that a continuous time. This allowed us to introduce this differential equation framework, and also to make these connections to score matching, but it is important to keep in mind that we are still describing the same types of diffusion models in this section, we're just using different tools and a different framework. So, let's now move on to the next slide, which is the first sample. So, we want to realize, after all, we obtained the same objectives, as we have seen on the last few slides. With that in mind, let us now move on, and we will talk now about the probability flow ordinary differential equation. So, let's now look at the difference between SDE, again, that one. So object you have already seen so far with this generative reverse diffusion SDE, we can when sampling random noise from this standard normal prior distribution, we can generate data, and more specifically, we basically can sample data all along the diffused data distribution. The reddish curves, reddish contrast. This is a ordinary differential equation called the probability flow, or D, that is a distribution equivalent to this reverse generative fusion. It will become clear in a minute. What exactly I mean with in distribution equivalent. So, let us first have a look at this ODE itself. It is written down here. In contrast to the generative diffusion SDE, it doesn't have some noise term, and also the score function term, which people then later learn with the neural network, it doesn't have this factor two in front anymore. So, what do I mean with in distribution equivalent. So, I sample many initial noise values from this standard normal distribution at initialization, when I want to generate the data, then simulating all these samples on backwards. And then we will generate probability flow D towards the data. By doing that, we will sample from the exact same probability distribution, like with the generative diffusion SDE, with the only difference that we don't have this noise anymore. So how does look like most specifically, we can see this on that slide. And again we have the probability flow ODE, just that we now have inserted the learned score function as a pattern for the score function. And now, these trajectories defined by this ODE they look like this. So, we see that when we sample from this standard normal prior distribution on the right, these trajectories they will all flow into the modes of the data distribution. And we see this also at these bluish lines here at the background. Yeah, so the probability, quite literally flows into the modes of the data distribution. This is why it's called the probability flow ODE. Here, we have an animation, how it looks like. And I think at this point it should really become clear what I meant with, they are the same and distribution, and they sample the same distribution. And on the left side we have the SDE that we have said I have introduced earlier already as the framework. And yeah, we see that these trajectories are zigzagging I have this noise injection, but I'm still landing at the most of the data distribution. And then we have the ODE formulation, I know have these pretty like not exactly straight but more deterministic trajectories that still land in the modes of the data distribution. When I initialize them randomly from this prior distribution. So each individual trajectory on the right, it's deterministic while here this is stochastic. So this probability flow ordinary differential equation, this is actually an instance of a new ordinary differential equations, which, a while ago, generated a lot of attention in the literature. More specifically, we can even see this as a continuous normalizing flow. So, why should you care, why should you use this probability flow ODE framework? It turns out that this ordinary differential equation framework that allows the use of advanced ordinary differential equation solvers. It is somewhat easier to work with ordinary differential equations, then with stochastic differential equation. And so really is a broad literature on how to quickly and very efficiently solve ordinary differential equation. So we can build on top of this literature here. But there are more advantages. This ordinary differential equation, I can run this in both directions, I can run it as generation, where I go to from the right to the left, where I sample noise from a prior distribution and then go to the left to generate data. But similarly, given the data point. I can also wonder probability flow ODE in the other direction, and encode this data point in the latent space of this diffusion model prior space. So this is interesting and yeah this allows for interesting applications, for instance, for semantic image interpolation. And to make clear what I mean with that. Let's look at this slide here. What I'm doing here is, I have drawn two noise values, or, yeah, let's look first at the lower left. So here we are drawing two noise values in the latent space of the diffusion model in this noise space. And now I can linear interpolate these noise values in this space. However, the model was trained in such a way that every sample, and those is noise distribution. So also, every sample, along this linear interpolation path between those noise values, decodes to a coherent realistic image. So when I then interpolate means that this, this results in continuous semantic meaningful changes in the data space, right. And keep in mind we could not just interpolate directly linearly and pixel space, this would not be meaningful, but we can do that in noise space, and then obtain semantically meaningful interpolations in the pixel space like this. And because this is so complex right under the hood. This means that we will sometimes have some jumps between notes and such like this. And we also see this in this animation here. So in this animation at the top. Yeah, we have been doing many of such interpolations one after another. And, yeah, sometimes you see like little jumps. This basically corresponds to that. And because this is only possible due to this deterministic encoding and decoding path with the probability. I think it's clear that you couldn't do this so easily with as to plastic projectors. All right. So there is another advantage of the probability flow ordinary differential equation. We can also use it for block likelihood computation, as in continuous normalizing flows. More specifically, we can take a given image, or a given data sample. For instance, this image of our she's cat peanut. Now we can take peanut and encode peanut in the latent space of our diffusion model. Then we get this state, x t. Now we can calculate the probability of peanuts encoding another prior distribution of our diffusion model. And additionally, we take into account this using this instantaneous change of variables formula, kind of the distortion of the ODS of volume change along the ODS trajectory. So then the ability of our data sample, in our case, the image of peanut is then given by that expression. So, we're really just using the tricks from the continuous normalizing flow literature here. What all this means is actually set in that probability flow or the formulation diffusion models can also be considered as continuous normalizing flows. However, in contrast to continuous normalizing flows. We train diffusion models with score matching continuous normalizing flows themselves are usually trained directly with this objective to maximize the likelihood of the data. However, training was such as this objective directly is actually a hard task, because for each train iteration. I have to simulate the whole trajectory here and back propagated through it. On the other hand, this diffusion model training relies on score matching. And as we have seen earlier, score matching works quite differently in score matching, you basically have like we can train for all these different times along the diffusion process separately. This leads to a much more scalable and robust learning objective. And yes, this makes the fusion models very scalable. In contrast to these normalizing flows, I would argue. So I have not talked a lot about these differential equations, derive these derive them and so on and so forth. However, how should we actually solve these SDS and ODS in practice. And we have already seen that we can probably not solve this analytically, because these SDS and ODS are defined with very complex nonlinear functions, namely, these neural networks that approximated the score function. So let us look at that. So let's start with the generative diffusion SDE. So the most naive way to do this is to use Euler-Mariyama sampling. We have already briefly talked about Euler-Mariyama sampling in this earlier one slide crash course on differential equations. What we do in that case is, we simply evaluate our function here for different for our state t and x, then we propagate for like a small time step delta t. And we additionally add a little bit of noise, which is also scaled by the time step. And yeah, so we do this and iteratively one after another, given a new step we evaluate again, again, at a little bit of noise, and so on and so forth. By the way, as a small comment here, and you may wonder about the sign flip from here to here, this is because our DT is actually negative, because we're running from like large time values to small time values and delta t here is not supposed to be like an absolute step size, so it's positive. It turns out, also this and chest or sampling, and that I was talked about in the first part of our tutorial, and more specifically the way he showed us how we can, we can sample from these discrete time diffusion models, and this, this can actually be also considered the generative SDE center with this particular discretization used in that part. So now let's look at the probability flow, or the. How can we generate that are using that. Again, we could basically use Euler's method, which is analogous to the other my aroma approach and just know without this noise injection. Again, we will just iteratively evaluate our network, and the function essentially do a small step linear step for small time delta t. We evaluate and continue doing that. However, this is usually, I think nobody really does this in practice. In practice we can hear, as I mentioned earlier, really build on the advanced ordinary differential equation literature and use much better solvers and much better methods and higher order methods in particular. So what we see for instance is the use of one could turn methods of linear multi stepping methods exponential integrators. In fact, there was a lot of literature in that direction. Yeah, like I just said, adaptive step size when you put on methods have been used also causes stochastic differential equation actually adaptive steps as higher order methods have been used. We permit tries to use the ODE has been proposed that also accelerates something. Yeah, like I mentioned linear multi stepping has been used exponential integrators have been used recently points message has been used was very successfully is another higher order solver. So yeah, there's a lot of literature in that direction. And the main reason is that one drawback of fusion models is that something from them can be slow. In contrast to like sampling common generative adversarial network or variation auto encoder and such methods for instance, sampling from a diffusion model requires many many function calls on what specifically neural network evaluations, in our case, because during each step of this iterative denoising, we have to call this neural network again so we often have to call it many many times. And yeah, so this is why we want to use efficient solvers so that we can reduce this number of new network evaluations that we have to use. So now I have talked about how you can use, how you can solve the STDs and the ODE and practice. But what should you use. Should you actually rather build on the SC or the ODE framework when you want to sample from the model. So to shine some light into that, let us look at the generative diffusion, st a little bit closer. So it's like that. But now we can actually decompose this into two terms. Right, so this is just from here to here. And it turns out the first term is really just the probability flow ODE that we have seen already, which itself can be used for deterministic data generation, like you're on the right. But then there was this additional term, this larger than diffusion, as the term. So this could basically corresponds to them, noise injection, and it has some nice injection here. So what, what do these terms do. So, this probability flow ODE term is essentially responsible for you're pushing us from the right to the left here. And this large of in diffusion, what it basically does is for each individual t, it actively pushes us towards correct diffuse data distribution. But because of this. So when I do, I was doing my soul, during my simulation going from the right to left you're going from noise to data. If I have ever said, then this larger than diffusion, st can help us to correct this was an active bring us back to the right data man called back to the fused data distribution. So this, yeah, this is an advantage can do some sort of error correction. On the other hand, it's, it's often slower because this term itself requires somewhat fine discretization during the solve. Yeah. So now let's look at the probability flow ODE. So in that case, we do not have this FTE term. And this knowledge of in term here. However, we can now leverage this really fast ODE solvers. And so this is good when we target very fast sampling. On the other hand, there is no stochastic error correction going on here. And because of this what we see in practice is that this is sometimes slightly lower performing than the stochastic sampling. When we just look at the quality of the samples. So to summarize what we see is, if we're not concerned about our budget of like new and network evaluations and we're willing to do like very a lot of steps, then this st framework can be very useful. But if you want to go as fast as possible, then probably the ODE from with this better where we then can leverage this really fast solvers. It is also worth mentioning that we can do things in between, where we have only like this much of in diffusion FTE active a little bit down. We can also you know like kind of solve for the first half using stochastic sampling and then afterwards switch to the ODE advanced methods are possible. I would like to refer you to this paper here which discusses some of these things in quite some detail. Next, I would like to talk about a connection between diffusion models and energy based models. So, what are energy based models? Energy based models are defined like this in an energy based model. So probability distribution that we want to model is theta of x. Defined to an exponential to the power of minus a scalar energy function, which is now the function of the data. And then the same as normalized by a normalization constant, make sure it's a well defined probability distribution. This normalization constant is also called sometimes called the partition function. Furthermore, in this case, I have added the time variable t, because this energy based model is now supposed to represent the diffuse data distributions for different t's in our case. When we want to sample an energy based model, we usually do that via logarithm dynamics, which is, we have seen logarithm dynamics already. Basically, this is very closely connected to these stochastic differential equations we have already discussed. So, to do this logarithm dynamics sampling, we basically require the gradient of this scalar energy function. And then we also iteratively update our sample with that. And we also have like an additional noise to an eta and some step size of learning rate at a time. The important part to realize is when we do, when we use this energy based models is that in practice, even though we are learning the scalar energy function. We only require the gradient of this energy function or more specifically the negative gradient of this energy function for sampling the model at the end, we do not require the energy function itself, nor do we require the partition function that set up. By the way, this is that it's implicitly defined through this energy itself that we're learning. And it turns out that in diffusion models, what we're basically learning is the energy gradient for all these diffuse data distributions directly. We are not learning energies but basically energy gradient. And what I want to add is that, because it's EDM sphere directly learn these energies, and we have to tell and the probability is an expression for the probability distribution, but also taking into account this partition function, because of this training energy based models can be actually really complex. This often requires an advanced map of chain one to color methods, which can be very difficult to deal with and. I'm not that scalable. But at the time diffusion models we kind of circumvented and we only directly learn these energy gradients. Can we somehow show that and derive that maybe. Let's recall again that in diffusion models, our neural network. Basically, we are trying to approximate our model more generally, we're trying to approximate this score function of the diffuse data distributions QT of x. Now let us suppose that our model is parameterized such that the diffuse data distributions QT are given by this energy based model here. So, now, let us insert this piece that I here, and yet through the map. We apply the logarithm both here, both on e to the minus the scalar energy function and also the denominator. However, this term drops out because the partition function does not depend on the state x. And what we are left with is just a negative gradient of the energy. But what does this mean so this means that this neural network s that we usually have in diffusion models to to model this function. It means that it essentially learns the negative energy gradient of. Yeah, so the this energy model based model that would describe the diffused data distribution. But yeah, once again, fusion models kind of circumvent these complications and directly model the energy gradients and avoid modeling this partition function explicitly for instance, which leads to some of these difficulties that we have in classical energy based model training. And also, these different noise levels that we have an infusion models. This is actually analogous to a new sampling and energy based models. I would like to talk about one more thing about diffusion models, which is unique identity ability. It turns out that the do not using model that we're learning in this diffusion models that is supposed to approximate the score function of the diffused data to your XP. This noisy model is a principle, you need to be determined by the data that we're given and the forward diffusion process. And not only the score model so and by learning this from all also these data and coding that we obtained by using the probability flow of the deterministically encode data and the length space. All this is uniquely determined by the data and the forward diffusion. What this means is that even if we use different neural network architectures for us, and different network initialization. We should, at the end, recover identical model outputs like identical score function outputs and data and holdings in the lane space, assuming we have sufficient training data model capacity and optimization accuracy. And this is in contrast, for instance to generate adversarial networks of variation auto encoded, which do not have this property. Because these models, depending on what kind of architecture, we use and what kind of initializations we use we will always obtain like somewhat different models and yet different data and holdings and so on. This is a unique property about these fusion models. Here's an example. What we are seeing here. So first 100 dimension of the latent code obtained from a random site for 10 image that was encoded in the lane space of the diffusion model using this probability flow, or the approach. We did this. And then, more specifically, song and I did this with two different models and model architectures that were separate each train. However, both of these and codings distributions here, as we see, they are almost the same they're almost identical. Even though these were different architectures. With that, I would like to come to a conclusion and briefly summarize. So in this part of this talk, I have introduced you to this continuous time diffusion framework. In contrast to what I was talking about in step one, we do not have finite size, the noisy and diffusion steps anymore and only a finite number of models. Rather, we consider continuous perturbations, the continuous forward diffusion process and then also a continuous generative process based on differential equations, and to for to train these models we make connections to explore matching, most specifically denoiting for matching. Now, maybe this appeared somewhat complex and mathematically involved. However, why should so therefore why should you use this differential equation and continuous time frame. It really has unique advantages, as I've shown, hopefully and fully I can convince you during this part of the talk. Most importantly, this allows us to leverage this broad existing literature on advanced and fast as he and od so that's when sampling from the model. This can help us to really accelerate sampling from diffusion models, which is very crucial, because they can be slow. Furthermore, in particular this probability flow ODE is very useful, because it allows us to also perform like this deterministic data encoding, and it also allows us to do like local IQ estimation like in continuous normalizing flows, and so on. Additionally, this is overall a fairly clean mathematical framework based on the diffusion processes is gone and so on. And this allowed us to use these connections to new ordinary differential equations to continuous normalizing flows and to energy based models, which I think provides a lot of insights into diffusion modeling. And with that, I would like to conclude my part and take them. Yeah, take the mic mic to which you will not talk about advanced techniques accelerated sampling conditional generation, and beyond. Thank you very much. Hi everyone, I'm Richie from Google brain team. So let's continue our study on diffusion models. So in the third part we're gonna discuss several advanced techniques of diffusion models which corresponds to accelerate the sampling a conditional generation, and beyond. So here's an outline of what we're going to cover in this part, basically want to address two important questions of diffusion models with advanced techniques. The first one is how to accelerate the sampling process of diffusion models. We're gonna tackle this question from three aspects, advanced forward process, advanced reverse process and advanced modeling, including hybrid models and model to simulation. The second question is how to do a high resolution optionally conditional generation. And I will talk about several important techniques to make this happen, especially the general conditional diffusion modeling framework, the classifier, classifier free guidance, as well as cascaded generation pipeline. So let's start from the first question so how to accelerate the sampling process of diffusion models. To see why this question is important. Let's consider what makes a good generation model. So in principle we want a good generation model to enjoy the following three properties. First, it should be fast to sample from this generation model. Second, we would expect the general model capture most of the major modes of the data distribution, or in other words, they should have adequate sample diversity. And third, of course we want the general model to give us high quality or high fidelity samples. However, there is this a generative learning for existing generative model frameworks, for example, for generating with several networks, they are usually fast to sample from, and they can give us high quality samples. However, because of this discriminative learning framework, there is a decent chance that GaNS may miss certain modes of the data distribution. And the other type of generative models are these likelihood based models like variational autoencoders or normalizing flows. So those models are usually optimized by maximizing likelihood or maximizing a variance of likelihood, for example, the evidence lower bound. So usually this type of models are good at sampling, and they are able to capture certain modes of the data distribution because of this maximum likelihood learning framework. However, usually they lead to subpar sample quality. And on the other hand, the diffusion models are good at both coverage because they are also optimizing the evidence lower bound of log-likeability, and they are able to generate high quality samples. However, the sampling of diffusion models is pretty slow, which usually requires a lot of time to generate. So if we can find techniques to accelerate the sampling process of diffusion models, we will get a generative model framework, which enjoys all those three great properties. That is, we can tackle the trilemma of this generative learning framework. So how to do that? So before diving into details, I would like to recap the general formulation of diffusion models. So for diffusion model, they usually define a simple forward process which slowly maps data to noise by repeatedly adding noise to the images. And then a reverse process is defined to map data, sorry, to map noise back to data. And this is where the diffusion model is defined and trained. And in terms of the diffusion model, it is usually prioritized by a unit architecture, which takes the noisy inputs at certain time step. And then it tries to predict the clean samples, or it tries to predict the noise added to this noisy inputs. So if we think about accelerating sampling, there are some naive methods that immediately come to our mind. For example, in training, we can reduce the number of diffusion time steps. Or in sampling, we sample every k time step instead of going over the whole reverse process. However, those naive acceleration methods will lead to immediate voice performance of diffusion models in terms of both the sample quality as well as likelihood estimations. So we really need something clever, cleverer than those naive methods. And more precisely, we want to ask, given a limited number of functional calls, which are usually much less than thousands, so how to improve the performance of diffusion models. And as a side note, although the following techniques I'm going to discuss take this accelerated sampling as a main motivation, they definitely provide more insights and contributions to diffusion models, as we will see soon. So we will answer this question from three aspects. The first one is advanced forward process, and second is advanced reverse process, and then lastly, the advanced diffusion models. So first, let's take a look at some advanced forward process. So recall that the original forward process defines a Markov process, where we start from this x0, it is the clean sample, and we gradually add noise until it goes to this x big T corresponding to white noise signal. And then the QxT, D minus T minus one is simply a Gaussian distribution. And this beta T defines the noise schedule, and they are hyperparameters that are predefined before training the models. So we are interested in the following questions. So first, does this forward process or noise schedule have to be predefined? And does it have to be a Markovian process? And lastly, is there any faster mixing diffusion process? So faster mixing is an important concept in Markov chain Monte Carlo, as we will discuss later. For the first work I would like to discuss here is this variational diffusion models. It basically enables us to learn the parameters in this forward process, together with the rest of parameters in the diffusion models. That is, we can really learn the forward process. So more specifically, given the forward process defined by QxT, P max 0, it follows a Gaussian distribution with square root alpha T bar x0 as the mean and one minus alpha T bar as the variance. So this is the formulation we have learned in part one. And this work proposed to directly parametrize the variance one minus alpha T bar through a learnable function gamma eta. And this function is decorated by a sigmoid function to ensure that the variance is within the range from zero to one. And this gamma eta t is further parametrized by a monotonic multilayer perceptron by using strictly positive ways and monotonic activations, for example sigmoid activations. And recall that in part one, we have learned that this diffusion models are directly connected to hierarchical variational autoencoders in the sense that diffusion models can be considered as a hierarchical variational encoders, but with fixed encoder. And this model is named as variational diffusion models because it is even more similar to hierarchical variational encoders because we are optimizing the parameters in the encoder together with the parameters in the decoder. And to optimize the parameters of the forward process, this paper further derived new parametrization of the training objectives in the Boolean sense. So basically, they have shown that optimizing the variational upper bound of the diffusion models can be simplified to the following training objectives. So note that this gamma eta participates in this weighting of like different L2 norm of different time steps. And this is the training objective they derived for the discrete time setting. And they have shown that by learning this noise schedule, it actually improves the likelihood estimation of diffusion models a lot, especially when we assume that there are fewer diffusion time steps. And recall that in the second part, we learned that the diffusion models can be interpreted from the perspective of stochastic differential equation, and we learn the connection between diffusion models and denoting score matching. So that gives us a hint or knowledge that the diffusion models can also be defined in a continuous time setting. And in this paper, they explicitly derive this variational upper bound in the continuous time setting with this gamma eta notation. Basically, they show that when we let this big T goes to infinity, meaning like we have infinity amount of diffusion time steps, this corresponds to a continuous time setting, and then the variational upper bound can be derived in the following formulation. So here, the only difference is that the weighting term of different L2 terms, L2 distance at different time steps equals to the derivative of this gamma eta T function over time T. And more interestingly, this paper shows that if we define the signal to noise ratio equals to alpha T bar minus one, divided by one minus alpha T bar, and then this L infinity is only related to the signal to noise ratio at the endpoints of the whole forward process, and it is invariant to the noise schedule in between the endpoints. So if we want to optimize the forward process in the continuous time setting, we only need to optimize the signal to noise ratio at the beginning and the end of the forward process. And they further show that the in-between process can be learned to minimize the variance of the training objective, and this enables the faster training of diffusion models besides faster sampling. And another contribution of this work is that they show it is possible to use diffusion models to get state-of-the-art likelihood estimation results. So before this work, the benchmark of likelihood estimation has been dominated by autoregressive types of models for many years, as shown in this figure, but this model shows that actually we can use diffusion models to get a big improvement out of the autoregressive model process. And one key factor to make this happen is to add forward features to the input of the unit, and those forward features can range from low frequency to very high frequencies. And the hypothesis or the assumption here is that to get good likelihood estimation, the model usually needs to model all the bits or all the details in the input signal. Either they are in perceptual or perceptual. However, neural networks are usually bad at modeling small changes to the inputs. So adding those forward features, especially those high frequency components, can potentially help the network to identify those small details. And the paper found that this trick doesn't bring much significant improvements to the autoregressive baselines. However, it leads to significant improvements in likelihood estimation for diffusion model costs. Okay, so next paper or method I'm going to discuss is this denoting diffusion implicit models. So in this work, the main idea is like they try to define a family of non-Markovian diffusion processes and the corresponding reverse processes. And those processes are designed such that the model can still be optimized by the same surrogate objective as the original diffusion models. Recall that this is the surrogate objective, right? So this L-simple, where we remove the weighting of each L2 loss term, and we just simply take the average of the L2 loss at different time steps. And then, because they can optimize by the same surrogate objective, so one can simply take a pre-trained diffusion model and treat it as the model of those non-Markovian diffusion processes, so that they are able, we are able to use the corresponding reverse processes to reverse the model, which means that we will have more choices of our sampling procedure. Okay, so to see how we can define those non-Markovian forward processes, let's recap the derivation of the KL divergence in the variational lower bound. So recall that this L t minus one is defined as the KL divergence between this posterior distribution q x t minus one given x t and x zero, and this denoting distribution p zeta x t minus one given x t minus one. So this p zeta is parameterized by the diffusion model, and because these two distributions, like both of them are Gaussian distributions with the same variance sigma t squared, and this can be written as the L2 distance between the mean of these two distributions times a constant. And recall that these two mean functions have been parameterized by simple linear combination of x t and ipsum, or the combination of x t and the predicted ipsum. So ipsum is the noise added to the clean sample to get x t. So we can further write this KL divergence in the form of lambda t times the L2 distance between the true noise ipsum and the predicted noise ipsum theta by our diffusion models. And if we really think about the derivation of this L t minus one, we found that if we assume like this lambda t can be accurate values, because in a surrogate object we simply set it to one, right, so it doesn't matter what value this alpha t is originally. Then we can find that a Bao formulation holds as long as first we have this q x t given x zero, it follows this normal distribution. This is to make sure that this x t still equals to this formulation. And we have those two assumptions. First, the forward process, like the posterior distribution q x t minus one given x t and x zero follows a Gaussian distribution, and the mean of this distribution is a linear combination of x t and ipsum. And we define our reverse process to be similar to the posterior distribution, which means it is also a Gaussian, and this mu theta is the same linear combination of x t and the predicted noise. And then we can further rewrite, because we know x t equals to a linear combination of x zero and ipsum, so we can replace this ipsum by the linear combination of x t and x zero. And similarly for the reverse process, we can rewrite it as a linear combination of x t and predicted x zero hat. So this x zero hat is defined as the predicted clean sample given the predicted noise. So if we make those three assumptions, then we can see that the above duration of L t minus one still holds, which means that we don't really need to specify this q x t given x t minus one, and we don't need to require it to be a common process. All we need to assume is the posterior distribution of x t minus one given x t and x zero. So that's the basic, the insights of this, like how we can define the normal copy forward process, which leads to the same tau objective as the original diffusion model. And more specifically, so this is the original diffusion process. And now the diffusion process changed to the right diagram, where for each x t, it depends on both the x t minus one and the x zero. And another remaining question is how we specify the linear combination parameters, a and b. So note that here we need to specify this a and b, such that this q x t given x zero still follows this Gaussian distributions. So to this end, this work defines a family of forward processes that meets the above requirements, which corresponds to specifying the posterior q x t minus one given x t and x zero in this formulation. And we can similarly define the corresponding reverse process by just replacing this x zero to a predicted x zero hat. And note that this specifying specification of formulation doesn't require like a specific value of this sigma t theta, which means like this sigma t theta can be literally arbitrary value, arbitrary value. So that's why it depends, it actually defines a family of forward processes with different values of sigma t theta. And more importantly, if we specify like sigma t theta to be zero for all the timestamps, this leads to this DDIMM sampler, where we should say deterministic reverse process, because this variance is zero here. And the only randomness comes from the starting point of the reverse process, which is the starting white noise signal. And recall that in the second part, we also build connection between the like stochastic reverse process with a probability flow ODE, which corresponds to deterministic generative process. And we can also interpret the DDIMM sampler in a similar way. Specifically, this DDIMM sampler can be considered as an integration rule of the following ordinary differential equation. And note that here we do a bit of change of variable, where we define this x bar equals to x divided by the scaling factor square root of alpha bar. And we define this eta to be basically the square root of the inverse signal to noise ratio. And if we look at the difference between the two, we can see that the difference between the two is the same, but the difference between the two is the same. And if we assume this sigma, this epsilon theta to be the optimal model, and then this ODE is equivalent to a probability flow ODE of a variance exploding SDE, which is in the following formulation. And note that although these two are equivalent, the sampling procedure can still be different, because for the above ODE, we are taking like the sampling process over this eta t. While for the second formulation, we are taking the, for example, for both of the two equations, we use the standard Euler's method. Then the second one is taking the Euler's step over dt. So basically, in practice, people find that the first one works better than the second one, because it depends less on the value of t, but it depends directly on the signal to noise ratio of the current time steps. And it is also found that with this DDIMM sampler, we are able to use less sampling steps, but reach better like performance. And in terms of why this is true, this paper by Karas et al. argues that the ODE of the DDIMM is favored, because as shown in those three illustrations, especially the third illustration, the tangent of the solution trajectories of DDIMM always points towards the denoted outputs. While for the first two ODE formulations, the variance preserving ODE and the variance is flowing ODE, which are two very commonly used ODE formulation diffusion models. They basically have more like high curvature regions along the trajectories. And while for the DDIMM, we can see like for the solution trajectories, most of the trajectories are linear and with low curvature. And it is known that low curvature usually means less truncation errors accumulating over the trajectories. So if we use this kind of trajectories, like we will have a smaller chance to accumulate more errors across the trajectories. Thus, enable us to use like fewer number of diffusion time step, fewer number of sampling steps in inference. Okay, so that's it. The third word I'm going to discuss for advanced forward process is this critically down to long-term diffusion model. Basically, they are trying to find a faster mixing diffusion process by using certain like background knowledge from Markov chain Monte Carlo. To see how this forward process is related to MCMC, let's see, like this is a regular forward diffusion process, which is a stochastic differential equation. And it is actually a special case of overdamped long-term dynamics. If we assume the target distribution or the target density of this MCMC is this standard Gaussian distribution. So given this connection, we can actually design more like efficient forward process in terms of MCMC. Specifically, this work proposed to introduce an auxiliary variable, this velocity v. And the diffusion process is defined on the joint space of this velocity and the input. And during the forward process, the noise is only added in this velocity space. And this image space or input space is only erupt by the coupling between this data and the velocity. And the resulting process as shown in this figure, we can see that the forward process in the v space is still zigzag. However, the process in the image space or the data space are much more smoother. And this v component is analogous to the Hamiltonian components in HMC or analogous to the momentum in momentum based optimizers. So by defining this joint space or defining the diffusion in this v space, it actually enables faster mixing and faster traverse of the joint space, which enables fast, like more smoothly and efficient, more smooth and efficient forward process. And the second, let's see some advanced reverse process. So before that, we would like to ask a question. So remember that we use this normal approximation of the reverse process, right? So we assume like the denoting distributions are always Gaussian distributions. But if we want to use less diffusion time steps, is this normal approximation of the reverse process still true or accurate? Unfortunately, the answer is no. So this assumption, normal assumption in this denoting distribution holds only when the adjacent, the noise added between these adjacent steps are small. If we want to use less diffusion time steps in training, then we can see that this denoting distribution is not a uni-modal normal distribution anymore. So they are tend to be like multi-modal and more complicated distributions. So in that case, it means like we really need more complicated functional approximators. So I will talk about two examples of how we can include more complicated functional approximators here. The first one is this denoting diffusion GANs. So in this word, they propose to model the denoting distribution by conditional GAN model. Specifically, the model is training this out of several learning framework. And we first get the samples xt-1 and xt by running this forward diffusion process. And then this generator takes xt as input, as well as the time step as input. And it's actually trying to model the xt-1. But instead of just directly outputting xt-1 from the network, it's first trying to predict this clean sample xt-0. And then it tries to sample xt-1 from this tractable posterior distribution. And then the discriminator takes the real xt-1 and the fake xt-1 prime as input and try to discriminate these two samples. And again, this discriminator is also conditional xt and the corresponding time step t. So one may ask, what is the benefit of this denoting diffusion GANs compared to a one-shot GAN model? But this paper shows that because, like right now, the conditional GANs only need to model like the conditional distribution of xt-1 given xt, this turns out to be a much simpler problem for both the generator and the discriminator compared to directly model the model distribution of the clean sample. And this simpler training objective for the two models leads to stronger mode coverage properties of GANs and also leads to better training stability. And recall that in the second part, we learned that there is a close connection between energy-based models and diffusion models. So therefore, a natural idea is like we try to approximate the reverse process by conditional energy-based models. Recall that an energy-based model is in the form of P beta x. It is solely dependent on the normalized log density function, which is f theta x. And we can further prime try this f theta x by minus e theta x. So usually people call this e theta x as the energy function. And this t theta is the partition function, which is usually analytically intractable. And we can parametrize this f theta by a neural network, which takes the signal as input and output as scalar to represent the value of this f. And the learning of energy-based models can be illustrated as follows. So suppose this is the energy landscape defined by the e theta function. And after learning, we would like to put the observation data points into the regions of low energy and put all the other inputs to the regions of the high energy. And optimizing the energy-based models usually require the MSMC sampling from the current model p theta x, as shown in this formulation, which is usually highly computational expensive, especially for high dimensional data. So if we want to parametrize the denoising distribution by conditional energy-based model, we can start by assuming at each diffusion time step. Marginally, the data follows an energy-based model in the standard formulation. So here I removed the script, like the time step script for simplicity. And let x theta be the data at a higher noise level. So we can derive the conditional energy-based models by Bayesian rule. Specifically, this p x, p x t theta is in this formulation. And if we compare this conditional energy-based models with the original marginal energy-based models, we see that the only difference is that there is an extra quadratic term here. And this actual quadratic term actually has the effect of localizing this highly multimodal energy landscape to a more single model or unimodal landscape, and with the model, focus around the higher noise level signal x theta. Therefore, compared to training a single energy-based model, the sampling here is more friendly and easier to converge because the energy landscape compared to the original marginal energy landscape is more unimodal and simpler. So that the training could be more efficient and the conversion MCMC can give us well-formed energy potential after training. And compared to diffusion models, using this energy-based model to parametrize the denoising distribution can give, enables us to define like much less diffusion steps up to six steps. And more specifically, to learn those models, we simply maximize the conditional log-likelihoods at each time step. And then after training, we just get samples by progressive sampling from the energy-based models from high noise levels to low noise levels. Okay, so the last part comes to the advanced diffusion models. Basically, we want to ask two questions. First, can we do model distillation so that the distilled model can do faster sampling? And second, can we lift the diffusion model to a latent space that is faster to diffuse? So the first idea comes to the distillation. So here I want to discuss one representative work in this domain, which is this progressive distillation of diffusion models. Basically, this work proposed to distill a deterministic DDI and sampler to the same model architecture of the original model. And it went into this progressive pipeline in the sense that at each distillation stage, we will have a teacher model and we will learn a student model. And this student model is learned to distill each two adjacent sampling steps of the teacher model to one sampling step of the student model. And after learning this student model, at next distillation stage, the student model at the previous stage will serve as the teacher model at this new stage. And then we learn another student model at the new stage. And we repeat this process until we can distill the original thousand of sampling steps to a single sampling step. And implementation-wise, the learning of the student model is quite similar to the original diffusion model training pipeline. The difference is how we define this training model and how we define the model. And the difference is how we define this training target of the diffusion model. Specifically, given the teacher model, and we randomly sample a time step t, and then we run the sampler for two steps. And then the target is computed to make sure that the student model can reproduce the two sampling steps within one sampling step. And then the loss is defined as Ural, where we minimize the L2 distance between this target and the predicted x hat from this diffusion model or from this student model. And after that, we halve the number of sampling steps and repeat this process until we reach one sampling step. Okay, so the second idea is like whether we can lift the diffusion models to a latent space which is more friendly to this diffusion process. And here's an example of this kind of idea where we can try to lift the diffusion models to a latent space of a pre-trained variational autoencoder. And because this latent space, the distribution of the data in this latent space is already quite close to the Gaussian distributions, which means like we can definitely use less diffusion time steps to diffuse the data in this latent space. So the advantages are pretty straightforward. So first, because this latent space is already close to normal distribution, we are able to use less diffusion time steps to enable faster sampling. And second, compared to the original variational autoencoders, which assume that the prior distribution of the z holds a single Gaussian distribution, this kind of hybrid model assume that the P of z is modeled by a diffusion model, which means that it has a diffusion prior. So it definitely will be much more expressive compared to the original variational autoencoder model. And more interestingly, recall that for the current stage, the diffusion models are only defining a continuous data space. However, there are more domains which may have more complicated data structure. However, for those data type, as long as we can find an autoencoder model which are tailored to that data type, and can map the data input to a continuous latent space, we will be able to apply the diffusion models to that latent space. So this gives us more possibilities to apply diffusion models to different modalities and different data types. And a bit of the detailed formulation. So in this work, again, we optimize the model in terms of by minimizing the variational upper bound of the negative log likelihood. And the objective contains three terms. The first two terms are similar to the variational autoencoder objecting, and the third term corresponds to the training objective of the diffusion models. And it actually corresponds to, we treat the encoding latency from this Q, Z0, given X, as the observed data of the diffusion models. And in that way, we can derive the similar training objective of the diffusion models as the original one. So we first do this random sampling of time step. And then we draw samples from this forward diffusion. And then we have this diffusion kernel. This is the forward, this is from the forward process. And then we learn this score function for ZT. And of course we have some constant that is irrelevant of the model parameters. Okay, so the second question we want to answer is how to do high resolution optionally conditional generation using diffusion models. So in the past two years we have seen many impressive conditional generation results using diffusion models. For example, this DAO-E2 and IMAGINE recruit diffusion models to do high resolution text to image generation. And another example includes using conditional diffusion models for super resolution or colorization. And panorama generation is another example where we take a small size input, but generate this panorama. So how can we do that? Let's first take a look at the general formulation of conditional diffusion models, which is pretty straightforward. So the only modification we need to make is in this reverse process. We can let this denoting distribution to incorporate additional input, which is this condition C. And this corresponds to modify the mean of this Gaussian distribution to take an additional input C. And optionally we can also let this variance to be learned and it takes an input C. But in practice, like most mostly we still just use this C in this mean. And the variation upper bounds only includes a small change, which lies in this KL divergence where we plug in this new formulation of the denoising distribution. But it is a design art in terms of how to incorporate different types of conditions into the unit, which is used to parametrize this mu theta. Specifically, these are the things people use in practice for scalar conditioning. For example, a class label. We can do something similar to what we did for time step conditioning. Specifically, we can encode the certain scalars to a vector embedding and then we simply add the embedding to the intermediate layers of the unit, or we do this adaptive group normalization layers. And if it is an image conditioning, we can do channel wise concatenation of the conditional image and the input image. And if it is a first text condition. This contains two cases. First, if the text, we embed the text to a single vector, then we can do some something similar to the vector derived from the scalar condition. And if we embed this text to a sequence of vectors, then we can consider using cross attention with the intermediate layers of the unit. And for high resolution conditional generation. Another important component is this classifier guidance. The main idea is like, recall the diffusion model correspond to learning the score of a probability, for example, the score of log p x right. And right now, because we incorporate the class condition, which means like we, the score actually creates the diffusion model actually gives us a score of a class conditional model p of x t given c. And given this, we can train additional classifier, which gives us the probability of c given x. And then we mix the gradients of these two models during sampling. And this corresponds to a sample from actually a modified score which corresponds to the gradient of log p x given c plus omega times log p c given x. And this omega controls the strength of the guidance, and it actually corresponds to a approximate sampling from the distribution, which is proportional to p of x given c times p of c given x to the omega power. And in practice, it corresponds to, we modified the normal distribution, where we sample from and the mean of this normal distribution corresponds to the mean predicted by the score model or predicted by the diffusion model, plus the gradient from the classifier. And if we use larger omega, then the samples will be more concentrated around the modes of this classifier, which usually leads to better individual sample quality, but if we use too large omega, it will reduce the sample diversity. So one really needs to find a sweet point for this omega to best balance the individual sample quality and sample diversity. And one downside for this classifier guidance is that we need to train additional classifier to do that right so that adds additional model complexity. So inspired by that work, this work tries to introduce a classifier free guidance, meaning that we can actually get an implicit classifier by jointly training a conditional and unconditional diffusion model in a polling sense. So suppose we have this p x given c, where the score can be derived by a conditional diffusion model, and we also have the score of a unconditional model p of x, and then we can derive an implicit classifier p c given x, which should be proportional to p of x given c, divided by p of x. And in practice, the gradient or the score of these two probabilities are estimated by randomly dropping the condition in the diffusion models at a certain chance for each iteration. And similarly, we can derive the modified score with this implicit classifier. Recall this is the original modified score. And now we replace the log p c given x by log p x given c minus log p x. And this is the resulting modified score we will use with this classifier free guidance, where this log p x given c and this log p x are both estimated by the single diffusion model. And in these three panels, we can see the trade-off between sample quality and sample diversity more clearly. So from left to right correspond to we gradually increase the strength of the guidance, and we can see clearly individually speaking, the sample quality of each image increases. However, the samples are look like more similar to each other if we use a large classifier or classifier free guidance. And the last thing I want to talk about is this cascading generation pipeline, which are important to high resolution generation. And this kind of idea have already been explored by other type of generative models for, for example, GAN model. And it's pretty straightforward. So we start by learning an unconditional diffusion model at the lowest resolution. And then we've learned several super resolution models, taking the down sampled training images at lower resolution as the condition. And during sampling, we just ran the progressive sampling pipeline starting from the smallest unconditional model and going through all those super resolution models until we reach the highest resolution. But one notorious problem of this kind of cascading training pipeline is this compounding error problem. It's more specifically recorded that during training, the conditions we fit into the super resolution model is the down sampled versions of the training images from the data set. However, during sampling, the conditions we fit to those super resolution models are actually generative samples of from the low resolution models. So if there are certain artifacts in the samples from the low resolution models, those artifacts or inaccurate samples will affect the sample quality of the super resolution models as well, because of this mismatch issue between the conditions in training and condition inference. To elevate this problem, this noise conditioning augmentation is proposed in reference works. Basically during training, we try to degrade the conditioning low resolution images by adding variant amounts of Gaussian noise, or just blur those images by Gaussian kernel. And during inference, we sweep over the optimal amount of noise added to the low resolution images, which are the conditions to the super resolution models. So the idea or the hypothesis is like, if we try to reduce certain amount of information from the condition, then the super resolution model will be trained to be more robust to different type of artifacts. When we send like the conditions as the samples. And later on, more complicated degradation process have been proposed. For example, we can add a sequence of different types of degradation operations to the image, the low resolution image before sending it as a condition to the super resolution models. Okay, so here's a summary of these parts. So in this advanced techniques session, we learn to answer two questions. The first one is how we can accelerate the sampling process, and we introduce several important techniques from the aspects of advanced forward process reverse process and modeling itself. And the second question is how we can do high resolution conditional generation using diffusion models, and we discuss the general framework of conditional diffusion models, Placifier and Placifier-free guidance, as well as cascaded generation pipeline. So in the application section we will see how all those techniques will benefit in terms of various tasks. Hi everyone, welcome to the first section of applications of diffusion models. So in this section we're going to study applications of diffusion models in terms of image synthesized controllable generation as well as text to image generation. So let's from text to image generation. So in the past two years. This task has been shown to be extremely suitable for diffusion models to work on. So basically this task is the inverse of the image captioning tasks, where we are given a text prompt C and we are trying to generate high resolution images, x, as shown in this video. So, this video shows the generative images by text to image generation model called Imagine, as we will show later. And let's start from this Glide model by OpenAI last year. So this is essentially cascaded generation diffusion models, which contains 64 by 64 base model and a 64 by 64 to 56 by 256 super resolution model. And they have tried to use classifier free guidance and clip guidance. So I will talk about a clip guidance in details later. And they generally found that classifier free guidance works better than the clip guidance. And those figures shows the generative samples from this model. And as we can see, the model is capable of generate fun, novel conversations of concepts that have never been seen from the data set. For example, a hedge dog using a calculator and robots meditating in a vipassana retreat, etc. So a bit introduction of the clip guidance, it can be treated as a special form of the classifier guidance. And in terms of a clean model, it contains two components, a text encoder G and image encoder F. And during training batches of the image and caption pairs are samples from a large data sets. And the model optimize a contrastive cross entropy loss, which encourages high dot product between this F and G, if the image X and C comes from the same image caption pair, and it encourages low dot product, if X and C comes from different image caption pairs. And it can be proved that optimal value of this FX times G C is given by log p X C divided by p of X times p of C, which equals to log p C given X minus log p C. So given this conclusion, we will be able to use this clip model as the classifier in the classifier guidance in a following sense. So recall that for the classifier guidance, we want to modify the score in this formulation. And we can consider augment this augmenting the second term by a minus log p C term, because when we take gradient over X, then this part just disappeared. And then we can see that these two terms together can be modeled by a clip model. So basically replace this part by the dot product between FX and GC. So that is the clip guidance. However, in Glide they show that the clip guidance is less favored compared to the classifier free guidance. And besides pure text to image generation, the Glide has shown that it is possible to fine tune the model for text guided impending tasks. Basically, they try to fine tune the trained text to image Glide model by fitting randomly occluded images with an additional mask channel as the input. So using this fine tune model, they are able to change or do image editing by changing the prompt. For example, given an old car in a green forest, they will be able to edit the background to a snowy forest. Similarly, they can add a white hat to a man's hat. And later on this DALI2 further scale up the Glide model to support 1k by 1k text to image generation. And this DALI2 has been shown to outperform the first version of text to image 1k by 1k generation by OpenAI, which is this DALI, which is the autoregressive transformer based model. And in terms of the model components of DALI2, it's built up on a pretrained clip model. More specifically, a clip model is first pretrained and the image embedding and text embedding are grabbed from this pretrained clip embedding and frozen. And after that, this PEP plan has been built to generate images from text. Basically, this generation model contains two parts. The first is a prior model. This prior model tried to produce clip image embeddings conditioned on the input caption. And then the second part is a decoder part, which produces the images conditioned on the clip image embedding as well as the text. So one natural question is why we want to condition the decoder on the clip image embeddings, right? So why now we just directly condition this decoder on text only? So the hypothesis here is that for the total amount of entropy of an input signal, for example, images, so there is certain part that captures the high level semantic meanings, while there's still a large proportion of the entropy, which corresponds to just low level details, either perceptual visible or even perceptually invisible. So the hypothesis is that clip image embeddings tend to have a higher chance to capture the high level semantic meaning of an input signal, especially those related to the caption information. And by conditioning on this high level semantic meaning, the decoder is able to capture or catch up those low level details of the images more quickly. And later on this figure, this paper also shows that this by part later replications of the clip image embedding, as well as the latency in the decoder model enables several text guided image manipulation tasks, as we will show later. And a bit more details of the model architecture of the prior and the decoder models. For the prior, the paper tries two options. The first one is the autoregressive pair, where they quantize the image embedding to a sequence of discrete codes and predict them autoregressively. And the second option is to model the prior by diffusion models, where they directly train a diffusion model based on the continuous image embedding, as well as the caption input. And the paper shows that the second option gives better performance. And in terms of the decoder is again a cascaded diffusion models, which contains a one base model and two super resolution models. And to save the compute and make the training more efficient, the largest super resolution model is trained on image patches of one quarter size, but during inference, the model will take the full resolution inputs and directly to the inference on the full resolution. And this paper also shows that the classifier free guidance and noise conditioning augmentation are super important to make the decoder work well. And a little bit more detail about the bipart latent representations. So given an input image, we can get the bipart latent representations in the following sense. So it contains two parts. First is this latent variable z, which is the clip image embeddings, and it can be derived by running the clip image encoder. And the second part is this x t, which is the latency from the decoder, and this part can be derived by running the inversion of an DDIMM sampler for the decoder. And after getting these two latent representations, the paper shows that it is possible to run this decoder and get near perfect reconstruction of the original input image. And given this bipart latent representations, the paper shows that it is possible to do several image manipulation tasks. For example, this image variation tasks target at getting multiple variants of an input image, while with the hope of preserving the high level semantic meanings of the input image. And this is achieved by fixing the clip embedding z, while changing to different latent x t in the decoder. And as showing this image panel, the first one is the input image, and the rest are the image variants generated by DALI tube. And we can see that certain high level semantic meanings are preserved, for example, the artist's style, and this clock is preserved in all the image variants, but with different details in the image variants. And the second task is this image interpolation task. So given two input images, it's possible to use DALI tube to do interpolation by interpolating the image clip embeddings of these two input images. And we can get different interpolation trajectories by using different x t along these trajectories, as shown in those three rows. And as we can see, although the trajectories are different, but the high level semantic meanings are kept well for the two input images for all those three interpolation trajectories. And the last task, the most interesting task that they show that can be done by DALI tube is this text diff task, which means like given an input image and the corresponding description, we would like to add this image towards a different prompt. And this corresponds to an arithmetic operation in the latent space. More specifically, they first tried to compute the difference between the text clip embeddings of the original prompt and the target prompt. And then they tried to change the image clip embedding of the given input towards the difference between the text prompts. And using this approach, they show that it's possible to do this image, text guided image editing by changing the prompt. And the last text to image diffusion model I want to talk about is this imagine model by Google brain team. So again, the task is the same as DALI tube. So we are given some text prompt as the input, and we are trying to output 1k by 1k images, align with the input text. And the highlight of imagine model is as follows. First, it provides a percentage degree of photorealism in terms of state of the art automatic scores such as FID scores, as well as state of the art human ratings. And it provides a deep level of language understanding, as can be taught by the generated samples. And it is really simple so there is no latent space and no condensation, and as we will see later it's just like a pure cascaded diffusion model. So I will first present several examples of imagine. This is my favorite one because I just created it to make it related to CPR. And in terms of the key modeling components of imagine. Like I mentioned, it is a pure cascaded diffusion models containing one base model and two super resolution models. And it uses classifier free guidance, and the dynamic thresholding, as I will talk about later. And unlike DALI tube which uses clip text embedding, as the using this clip text encoder. This imagine used frozen large pre trained language models as a text encoder, more specifically this variant of T5 model. And there are several key observations from imagine. First, it is beneficial to use text conditioning for all the super resolution models. The explanation is as follows. So remember like for cascaded diffusion models we need to use this noise conditioning augmentation technique to reduce the compounding error. But, however, this technique has a chance to weaken the information from the low resolution models. Thus, we really need the text conditioning as extra information input to support the super resolution models. And second observation is that scaling the text encoder is extremely efficient in terms of improving the performance of imagine. And it has been shown that this is even more important than scaling the diffusion model side. And lastly, comparing using the pre trained large language model as the encoder versus the clip encoder, human readers actually prefer the large language model over the clip encoder on certain data set. And this dynamic thresholding is a new technique introduced by imagine. And finally, we need to solve the trade off problem of using large classifier free guidance weights. More specifically, as we also discussed in the previous part. So, when we use large classifier guidance weights, there is a chance that it gives us better text alignment, but worse image quality. So, as we use large free guidance weights, the clip score, which correspond to a better text alignment increases. However, the IFID score also increases, which correspond to worse sample quality. So to elevate this issue, because we really want both the sample quality, like the good sample quality, as well as good text alignment, right. So to elevate this trade off issue. The hypothesis this paper made is that the reason why the sample quality decreases at large guidance ways is that at large guidance weight, usually it correspond to very large sample gradients in inference, and then the generative samples have a chance to be saturated, because of the very large gradient updates. So the solution they propose is this dynamic thresholding, meaning that at each sampling step, we adjust the pixel values of the samples to be within a dynamic range, and this dynamic range is computed over the statistics of the current samples. And these two panels shows the qualitative comparisons between statistics, a static thresholding and dynamic thresholding. And you can see, if you've seen this, if we use this static thresholding, the images look kind of saturated, while the dynamic thresholding, the samples look more realistic. And another contribution of IMAGINE is that they introduce a new benchmark, especially for this text to image evaluations. So the motivation of introducing new benchmark is that for existing data sets, for example, COCO, the text problem is kind of limited, and it's kind of easy. So this benchmark introduced more challenging prompts to evaluate text to image models across multiple dimensions. For example, it tries to evaluate the ability of the model to faithfully render different colors, numbers of objects, spatial relations, text in the scene, unusual interactions between objects. And it also contains some complex prompts, for example, long and intricate descriptions, where was, and even misspelled prompts to test the robots needs of your model. And this figure shows several examples of the text prompts in this benchmark, and the corresponding generated images from IMAGINE using this text prompts. And a bit more of the quantitative evaluations of IMAGINE. IMAGINE got state of the art automatic evaluation scores on the COCO data set. And it's also preferred over recent work by human raters in both sample quality and image text alignment on the Drawbench data set. And the recent work compared by IMAGINE includes this DAL-E2, BLIDE, VQGAN plus CLIP, as well as the latent diffusion models. Okay, so besides text to image generation, I also want to talk about the controllable generation using diffusion models. And a representative work is this diffusion autoencoders, which propose to incorporate a semantic meaningful latent variables to diffusion models. So more precisely, so given an input image, semantic encoder is learned in this framework to generate this Z-SIM. And this Z-SIM is fit into a conditional diffusion models to further predict the clean samples. And this leads to some like the bipod latent representation similar to the DAL-E2 model, where we have this Z-SIM with the hope that it can capture high level semantics. And we also have this X-Big-T, which is the inversion of this conditional DDIM sampler. And the hope is that it captures the low level stochastic variations of the images. And if we want to do unconditional sampling from this model, optionally we can learn another diffusion model in the latent space of the Z-SIM, very similar to the latent diffusion models we talked about in the last part, to support this unconditional generation tasks. And interestingly, they found that by assuming a low dimensional semantic vector Z, they are able to learn like different semantic meanings for different dimension of this latent vector Z. For example, by changing the certain dimension of this latent Z, they are trying to identify different semantic meanings such as the higher style, the expression, the age, and also the color of the hair. And they also examine that if we fix the Z-SIM for each row, and we change the latent X-Big-T in the conditional diffusion model, we can see it only corresponds to very tiny details in this image and perhaps other like perceptually invisible features in the images. So that ends the first part of the application. Thanks for listening. Awesome. Thanks, Ruiqi, for the nice introduction of the first group of applications. Here, I'm going to start with the second group of applications. And in this part, I will mostly focus on image editing, image-to-image translation, super resolution, and semantic segmentation. This is the super resolution. I'll start with talking about this work called super resolution via repeated refinement, or SR3, which was proposed by Sahari et al. at Google. In image super resolution, we can consider this problem as training a conditional model, P of X given Y, where Y is the low resolution, and Y is the low resolution, and Y is the low resolution. And X is the corresponding high resolution image. So we want to be able to generate high resolution images, given some input low resolution image. In order to tackle this problem, the authors proposed to train a diffusion model, a conditional diffusion model, using this objective. Here in this objective, we have expectation over pairs of high resolution image X and low resolution image Y. We have expectation over epsilon, which is drawn from standard normal distribution, and we have expectation over time, where time varies from, for example, zero to capital T corresponding to the diffusion process. We have this epsilon setup. This is a noise prediction network that takes diffused high resolution image X, T, the time, as well as this Y. This is the low resolution image that is provided as conditioning into epsilon prediction network, and we train this epsilon prediction network to predict the noise that was used in order to generate diffused high resolution image. The authors in this paper proposed to use different norms for minimizing this objective. They introduced L1 norm, L2 norm, and they observed that one can trade quality for diversity by using L1 norm as a L2 norm. Since we are training a conditional model, now you have in mind that we need to modify the unit that is used for epsilon prediction. For the input of the unit, we will have access to diffused high resolution image, as well as this low resolution conditioning input. Since these two images don't have the same spatial dimensions, the author proposed to use just simple image resizing algorithms to up sample the input low resolution image and concatenated with diffused high resolution image and the channel dimension, and they provide both of them to us. This is the unit diffusion model or the epsilon prediction model and the network is trained to predict the noise that was used in generating the high resolution image. This method achieves very high quality results for super resolution. For example, here you can see super resolution results with 64 by 64 pixel input when the output is 256 by 256. And you can see that this method here shown in this column achieves really high quality result compared to for example regression models, or just simple image resizing algorithms, or using bi-cubic interpolation. And you can see that this actually does a good job of generating low level details. Another thing I'd like to mention is called Palette image to mesh diffusion models. This method is also proposed by the same authors as the previous method. So the lead author is Saharia et al. Similar to super resolution, many image to image translation applications can be considered as training a conditional model X given Y, where Y is the input image. For example, if you consider colorization problem, X is the output color image, and Y is the grade level input image. So similar to the previous slide, we're going to again train a conditional diffusion model using this objective. Similarly, we have an expectation over pairs of input conditioning Y. For example, Y is the grade level image, X is the output color image. We have expectation over epsilon drawn from standard normal distribution, expectation over time. We're training a conditional diffusion model that takes input grade level image, for example, conditioning time, as well as the diffuse output image that we want to generate at the end. And then the model is trained to predict the noise that was used to generate the few samples. Similar to previous part, again, we need to give a pair input to the unit model. And here, because for example, if we're tackling the color problem, we're going to have this grade level image and diffuse color image as input to unit. Similarly, it's trained to predict the noise that was used for generating diffuse samples. The authors tried their image-to-image translation, image-to-image diffusion model on four different tasks, including colorization, in-painting, JPEG restoration, and uncropping, which is basically given this image. They wanted to extend the image and provide the copper part of this, what they call uncropping. And this paper shows that actually diffusion models can achieve very good results on these four tasks. We should have in mind that this particular paper assumes that you have access to pairs of input and output data. So they're training a conditional model, assuming that we have input image and output image that we want to generate. If we don't make that assumption, what we can do, we can potentially take an unconditional model that is trained on, for example, natural images, and we can modify it for a particular task. So, as an example of that approach, I want to mention, I'd like to mention this paper called Iterative Latent Variable Refinement, or ILBR for short, that was proposed by Choi et al. at ICCV 2021. This paper proposes an approach where, given a reference image, the authors like to modify the generative process of diffusion model, such that the output of the diffusion model can be similar to the reference image. So, again, we have a reference image, and we want to modify the reverse generative SDEs, or reverse generative diffusion process, the imaging process, such that we can generate images that correspond to a reference input image. So, and the authors, and this paper proposes to do this through using some unconditional model that is not trained for a specific task, it's just the unconditional model is trained to generate realistic, for example, faces on this slide. So, the basic idea is to modify the reverse denoising process, such that we can pull the samples towards the reference image. So here you have the algorithm proposed in this paper, the algorithm starts from capital T goes to one, so this is the reverse denoising process, at every step we draw a random noise vector from standard normal distribution. We sample from the reverse denoising distribution, so we generate this proposal, let's call it x prime t minus one, the proposed denoising sample drawn from the denoising model. y here represents the reference image, so we're going to use the forward diffusion canon to generate the diffused version of reference image, so we're going to go forward in time for this reference image. And we're going to generate y t minus one. So ideally what we want to do, we want to make this proposed denoised image x prime t minus one to be more similar to y t minus one. So to do so, this paper proposes this simple operation. Here, phi n represents some low pass filter. So this operation is very simple. We have this proposed denoised image x prime t minus one, we subtract the low pass filter applied to this x t minus one, and we add back low pass filter output of y t minus one. So you can think of this operation as an operation that takes x t minus one, the x prime t minus one, this is the proposed denoised image, it removes its low pass filter, low frequency content. So here we are removing the low frequency content of x prime t minus one, and we're adding back the low frequency content of y t minus one. And since we're putting the low frequency content of y t minus one, this is the reference image, into x prime, into x prime denoised, the proposed sample, what we are doing, we're basically making sure that in the reverse process, we're generating a sample where the low frequency content is similar to the reference image, so that most of the structure is very similar to the reference image. So, as I mentioned, find n is just a low pass filter, and in order to implement this, the authors proposed to use a simply downsampling-upsampling operation where n represents the downsampling ratio used for in this operation. When n is equal to two, it means we're gonna just take reference image, or take this input, downsample by a factor of two, and then upsample it again by a factor of two, which corresponds to a low pass filter operation. Here you can see these two reference images, and how we can generate images with different values for n. So when n equals to four, it means that we actually take this during the generation, we downsample the low pass filter, we downsample the images by factor of four, we upsample it by factor of four. Since this factor is small, this means that most of the structure in the generation will be similar to reference image. So you can see that, in fact, we're generating an image that is very similar to reference image, and as we increase this n, we can see that now different levels of details can be generated through the diffusion model, and the more global characteristics, like more global arrangements or low pass, low frequency content of the image, still remains the same as the reference image. And that will show that actually you can do this for different tasks. Image translation, given a portrait image, they can generate a realistic image that corresponds to that portrait image. They can do paint to image, so they can take all painting and generate realistic image, and they can do some simple editing, and for example they can add back this watermark into it. In this part, I'd like to talk about how we can take representation learned through diffusion models and use them for some downstream applications such as semantic segmentation. I'm talking about a specific paper called label efficient semantic segmentation with diffusion models. That was proposed by Warren Chok et al at ICILAR 2022. This particular paper proposes a simple approach for using the representation trained in diffusion model for semantic segmentation. The authors proposed to take input image and diffuse it by following the forward diffusion process. They only go to small steps in forward diffusion step which corresponds to adding just a bit of noise into input image, then they pass this diffuse image to the noise in diffusion model, the unit model, the epsilon prediction model. They extract representation from the internal representation form in this unit at different resolutions of the unit decoder. And then given these representations they up sample all this intermediate representation, so that they have the same dimensional spatial dimensionality as input image, so we have these up sampling layers. We have these feature maps that have the same dimensionality as the input image, and now they simply concatenate all these intermediate feature maps, and they pass them to 1.1 convolutions that will do semantic segmentation per pixel so you can think of these as pixel classifiers that just classify each pixel for each semantic object groups or semantic groups. So, in order to train this model that was proposed to use a pre trained diffusion model and they're only training this component here, up sampling component doesn't have usually any training parameters but most of the parameters, additional parameters are basically here in this 1.1 convolution networks. This paper particularly shows that this approach is labeled efficient using very few labeled instances they can train diffusion models on several datasets as you can see on this slide. And the other show that actually diffusion based segmentation models cannot perform mass autoncorders, can or be a E based models. On this task which is very interesting shows that actually representation learning diffusion models can be used for downstream applications, such as segmentation. In this part I'd like to talk about the particular paper, particular method that is proposed for image editing, it's called SDE Edit. This paper was proposed by Meng Itao at Stanford University, and it was proposed, it was presented at ICLR 2022. The problem that this paper is trying to tackle is that, given this stroke painting, the artist proposed a simple approach to generate realistic images that corresponds to that stroke painting. The main intuition, or the main idea in this paper is that the distribution of real images and stroke painting images, these are two different distribution. These two distributions have some mismatches they're not the same and in the data space, they are not completely overlapping each other, because of these differences. But if you follow the forward diffusion process, if we have this distribution of realistic images distributions to paint it, if we follow the forward diffusion process these two distribution will start having overlaps with each other, because of the definition of forward diffusion process we know that actually, if you have two distribution and you diffuse the samples in those two distributions, the distribution will still have overlap. This forward diffusion simply corresponds to adding noise into input stroke painting. Now that we know these two distribution are overlapping we can use just a generative model trained on real images to solve the reverse SDE, reverse denoising SDE. And this is a model that will start from this diffuse stroke painting and try to generate a realistic image that corresponds to this noisy input. And the other sort of actually using a generative model, this is an unconditional model again, trained on realistic images, they can come back to realistic images where the colors here are very similar to this stroke painting colors. So, this is, I think, a very clever idea to take stroke paintings and generate realistic images that correspond to those stroke paintings. However, there is another thing actually we should have in mind that is trained in a conditional setting. However, we should have in mind that this approach mostly relies on the color information in order to take the stroke painting and generate the corresponding image, and this is a bit different than for example methods that would use the semantic layout, semantic mask of objects, in order to tackle this problem. So, it has some advantages and disadvantages that we should have in mind. This shows very interesting results on different datasets. Here you can see stroke paintings for models trained on some bedroom, some church, and some cellophane. And you can see here in these two rows how a generative model using SDE now can be used to generate realistic images that correspond to stroke paintings. Lastly, in this part, I'd like to talk about particular work that we did at NVIDIA for adversarial robustness. And in this particular work, we introduced diffusion models for adversarial purification. So the basic problem we want to address is that given an adversarially perturbed image, we want to see if we can use diffusion models to remove adversarial perturbations and clean this image such that when we apply classifier on these adversarially perturbed images, we can actually get robust classification output. So, the proposed idea here is similar to SDE, mostly applied for adversarial purification. Given this adversarially perturbed image, what we propose to do is we propose to follow the forward SDE, which corresponds to basically adding noise and diffusing the input adversarially perturbed image using just a forward diffusion kernel. So we go to a particular time t star, we call, and we simply diffuse the input adversarially perturbed image. We know that by adding noise, we can now wash out all these adversarial perturbations that are applied into image. Now that we have this noisy image, we use the reverse genitive SDE or reverse noisy SDE to start from this noisy input and generate clean image that corresponds to this noisy image. And we know that through this process, we can now remove all the noise that was injected as well as all the adversarial perturbations presented here in this image. Now that we have this clean image, or purified image, we can just pass it to classifier and hopefully make a robust classification prediction. Like any adversarial defense mechanism, we need to be able to attack this model. In order to evaluate our performance, we need to be able to attack this model. So in this paper, we also show how we can attack this model by backgating end-to-end through classifier as well as our purification algorithm. And this involves basically backgating through this reverse SDE. So we showed in this paper how we can do this and how we can attack this mechanism end-to-end. On the left side in this slide, you can see an example of adversarial perturbed images and first column on a cell A dataset. Here, we intentionally increase the magnitude of adversarial perturbations so that they are visible to us. These two groups are representing adversarial perturbation for a smiling class and these two represent adversarial perturbation for eyeglasses. Here you can see diffuse samples that are generated by following the forward diffusion. This simply corresponds to sampling from diffusion kernel. And then here in these two columns, you can see samples that are generated when we're solving the reverse generative SDE. And as you can see, at time equals to zero, we can remove not only the adversarial perturbations as well as all the nodes that was injected through forward diffusion process. And you can see that our generated images at A equals to zero are very similar to input clean original images. And we can see that the semantic attributes of these images are very similar to semantic attributes of the original images. The nice thing about using a generative model for adversarial purification is that these models are not trained for specific attacks and specific classifiers. So at the test time, we can just apply them for unseen adversarial attacks. In comparison to the state-of-the-art methods that are designed for similar situations for unseen threads, we actually see that our proposed diffusion purification method outperformed these methods by a large margin. And we believe that, in fact, diffusion models can be really strong models for designing adversarial purification techniques. And this is primarily because diffusion models can generate very high quality images and potentially can be used for removing all the artifacts that are generated by adversarial perturbations. This brings me to the end of the second part of applications. Next, Carson will continue with the third part of applications. Thanks. Alright, I will now talk about video synthesis, medical imaging, 3D generation, and discrete state diffusion models. Let's get started with video generation. Here are samples from a text conditioned video diffusion model by Jonathan Ho et al. where we condition on the string fireworks. So I think these samples look pretty convincing. So there are actually in general different video generation tasks. For instance, there is unconditional generation, where we want to generate all frames of the video from scratch. Without conditioning on anything, there was a future prediction, where we want to generate future frames, conditioning on one or more past frames. We can also do past prediction the other way around. And there was also interpolation, when you have some friends and we want to generate in between friends, just for instance useful to increase the frame rate of the video. All these generation tasks can basically fall under one modeling framework. In all cases, we basically want to learn a model of form theta of x t one to x t k given x tau one to x tau m. And now those t's and tau's denote the times for frames that we want to generate and for the frames that we condition on. So for future predictions these tau's will all be smaller than the t's or unconditional generation and we wouldn't have any tau's and so on and so forth. Something we see in multiple of these recent works is that they try to learn one diffusion model for everything. So what they do is they concatenate and combine both frames to be predicted and conditioning frames together. And then some of these frames are maxed out, so wants to be predicted. And yeah, based on the conditioning frames, those as a generated and varying the masking and conditioning combinations during training, we can train one model for these different tasks. So during training we would also tell the model, which friends are masked out, and we will fit the model time position and coding tranquil the times for the different frames. Let's visualize here, for instance, in terms of architecture. These models are still using these units, which we already know from the image based diffusion models. But there's no like small detail. So of course, now our data is higher dimensional, because in addition to the image and height and width dimensions, as well as the channel dimensions. We now also have the time dimensions with the number of frames, so the data is essentially four dimensional. So how can we deal with that? One way is to use now 3D convolutions instead of 2D convolutions to run convolutions over height, width, and the frames. This can be computationally expensive. Another option is, for instance, to keep spatial 2D convolutions and use attention layers along the frame axis. Some papers do that. This has the additional advantage that ignoring those attention layers and model can be trained additionally on pure image data, which is kind of nice. So let's see some results. It turns out that this video generation diffusion models. They can actually generate really long term video in a hierarchical manner which is quite impressive. And there we precisely leverage these masking schemes that we just had, and these generalized video diffusion frameworks. So one thing you can do for instance is we generate future frames in a sparse manner by conditioning on frames far back. This gives us long term consistency. And then we interpolate the in between frames afterwards. So we kind of generate the video in a stage wise hierarchical manner. And with that, it's possible to actually generate really long like one hour coherent videos, which is quite impressive. So here are some samples from this recent Harvey et al. work. All right. Let us now talk about another application of diffusion models, which is solving inverse problems in medical imaging, another very relevant application. So medical imaging may refer to computer tomography, or medical or magnetic resonance image. In those cases, we're basically interested in like an image x, but that is not what we actually measuring from like CT scanner or MI scanner. So let's consider the measurement process. For instance, in computer tomography. The forward measurement process can be modeled in the following form. Given the image, we are basically performing a lot on its form, which gives us a sinogram. And then maybe this is sparsely sampled. So we end up with this sparsely sampled sinogram. And now, the task is that needs to be solved to reconstruct the image, given this measurement, why. So this is an inverse problem. It's similar and magnetic resonance imaging just set to follow up process is not basically model to fully transform, and it just ends up with sample. So this is where diffusion models now come in, so they can actually be really used in this task, and the highlight idea is to learn the generative diffusion model as a prior over the images, we want to reconstruct. And then, while sampling from this diffusion model, we guide synthesis condition, while conditioning on the sparse observations that we have. This is the idea. And it turns out that doing this actually performs really really well. And even this outperforms even fully supervised method, sometimes. What does it mean specifically. The thing is, when we train this diffusion model over these MCT or MRI images, we really just need the images, we do not need paired image measurement data to train this. So yeah, there's actually a lot of work in that direction, because it's a really high impact application of course, and there are some citations, if you are interested in learning more about this. So, let's move on to the next application topic, which is 3D shape generation. Also, 3D shape generation has recently been tackled with diffusion models. So let us consider this work by Zhu et al, for instance, here, 3D shapes are represented as point clouds. So point clouds have the advantage that they can be diffused really easily and intuitively. We see this here at the bottom, right, so where the diffusion actually goes from the right to the left, we have a bunch of points, and they are perturbed in 3D towards this, yeah, Gaussian noise ball, kind of, and the generation goes into the other direction. So in those cases, the architectures that we use to implement the denoiser network are like typical state-of-the-art modern point cloud processing networks, like PointNet, advanced versions of PointNet, and PointVoxel, C&Ns, and so on and so forth. How does this look like then? Here's another animation. I think this is quite nice. So, yeah, we can generate these pretty good shapes. We can also train conditional shape completion with diffusion models, where we condition, for instance, on like measured depth or like some sparse points like this, and then complete those shapes. And we can do that in a multimodal fashion. For instance, in this example, we have those legs of the chair given, and now we have like different plausible completions of the chair here. And something that is also quite cool is that it works on real data. I think here the model was trained only on synthetic shape net data, and yet we can fit the model images that we take, these depth images, and generate plausible 3D objects. Very nice. Alright, finally, I would like to talk about discrete state diffusion models. This is less of an application, but a slightly different type of diffusion model, but I think it is worth mentioning as part of this tutorial. So, so far, we have only been considered continuous diffusion and denoising processes, which I mean with that is, we basically kind of assume our data is of a continuous nature, and we could add a little bit of Gaussian noise to it in a meaningful way. So both our fixed forward diffusion process, and also our reverse generative process, are usually, were usually implemented as Gaussian distributions, like here. But what if our data is discrete, categorical, then continuous perturbations are not meaningful, they are not possible. So if we imagine for instance our data as text data, or you know, pixel-wise segmentation labels, or discrete image encodings, then yeah, if our data is discrete, adding Gaussian continuous noise to it doesn't really make much sense. So, can we also generalize this diffusion concept to like discrete state situations. So, these are categorical diffusion models. And in those cases, the forward diffusion process, or like the perturbation, now is defined using categorical distributions. For instance, let us consider the perturbation corner Q of x t given x t minus one, that is supposed to perturb the discrete data. So this cannot be a categorical distribution, where the probability to sample one of the categories is now given by some transition matrix Q, multiplied together with the state we are in x t minus one. So the probability to sample like the new state x t. This x t is usually a one-hot state factor describing the state of n, and this transition matrix multiplied to it will then give us probabilities to sample the next state. So with that, we can perturb complex distributions, categorical distributions, towards like very random discrete distributions, if we choose this transition matrix accordingly. So for instance, in this example, if we look at the right, this may now be a complex data distribution, we can perturb this towards a uniform discrete distribution over these three different states, one, two, three. And then the reverse process for generation, which is then implemented through a neural network, we can also parameterize this as a categorical distribution. So in fact, there are different options for this perturbation process, this forward perturbation process. One thing is we can use uniform categorical diffusion, where we pull everything towards a uniform distribution over the different categories, like I have just shown. We can also progressively kind of map out the data, where we pull everything into one particular state, we can also analytically sample from such a distribution. So it's also well suited for diffusion model. We can also tailor our diffusion processes to ordinal data and use something like discretized Gaussian diffusion process, that's also possible. How does this look like, for instance? So here now I have the data distribution, it's a bit complex, but so this is basically each pixel of this image represents one categorical variable, and now the color of this pixel represents which teacher we are in. So now if I would do like this uniform categorical diffusion, I would kind of, you know, yeah, it would look like this, where I would transition into different states everywhere in the image. I could also do something like Gaussian diffusion where it's more like this ordinal thing, where it's more based, I transition to neighboring states. Then there was also this absorbing diffusion where I kind of progressively mapped out or absorbed my states, sort of, just the different ways to do this. And then in the reverse process, this may for instance look like this. So here on the far right, this is the stationary distribution of this categorical distribution from which I can sample analytically, and then denoising kind of progressively denoises this back towards the data distribution. So yeah, one can use this and some papers have explored such discrete state diffusion models. For instance, we can also apply this on images by modeling the pixel values of images as discrete states to be in. One can do this start from a uniform, uniformly distributed pixel values right here from this all gray kind of state, or all masked out state. Another application is to use this in a discrete latent space. So in this work for instance images are encoded using vector quantization techniques into a visual tokens in a discrete latent space, and then we can use something like discrete diffusion models and similar techniques to model the distribution over this visual tokens. This is also something one can do. One can also use discrete state diffusion models to generate segmentation maps which are also categorical distributions in pixel space. And, yeah, that concludes my part. And with that, I would like to pass the mic back to Arash, who will now conclude our tutorial. Thank you very much. Thank you for being with us. This basically brings us to the last part, conclusions, open problems, and final remarks. So today was a big day, we learned about diffusion models. At the beginning of this video, I started talking about the Noising Diffusion Prophecy Models, which is a type of discrete time diffusion models. I showed you how these discrete time diffusion models can be described using two processes, a forward diffusion process that starts from data and generates noise by adding noise into the input, and then reverse the noise in process that learns to generate data by starting from noise and denoising the input image one step at a time. I also talked about how we can train these diffusion models by simply generating diffuse samples and training network to predict that, to predict the noise that was used to generate diffuse input images. In the second part, Carson talked about the Score-based Genitive Modeling with Differential Equation, which corresponds to continuous time diffusion models. Specifically, Carson talked about how we can construct diffusion models in the limit of infinite number of steps, and how we can define, or how we can describe these forward and reverse processes using Stochastic Differential Equations, or STEs. He also talked about Probability Flow, Ordinary Differential Equations, or ODEs, which describe a deterministic mapping between noise distribution and data distribution. The nice thing about working with the stochastic differential equations, or ordinary differential equations, is that we can actually use the same training that was used for training discrete time diffusion models in the previous slide. However, at the test time, we are free to choose different discretization, or different OD or ST solvers that have been studied widely in different areas of science. And this allows us basically to change the sampling time by using, for example, OD or ST solvers that don't require a lot of functional evaluations. In the third part, Ruqi talked about advanced topics in diffusion models. She mostly focused on accelerating sampling from diffusion models, and she studied this from three different perspectives, including how we can define forward processes that accelerate sampling from diffusion models, how we can come up with better reverse processes, or how we can come up with better denoising models that allows us to accelerate sampling from diffusion models. Beyond that, she also talked about how we can scale up diffusion models to generate high resolution images in conditional and unconditional setting, and she specifically talked about cascaded models and guided diffusion models that are heavily used in the current state-of-the-art image-to-text diffusion models such as Imagine or 32. After talking about fundamental topics, all three of us talked about various computer vision applications that have been recently proposed, and mostly applications that rely on diffusion models at their core. So now that we know about diffusion models and we know how we can use these models in practical applications, let's talk about some open problems. I do hope that now we could make you interested in this topic. And now that you know some fundamentals in this area, maybe you can think about open problems that exist in this space, and together we can tackle some of these. The first set of problems I want to mention are more on the technical side, and later I will talk about more applied questions that arise in practice. So, if you remember at the beginning of the talk, Ross Carson and I talked about how diffusion models can be considered as a special form of MEAEs, or continuous time normalizing flows. But we exactly don't know why diffusion models do much better than MEAs and continuous time normalizing flows. If you can understand this, maybe we can take the lessons learned from diffusion models and why they do so much better than MEAs and continuous time normalizing flows in order to improve these frameworks, meaning we can maybe use the lessons learned from diffusion models to improve MEAs or normalizing flows. Even though there has been a tremendous progress in the community for accelerating sampling from diffusion models, we can still do, in the best case scenario, we can still do accelerated sampling using 4 to 10 steps on small databases such as Sci-part 10. However, the main question remains on how we can get a one-step samplers for diffusion models. And this can be very crucial for interactive applications where a user interacts with the diffusion model and this, we can reduce the latency that usually users observe when they are using generative models. So one of the existing problems is how to define one-step samplers. And part of the solution might be to come up with a better diffusion process that are intrinsically faster to generate sample from. Diffusion models, very similar to VAEs or GANs, can be considered as latent variable models, but their latent space is very different. For example, GANs, we know in their latent space often have semantic meaning and using latent space manipulations, we can actually come up with image editing or image manipulation frameworks. But in diffusion models, the latent space does not have semantics, and it's very tricky to come up with latent space semantic manipulation diffusion models. So part of the problem here is how can we define semantically meaningful latent space for diffusion models that allow us to do semantic manipulations. In this talk, we mostly focus on generative applications, but one open problem is how we can use diffusion models for discriminative applications. For example, one way of using diffusion models might be for representation learning, and we might be able to tackle high-level tasks such as image classification versus low-level tasks such as semantic image segmentation. And these two may require different traits of when we're trying to use diffusion models to have this. Another group of applications that may benefit from diffusion models is uncertainty estimation. One question is how can we use diffusion models to do uncertainty estimation in downstream discriminative applications. And finally, one question that remains open is how we can define joint discriminator generator models that not only class-wide images or input, they also can generate similar inputs. So the community mostly have been using unit architectures for modeling a score model in the score function in diffusion models, but one question is whether we can go beyond units and come up with better architectures for diffusion models. One specific open area is how we can feed time input or other conditioning into diffusion models, and how we can potentially improve the sampling efficiency, or how we can reduce the latency of sampling from diffusion models using better network design. So far in this talk, we mostly focused on image generation, but we may be interested in generating other types of data. For example, 3D data that has different forms of representation. For example, it can be represented by stance function, meshes, boxes, or volumetric representation. Or we might be interested in generating video text graph, which have their own characteristics. And given these characteristics, we actually may need to come up with specific diffusion models for these particular modalities. One area of research is to do composition and control book generation. And this will allow us to go beyond images and be able to generate larger scenes that are composed of book books, for example, objects. And also this will, as a technical problem, will allow us to have fine grained control in generation. So, one interesting open problem is how we can achieve composition and control book generation using diffusion models. And finally, I think, if we look back to the vision community and the type of the problems that we solved in the past few years, we see that most of the applications try to solve, most of the applications that rely on generative models, they try to solve, most of the time solve with generative address on networks. So, maybe it's time for us to start revisiting those applications and see whether they can benefit from the nice properties that diffusion models have. So, one open question is which applications will benefit most from diffusion models, given that we have such amazing, strong tools. This brings me to the final slide. I want to say thank you for being with us today. This is a very long video, and I do hope that we could provide some useful and fundamental background on diffusion models and how they are used in practice. All three of us are active on Twitter. If you are interested in knowing about follow up works that we build on top of diffusion models, please make sure that you follow us. And lastly, I want to mention that all the content on this video, including slides, will be available on this website. If you happen to enjoy this video, I would like to ask you to share this video with your colleagues and collaborators, and hopefully together, we can come with more people to start looking into diffusion models and applying them to various interesting applications. Thanks a lot.",
        "output": "### Part 1: Denoising Diffusion Probabilistic Models\n\n#### Forward Diffusion Process\nThe forward diffusion process starts with data (e.g., an image) and adds noise step by step. At each step, a normal distribution (denoted by Q) is used to generate a noisy version of the image, depending on the previous image. The joint distribution of all samples in the trajectory can be expressed as the product of conditionals formed at each step.\n\nThe diffusion kernel can be directly sampled from by calculating a scalar alpha bar, which depends on the parameters of the noise schedule (beta values). The noise schedule is designed to make alpha bar converge to zero at the final step, resulting in the diffusion kernel converging to a standard normal distribution.\n\n#### Marginal Diffused Data Distribution\nThe marginal diffused data distribution (Q_xt) can be obtained by first forming the joint distribution of clean data (X_0) and diffused data (X_t), and then marginalizing out X_0. The diffusion channel effectively applies a Gaussian convolution to the data distribution, making it smoother over time.\n\nWe can sample from the marginal data distribution at a specific step by sampling from training data X_0 and diffusing it to X_t using the diffusion kernel.\n\n#### Parametric Reverse Denoising Process\nThe reverse denoising process starts from noise with a standard normal distribution and generates data by denoising one step at a time. A parametric denoising distribution is used, approximated by a normal distribution. The mean parameter is a deep neural network that takes a noisy image X_t and predicts the mean of the less noisy image X_t_minus_1.\n\n#### Training Denoising Models\nThe denoising model can be trained using a variational upper bound, similar to those in variational autoencoders (VAEs). The training objective consists of optimizing the marginal likelihood of training data under the parametric model. This can be achieved by minimizing the expectation over training data distributions and samples drawn from the forward process."
    },
    {
        "instruction": "<|SYSTEM|>You are a scholars that is taking markdown notes about some video, you would be giving some written scripts for the task.\nYou have finished writing the notes, and you want to extract the title from the notes.\nThe title should only be the name of the technology or algorithm, no extra words needed.\nPlease write the notes in markdown format. No need to write \"Title:\" for the Title. Make sure the title is within 10 words.\nHere is an example:\n## Hidden Markov Model\n<|ASSISTANT|>## ",
        "input": "<|USER|>### Part 1: Denoising Diffusion Probabilistic Models\n\n#### Forward Diffusion Process\nThe forward diffusion process starts with data (e.g., an image) and adds noise step by step. At each step, a normal distribution (denoted by Q) is used to generate a noisy version of the image, depending on the previous image. The joint distribution of all samples in the trajectory can be expressed as the product of conditionals formed at each step.\n\nThe diffusion kernel can be directly sampled from by calculating a scalar alpha bar, which depends on the parameters of the noise schedule (beta values). The noise schedule is designed to make alpha bar converge to zero at the final step, resulting in the diffusion kernel converging to a standard normal distribution.\n\n#### Marginal Diffused Data Distribution\nThe marginal diffused data distribution (Q_xt) can be obtained by first forming the joint distribution of clean data (X_0) and diffused data (X_t), and then marginalizing out X_0. The diffusion channel effectively applies a Gaussian convolution to the data distribution, making it smoother over time.\n\nWe can sample from the marginal data distribution at a specific step by sampling from training data X_0 and diffusing it to X_t using the diffusion kernel.\n\n#### Parametric Reverse Denoising Process\nThe reverse denoising process starts from noise with a standard normal distribution and generates data by denoising one step at a time. A parametric denoising distribution is used, approximated by a normal distribution. The mean parameter is a deep neural network that takes a noisy image X_t and predicts the mean of the less noisy image X_t_minus_1.\n\n#### Training Denoising Models\nThe denoising model can be trained using a variational upper bound, similar to those in variational autoencoders (VAEs). The training objective consists of optimizing the marginal likelihood of training data under the parametric model. This can be achieved by minimizing the expectation over training data distributions and samples drawn from the forward process.",
        "output": "Denoising Diffusion Probabilistic Models"
    },
    {
        "instruction": "<|SYSTEM|>You are a scholars that is taking markdown notes about some video with Wiki style, you would be giving some written scripts for the task.\nAnd summarize each paragraph with a title.\nRemember to keep the technology or algorithm name, very important.\nPlease write the notes in markdown format.\nHere is an example:\n### Definiation\nA deadlock is a situation in which two computer programs sharing the same resource are effectively preventing each other from accessing the resource, resulting in both programs ceasing to function. \nThe earliest computer operating systems ran only one program at a time.\n\n### What it have to do with Relationsal Database ManagementSystem\nIn a database, a deadlock is an unwanted situation in which two or more transactions are waiting indefinitely for one another to give up locks. \nDeadlock is said to be one of the most feared complications in DBMS as it brings the whole system to a Halt. \nWhen you tries to insert or update data on database, sometime your record need to depend on the row record.  \n<|ASSISTANT|>#### ",
        "input": "<|USER|> Great. Yeah, so hi, I'm Hyunjin Chung. I'm a PhD student at Bioimaging and Signal Processing and Learning Lab at KAIST Korea. I'm honored to give a talk in such a prestigious group in the Netherlands. And so thank you very much for having me here. So today I'll give a talk mostly about diffusion models and how to use them to solve inverse problems in imaging, focusing on a few recent papers that I wrote. So specifically, it will be focused on Bayesian inference in the context of inverse problem solving. So I believe it will be of quite a relevance to you. And I'm pretty sure that most of you are already familiar with diffusion models, but some of you may not be familiar with inverse problems. So and since we have a rather small group, feel free to speak up with your microphone. I'm very sure that I'm going to miss some chats, so I hope the talk will be as interactive as possible. Yeah, so the talk will be structured in a bit of an odd way because we're going to first talk about two closely related work in a reverse order. So the first part of the talk will be about a paper named DPS, accepted to ICLR recently. Then we will move backwards and talk about MCG, which was supposed to be the main content of the talk today. And finally, if time allows, I would also like to briefly touch the most recent work that applies these methods to more advanced problems called blind inverse problems. So I'll just briefly cover the basic concepts of diffusion models. I don't want to bore you too much, but I'll just speak in a briefly in a score perspective. So what we do in diffusion models is we try to estimate the gradient of the lock density with the neural network S of theta, which points to the high density modes of the distribution. In order to circumvent the intractable explicit score matching, you train your network with denoising score matching, which boils down to essentially training a residual denoiser. Once that is done, you can use the train score functions to sample from the distribution P of X with, for example, Langevin MCMC. And another interesting view is that one can view the data noising process as some linear Ford se and the data generating process of this as the corresponding reverse stochastic differential equation where the drift function is governed by the score function. Hence, when we want to sample data, you can discretize the reverse SD and numerically solve it using the pre trained score function. On the other hand, here we are interested in solving inverse problems. In the inverse problem setting, our aim is to recover the ground truth X from the noisy measurements Y obtained through some integral imaging system A. And there will be some pollution of noise and here, a sorry. And the problem is naturally opposed, which means that there exists infinitely many solutions to this problem. Meaning that given this noisy image, noisy blurred image of the leopard, there could exist a lot of different feasible answers to to the actual clear clean image. So, in order to correctly specify which one of the solutions is the is the one that we want. We need to specify the prior of the data distribution. And in other words, how the images would actually look like in real world. Examples of such inverse problems include in painting and deep learning and in the context of low level vision. They're also called image restoration problems. And in the context of biomedical imaging, there could be compressed sensing MRI sparse sparse, few CT and so on. So we'll go over how we can solve such inverse problems with diffusion models. So let's first consider the following measurements model. Now, given why what we want is to sample from the posterior distribution P of X given Y. And using Bayes rule, it's straightforward to see that the gradient of the lock posterior is the sum of gradient of the likelihood plus the score function. Here gradient of the likelihood in red gives us the information about the measurement process. And this whole function is what we've already estimated with a neural network. So once you know that the likelihood function P of Y given X is in most cases known and in 95% of the cases, it can be modeled relatively well with the Gaussian. And among the rest of the 5%, 4% is Poisson, meaning that the likelihood function is often known a priority. So hence, in the perspective of Bayesian reconstruction, all we have to do when posterior sampling is desirable is to specify the correct prior distribution. Then the measurement processes hand wavy, it's known so we can sample from the posterior. So let's consider the case where we model the data distribution with the diffusion model. From the score or the SDE perspective, recall that the generative process is defined by this general form of reverse SDE. So in order to do unconditional sampling from the prior distribution, we can use the following discretization steps of the reverse SDE. So for simplicity, I denoted Euler-Maruyama discretization here, but other forms of sampling can of course be used, such as ancestral sampling of DDPMs or hybrid methods that incorporate MCMC chains within the numerical integration solvers. They're called predictor-corrector solvers in score SDE paper. Moreover, when we want to do posterior sampling, we can use what we've derived from the Bayes rule and incorporate the gradient of the walk likelihood. So here comes the problem. Can I ask a question? I'm sorry, I'm decently new to diffusion models, so maybe there is something I didn't understand. I don't understand the sign, why there is a minus sign in front of the, even in the original paper, why there is a minus sign in front of the gradient of the score. Oh, so you're asking, how did the reverse SDE come along here, right? Well, I'm saying that with that minus sign, it will do the opposite of going to the high probability regions. It will go away from it. So am I missing something? Oh, yeah. So intuitively, if you want to sample from the high density modes of the distribution, you would kind of do gradient assets with it. And yeah, there's a caveat here because the differential DT here is a time running backwards. So, yeah. But then the sign of F is wrong because the sign of F should have the same sign of G, doesn't it? Actually, no. So like the derivation process of this reverse SDE comes from a thing called Anderson's theorem. So if you plug in this general form of forward SDE, then there's a theorem that states that this reverse SDE is the correct form. I understand. But so if there isn't, let's say that there is no score, so there is no goal, then if you have a, let's say, an Olliston-Ullenberg process, the forward and the reverse dynamics are the same. So you're saying that if DT is running backwards, then F should be negative. If you have a stationary process like an Olliston-Ullenberg, regardless whether you run it forward or backward, you have the same dynamics. Therefore, you are not going to flip the sign of F if you change the time. So let's say if in an Olliston-Ullenberg process, F induces the mean reversion. So if you actually flip the sign, you will have the process going away from the mean, which is not the reverse time process. The reverse time process wants to go back to the mean exactly like the forward process. Right, right. I'm also a bit confused myself. Because everything works, I think, I don't know. So again, I could be misinterpreting something. Because I do see this in papers, but I just... Again, if it's an Olliston-Ullenberg process, the forward and reverse dynamics, assuming score 0, should be the same. So you cannot flip the sign. Otherwise, you get the opposite, you get the diverging process. And if you assume the generative direction to be forward, then the sign of the score should be plus, not minus. But of course, it would become minus if you are actually considering that going backward. But maybe we can discuss this offline. And also, again, it could be that I'm missing some details of the notations for which things will match up. I'm not sure if, especially in the case of Ornstein-Ullenberg process, is the score function 0 there? Well, if the forward process is Olliston-Ullenberg, then f of x of t is just minus alpha x. And that would be true both in the forward and the backward. Because again, it's time invariant. If you flip, you don't change anything. The score happens in order to enforce the terminal condition that the process has to fit the data. So in this sense, it's nothing to do with the Ornstein-Ullenberg process. It's just the starting point of the process, which if then if you want to reverse, it will actually go back to there. Yeah, I think we should come back to this later. I think it's better to discuss after. Yeah, exactly. So coming back to where we were, I think it was right here. So I was talking about how we could switch from prior sampling to posterior sampling by just plugging in this Bayes rule. And here I said that care must be taken here because the gradient of the log likelihood is in fact intractable. And note that this is different from my claim earlier that the likelihood function is in most cases known. And this arises from the fact that there are noisy xi's here rather than x0. So let us dive in to closely examine what I mean by this. To see why this is the case. I'm sorry that I'm switching notations with int. For i's I'm denoting discretized things. And t, I'm just pointing at some general continuous time instantiations. So here consider the following probabilistic graph in the context of diffusion models. So we know two conditional distributions p of y given x0 and p of x t given x0. And for now, let's assume that the first one is the measurement distribution is given as typically Gaussian. So and the second word, the four distribution of the diffusion is also Gaussian. However, the reverse distribution p of x0 given x t shown with blue dotted line is intractable in general. So hence, p of y given x t is intractable because we have no information about this blue dotted line. So in our work, we aim to approximate the intractable distribution. The first key comes from the factorization. Since x0 is conditionally independent on y and x t, we can factor the integrand as follows. Where the former term is what we know and the latter term is what we partially know. And by partially known, I mean that we know how to obtain the posterior mean of the distribution, which is given by the Tweedie's formula. Note that Tweedie's formula is used widely in diffusion model context, as it states that the posterior mean or the denoised estimate can always be achieved when we know the so-called blurred score function. That is the score function of the intermediate noisy variables x t. Here we see that we can plug in Tweedie denoising to achieve some posterior mean of the distribution in the context of the DDPMs. And to fully enjoy the effectiveness of Tweedie's formula and because leaving the expectation outside is intractable, let us push the expectation inside for now. When we do that, by the Tweedie's formula proposition, we have a fully tractable distribution where the condition is now given by x0 hat, the denoised estimate from x t. Now, since we proposed an approximation here, it is important that we quantify the approximation bounds. And for that, we have a theorem that states the approximation error, or the so-called Jensen gap, induced by pushing the expectation inside the function. And we show that this approximation error is upper bounded by this constant with three constituents. So the latter two parts are usually bounded in most practical situations. And the interesting part here is the first one where we see that when sigma goes to zero, the entire constant goes to zero. This is useful because it states that in cases where we have negligible measurement noise, the approximation will be tight. However, in practice, even when we have high measurement noise, the method also works very well. So just summing everything up, thanks to theorem one, we can achieve what we call diffusion posterior sampling, or DPS in short. We started with standard Bayes' rule in the context of diffusion models. We can apply theorem one to get the approximation. I lost him. I don't hear anything anymore. Anybody? Oh, okay. Do you hear me now? Yeah. Yeah. Sorry. Where did the... I don't have any sound anymore. Yeah, now we hear. Yeah, now it's back. But we cannot see the screen. Can you please try to share the screen again? I can see a screen. I can see the screen, the Gaussian and Poisson. Yeah, me too. Yeah. I don't. Can you try to just stop sharing and re-share it again? Yeah, sure. Thanks. So is this where the connection was disconnected? Yeah, you were just starting about the Gaussian and Poisson. Yeah. Yeah. So I was saying that these gradients can be analytically computed because these functional forms are already known. So we see that for Gaussian, we are essentially performing gradient descent that minimizes the squared L2 norm of the residual. I cannot see the slides. I can't see the screen. Yeah, neither. No. They just dropped away in the halfway through your sentence. Okay, so let's try to re-share. Yeah, now I can see that. Yeah, me too. Thanks. So does this work? Yes. Yes, it works. Seems like it. Yeah, so, so yeah, I was saying that for Gaussian measurement models, we're trying to do gradient descent that minimizes the squared L2 norm of the residual. And for Poisson measurements, we're minimizing the squared weighted norm of the residual. So incorporating ancestral sampling for DDPMs, we have our algorithm of DPS where we can derive separate algorithms according to the measurement model in hand. Note that line seven is where we, where DPS takes place. If we were to remove line seven, we would simply be sampling from the prior distribution, the usual diffusion models, what they do. And hence, our algorithm is just very simple modification of the usual DDPM sampling. When we do that, we achieve these results. So, since our method is not dependent on the measurement model, we can apply the same score function to various problems. For example, this is the result of applying our method to super resolution that are contaminated with Gaussian noise. We can also apply our method to noisy inpainting. And this is the case where 92% of pixels are blocked out in a 256 by 256 image. This is noisy Gaussian deep learning. We can do noisy motion deep learning. And what's even more, since we're not restricted to the choice of the Ford operator a. For the first time in the context of inverse problem solving with diffusion models, we show that we can also solve nonlinear inverse problems. Such as phase retrieval presented here. And for those of you who do not know what phase retrieval is, it's a problem that tries to estimate the phase and the Fourier domain. And this is a notoriously hard problem because most of the information of the image is actually concentrated on the Fourier phase rather than the Fourier magnitude of an image. And we can also solve problems like nonuniform deep learning, which is another nonlinear inverse problem. Given that the Ford measurement model is actually differential. This is one application that I want to highlight because we actually used a complex neural network that emulates the nonlinear blurring here. And neural network is one of the most nonlinear Ford models that you can imagine. So even when the Ford operator is highly nonlinear, we can see that DPS is capable of recovering the image with high fidelity. So that was it for DPS. I think we can briefly stop to see if any of you have any questions. I have a question. So basically, did you also show, because you can have due to the stochasticity, you can have posterior distribution of the noise samples, right? So do you notice a difference compared for the one you did, for example, this image, do you get some deviations that are meaningful in one sense? Yeah, so I'm sorry that I don't have examples here, but yeah, you do see quite a bit of stochasticity, especially when the degradation is heavy as, for example, the example here, blurring is heavy. So the posterior samples have high standard deviation in terms of reconstructions. I have another question. I don't know if you're explaining that maybe I missed that detail, but you also mentioned that reconstruction is possible if you have a prior, right? Right. But how do you choose the prior? Do you learn it or is it something that you'd have to... Oh yeah. So the thing that we're proposing here is that we're using the diffusion prior, right? So we're using this generative prior and the prior is learned through the usual training process of diffusion models. So whether it be score matching or epsilon matching or whatever, we only, what we only need in the sampling process is the pre-trained score function S theta. So all of the examples that I show you here are generated by pre-trained models available online. So specifically we use the OpenAI model and we don't need any fine tuning for any of these problems. We can just plug it into the solver and that will act as the prior of the distribution. Okay. So the prior is trained on the data that you use it on. So you use the same version of the model pre-trained on different data. Yeah. Thanks. So how this method would differ from the paper also from Song et al. for inverse problems in medical imaging? So is there a likely difference? They are different and that will be covered more in detail in the second section where I explain the nearest paper. Yeah. So if you could summarize, for example, in a few words, because the main paper from Song et al. you can also do inverse problems straightforward. I mean, you can do the painting. What would be like the main contribution is this? Yes. Non-linear approach to inverse problems or what would be the main difference? So it's two things. I guess I'll try to explain it later. So you're mentioning the original paper of Song et al. at ICLR 2021. And there was a paper that you mentioned that tackled medical imaging in the next year of the same author. They solved inverse problems by using a so-called projection approach, which means that you're directly replacing what you have as the measurements and you're keeping only the rest of it. So this is the visualization of such projection process. And this projection process works for certain inverse problems, for example, in painting and for in painting, when you have like a small degradation, for example, the mask is small. Or the blocked out pixels are not dominant. In those cases, projection approaches work fine. But for example, when you have a large mask, for example, a 128 by 128 mask here, when you apply Song's method, projection type approaches tend to fail dramatically. This is also relevant to the case of medical imaging, where you have high degradations. These projection based approaches will typically fail. Whereas DPS that I proposed in the last section and the MCG that I will explain soon after does not have this property. And the methodological main difference is that the Song et al. use projection type approaches, whereas MCG or DPS uses gradient type approaches. So it's a smoother transition towards the that adapts to the measurement process. That was pretty nice. That's pretty clear. Thanks. Okay. Should I should I move on to the next section? Okay. Yeah. Thanks. Yeah. So. The next paper that I'm going to talk about is is actually a paper that came before DPS. But I explained DPS first because it really uses the same gradient. I would say similar but almost the same gradient method with DPS. But at the time of development of MCG paper we did not really understand how mathematically the, the, the solvers would be derived. So here in this paper we focused more on the explanation in the geometric context of diffusion models. So I hope that this also helps the understanding of of the underlying intuitive things about these gradient based methods. So in order to understand the paper, let us review some of the important properties of high dimensional Gaussian random variables. And specifically for very high dimensional Gaussian random variables, while the mode of the PDF may be near the mean of the distribution, the measure is actually concentrated in the annulus that is distance from the center. This is the reason why when we add fixed variance Gaussian noise to an image. You never really see a clean image. Even when the highest probability of a Gaussian noise would be zero noise. We always get images with very similar noise levels. Now, extending that, let us think of a random variable why that is corrupted with Gaussian noise by adding some noise. So we can you maybe rephrase that. I mean, that thing that's interesting. The slides before. So the concentration of Gaussian measure, right? So this is a Gaussian annulus theorem, right? And then the effect that we see, what you said is that we don't see like, so we see the same effect, right? Or what do you mean? So because of the Gaussian annulus theorem, what we see when we add Gaussian noise to images with, let's say, fixed variance Gaussian noise to an image. Like, I guess the most, like the highest probability instantiation of this Gaussian distribution would be no noise, right? Zeros. But that never happens. So if you add some random Gaussian noise to an image, you will always see some very similar noise level images that are corrupted in a very similar way. So that was what I was talking about. Yeah. So it's also due to the convolution, right? You can observe this as a convolution. So this is kind of related to what I was talking about before. So let's say you have like a linear space of X. And what you would do is you add some Gaussian noise to that random variable X. Because this is a convolution, we can show that the marginal distribution P of Y is actually a, the measure actually concentrates on the hyper cylinder that is distant from the center of PX. So this is kind of intuitive reason why the noisy images with the fixed variance have similar looking structure. So in this work, we propose to interpret diffusion models using some assumptions about the data manifold. So the usual manifold hypothesis states that the data lives in a low dimensional manifold with much smaller dimensionality than the ambient space. And in this work, we add an additional strong assumption that the manifold, the central data manifold is locally linear. And in order to leverage, and this is because we want to leverage the results from the concentration of Gaussian measure. And specifically, when we assume that our data manifold is locally linear, what we can show is that the geometry of diffusion model is given by the successive manifolds, where there exists a clean data manifold in the center. And there are continuously many noisy manifolds where the noisy samples reside. So these are the manifolds of the noisy images. Extending that to a continuous limit, we propose that diffusion model is an onion because when we push T to infinity, the distribution actually becomes a pure Gaussian. And since the measure of a high dimensional Gaussian resides in a Gaussian hypersphere, the intermediate manifolds are covers of the central manifolds that interpolates between these two. Therefore, diffusion process would correspond to these red arrows and the reverse diffusion would correspond to the blue arrows. And furthermore, when we are considering the case where we are trying to solve inverse problems with diffusion, we are left with the figure on the right. Here we are not only trying to find points in the central manifold M, but we're trying to find the intersection between the manifold M and the line y equals hx. So I've already said this earlier, but let's examine some of the representative methods that solve inverse problems using diffusion. These methods, Song et al and Choi et al, are type of projection based approach since they directly replace the part that is known and keep only the unknown part. Another way to impose data consistency in the Bayesian framework is to incorporate likelihood. In other words, try to minimize the residual by gradient descent. However, with diffusion models, if we were to use naive gradient descent with a noisy residual, we will acquire no meaningful gradient as the residual between hxi and y will only be focused on the Gaussian noise part of the residual. Now, as we saw in diffusion posterior sampling, one can actually approximate the correct likelihood by switching to 3D denoised estimate x0 hat here. Visually and intuitively, we see why this may help. This is because after denoising, the residual is more likely to focus on the structural differences between hx0 hat and y. In practice, using this gradient step over the projections work much, much better. And you can see the kind of the resulting differences when we use projection type approach and the gradient approaches. So in the paper, we properly analyze why this would be the case, leveraging the geometric understanding of diffusion models. So first, under our assumptions, we show that 3D denoising corresponds to an orthogonal projection to the data manifold. However, note that we have to consider two components of the data manifold. The orthogonal one that is related to the noise components and the tangential one that is related to the data fidelity. So score function alone cannot handle the data fidelity. And here comes our theorem, which states that the proposed manifold constraint gradient term points to a direction that is tangential to the central data manifold. And therefore, our solution will move closer to y equals hx every time we apply this MCG step. Another important aspect of this is that since MCG lets the sample move on the data manifold, whereas projections move the sample off the data manifold, because projections are hard constraints that try to make y minus hx equals zero. Note that MCG will not do that. It will move on the data manifolds in the direction tangential to the manifold. So those falling off the manifold will not happen. So here note that the score function that score functions that are pre-trained were not trained with samples that are off the data manifold. So they will have been they were trained with the samples that were on the noisy data manifolds. So it will not be able to properly denoise samples that are off the data manifold. So by using projections, the intermediate error will probably accumulate. And we conjecture that this is the reason why projection based approaches often fail to produce a good sample. But however, as I said, in the time we wrote MCG, there was a gap between theory and theory in practice, because even with the MCG steps, we had a hard time trying to solve inverse problems. So we had to resort to some projections within the denoising and the gradient steps. But in DPS, we managed to achieve a general solver that only uses these gradient steps. Now for the Gaussian case, the only difference between MCG and DPS is that we do not use projections for DPS. Leveraging the geometric viewpoint of diffusion models, we can think that DPS is superior to MCG specifically because we avoid such projections, as projections here can potentially throw samples off the data manifold. Hence, DPS will typically be more stable by avoiding such throw offs. And by the way, this is just an illustration. It doesn't really guarantee anything that our solver will always stay on the noisy data manifolds. Yeah, so that was about it for MCG. Any questions up until now? I just have a question, but more like maybe a very simple question. At the beginning of the slides, when you were explaining the, I think it's, no, the next one, two after this one. And then, but yeah, this one, the previous one. How do you define this K? So N, what is N and what is K? I would have to look for the definition of N and K here. So one of them. Yeah. So I would. From my memory, N is the dimension of the ambient space and K is the dimension of the locally linear low dimensional dimensionality of the data manifold. I guess you're assuming that these, so the brown, the mean is basically just the mean of each intermediate distribution, right? So is it mean of the diffuse or the noise distribution, right? And then this, I'm just wondering, shouldn't start going from a big standard deviation to a small standard deviation? Because basically when you do the de-noising, you start with a high variance and then you start de-noising and the variance slowly decreases. That's just something I was thinking that if that's not the case, so I'm missing out something. Can you repeat the question again? I'm not sure if I understood it correctly. Yeah. So the point of this is to interpret what's going on, like how the noisy samples behave in the data manifold through the trajectory. So when you do the de-noising, so you start with the isotropic Gaussian, which is the highest entropy you can have. So you start with the maximum variance that you have and then slowly you start removing the noise. And by doing that, interactively, the variance at each intermediate step goes down. So what I was expecting is to have this behavior when you have the mean and then the variance is large at the beginning and then starts decreasing. Because ideally you would reduce the variance, but I don't know if I'm missing something there. Oh, but I think it's like what you're saying is kind of different from what this slide is saying. So I'm not really stating anything about de-noising anything because like the flow here is that we're first assuming that there exists a central data manifold M here. And then we're trying to visualize how the manifolds at the noisy manifold of the noisy images would look like if there exists the central data manifold M. And I'm claiming that these noisy data manifolds will be covers of the central data manifold where the distance between from the central data manifold will be defined by this constant value. So it's really not about starting from high variance Gaussian noise and reducing it to anything. We're starting from a clean image and then we're defining the manifolds of these noisy images. Okay, I would have to look at the paper, I guess. Yeah, thanks. Yeah. I guess we just tell me if I'm... It's a good cliffhanger. It's frozen again. Do you hear me okay? Yeah, I know, yes. Yeah, so I think it's about 48 minutes so I don't think we have to go over the final section. So I'll just try to... We just, we cannot see you or the slides anymore if you can do the same thing as before. Stop sharing and start again. Yeah. Well, the talk is basically done so. Okay. That was the last slide if we don't cover the last section. I'm not sure if this is the correct way of saying thank you very much in Dutch. Maybe you should put it in Italian. Yeah, so that is the right way to say thank you. Thank you too.",
        "output": "### Introduction\n- Speaker: Hyunjin Chung, PhD student at Bioimaging and Signal Processing and Learning Lab at KAIST Korea\n- Topic: Diffusion models and solving inverse problems in imaging using Bayesian inference\n\n### Diffusion Models\n- Estimate the gradient of the log density with a neural network S of theta\n- Train the network with denoising score matching\n- Use the trained score functions to sample from the distribution P of X with Langevin MCMC\n- Data denoising and data generating processes can be viewed through stochastic differential equations (SDEs) with drift functions governed by the score function\n\n### Inverse Problems\n- Recover the ground truth X from noisy measurements Y through an imaging system A\n- Problem is ill-posed – there are infinitely many solutions\n- Require specifying a prior of the data distribution to find the correct solution\n- Examples: inpainting, deblurring, compressed sensing MRI, sparse-view CT\n\n### Diffusion Posterior Sampling (DPS)\n- Combines diffusion models and Bayesian inference to solve inverse problems\n- Can solve Gaussian, Poisson, and non-linear inverse problems\n- Results in high fidelity reconstructions\n- DPS outperforms projection-based approaches because it stays on the data manifold, while projections move samples off the manifold\n\n### Manifold Constrained Gradient (MCG)\n- Proposes a geometric interpretation of diffusion models\n  - Data resides in low-dimensional manifold with locally linear structure\n  - Intermediate manifolds are covers of the central data manifold\n- MCG Theorem: manifold constrained gradient term points to a direction tangential to the central data manifold, resulting in a closer approximation to the intersection between the manifold M and the line y = hx\n- MCG is stable and avoids falling off the data manifold\n\n### Comparison of DPS and MCG\n- DPS uses gradient-based approach, while MCG uses projection-based approach\n- DPS is more stable by avoiding off-manifold projections\n- Both methods can achieve high-quality results for various inverse problems"
    },
    {
        "instruction": "<|SYSTEM|>You are a scholars that is taking markdown notes about some video, you would be giving some written scripts for the task.\nYou have finished writing the notes, and you want to extract the title from the notes.\nThe title should only be the name of the technology or algorithm, no extra words needed.\nPlease write the notes in markdown format. No need to write \"Title:\" for the Title. Make sure the title is within 10 words.\nHere is an example:\n## Hidden Markov Model\n<|ASSISTANT|>## ",
        "input": "<|USER|>### Introduction\n- Speaker: Hyunjin Chung, PhD student at Bioimaging and Signal Processing and Learning Lab at KAIST Korea\n- Topic: Diffusion models and solving inverse problems in imaging using Bayesian inference\n\n### Diffusion Models\n- Estimate the gradient of the log density with a neural network S of theta\n- Train the network with denoising score matching\n- Use the trained score functions to sample from the distribution P of X with Langevin MCMC\n- Data denoising and data generating processes can be viewed through stochastic differential equations (SDEs) with drift functions governed by the score function\n\n### Inverse Problems\n- Recover the ground truth X from noisy measurements Y through an imaging system A\n- Problem is ill-posed – there are infinitely many solutions\n- Require specifying a prior of the data distribution to find the correct solution\n- Examples: inpainting, deblurring, compressed sensing MRI, sparse-view CT\n\n### Diffusion Posterior Sampling (DPS)\n- Combines diffusion models and Bayesian inference to solve inverse problems\n- Can solve Gaussian, Poisson, and non-linear inverse problems\n- Results in high fidelity reconstructions\n- DPS outperforms projection-based approaches because it stays on the data manifold, while projections move samples off the manifold\n\n### Manifold Constrained Gradient (MCG)\n- Proposes a geometric interpretation of diffusion models\n  - Data resides in low-dimensional manifold with locally linear structure\n  - Intermediate manifolds are covers of the central data manifold\n- MCG Theorem: manifold constrained gradient term points to a direction tangential to the central data manifold, resulting in a closer approximation to the intersection between the manifold M and the line y = hx\n- MCG is stable and avoids falling off the data manifold\n\n### Comparison of DPS and MCG\n- DPS uses gradient-based approach, while MCG uses projection-based approach\n- DPS is more stable by avoiding off-manifold projections\n- Both methods can achieve high-quality results for various inverse problems",
        "output": "## Diffusion Posterior Sampling"
    },
    {
        "instruction": "<|SYSTEM|>You are a scholars that is taking markdown notes about some video with Wiki style, you would be giving some written scripts for the task.\nAnd summarize each paragraph with a title.\nRemember to keep the technology or algorithm name, very important.\nPlease write the notes in markdown format.\nHere is an example:\n### Definiation\nA deadlock is a situation in which two computer programs sharing the same resource are effectively preventing each other from accessing the resource, resulting in both programs ceasing to function. \nThe earliest computer operating systems ran only one program at a time.\n\n### What it have to do with Relationsal Database ManagementSystem\nIn a database, a deadlock is an unwanted situation in which two or more transactions are waiting indefinitely for one another to give up locks. \nDeadlock is said to be one of the most feared complications in DBMS as it brings the whole system to a Halt. \nWhen you tries to insert or update data on database, sometime your record need to depend on the row record.  \n<|ASSISTANT|>#### ",
        "input": "<|USER|> In this work, we present L2G-NURF, a local-to-global registration method for bundle-adjusting neural radiance fields. It integrates local and global alignment methods to simultaneously reconstruct the neural fields from RGB images and register the camera frames. We first unfold the validation of L2G-NURF and baselines on 2D neural image alignment experiments. We use the image of girl with a pearl earring for the rigid image alignment experiment. As shown here, given color-coded image patches generated with random rigid perturbations, we jointly find the alignment in the top row and learn the neural field of the entire image in the bottom row. We evaluate our proposed method against a bundle-adjusting extension of the naive 2D neural field, dubbed as naive. And another baseline is the current state-of-the-art BARF, which employs a coarse-defined strategy for registration. Alignment with naive leads to ghosting artifacts in the recovered neural image, due to large misalignment, while alignment with BARF improves registration results, but still falls into the suboptimal solutions, and thus struggles with image reconstruction. As L2G-NURF discovers the precise geometric warps of all patches, we can optimize the neural image with high fidelity. We then use the representative images of the cat from ImageNet for the homography image alignment experiment. As shown here, alignment with naive and BARF still falls into the suboptimal solutions. On the other hand, we also evaluate our proposed L2G-NURF against an ablation that builds upon our full model by eliminating the global alignment objective. Note that the ablation is equivalent to a local registration method, and BARF is the chosen representative global registration method. Here, we zoom in to see the artifacts in cat ears. This indicates that global parametric alignment is susceptible to suboptimal solutions if poorly initialized, while local non-parametric alignment can minimize photometric errors, but does not obey the geometric constraint, which results in distorted artifacts. These two image alignment experiments demonstrate the general advantage of local-to-global registration, for both rigid and homography registration for 2D neural fields, which can be easily extended to other geometric transformations. We then investigate the problem of learning 3D neural radiance fields from imperfect camera poses with synthetic object-centric scenes. Here we visualize the expected depth-through-ray compositing at the top, the image synthesis in the middle, and optimized camera poses at the bottom. We evaluate L2G-NURF against a naive extension of the original NURF model that jointly optimizes poses, dubbed as naive, and the coarse-to-fine registration BARF model. We can see that L2G-NURF achieves near-perfect registration and comparable visual quality to the reference NURF-trained underground truth camera poses, while naive suffers from suboptimal registration and results in artifacts. BARF is able to recover a part of the pose misalignment and produce plausible reconstructions. However, it still suffers from blur artifacts like the fog effect around the objects. This fog effect is the consequence of BARF's attempt to reconstruct the scenes with half-baked registration. Here we provide more examples, where the image synthesis is on the left, and the expected depth is on the right. We then investigate the ablation study of eliminating global alignment. The results show that L2G-NURF is not able to recover the result of the analysis. We then investigate the ablation study of eliminating global alignment. The results show that L2G-NURF achieves near-perfect registration, while ablation results in suboptimal registration, which leads to synthesis artifacts. We further investigate the challenging problem of learning 3D neural fields with NURF in real-world scenes where the camera poses are unknown. We can see that L2G-NURF produces high fidelity results, while BARF suffers from blurs, especially on the leaves. We take one more step to experiment with images captured using an iPhone under hierarchical camera poses. The results remarkably demonstrate the advantage of L2G-NURF in registering images captured under large displacements, while BARF exhibits artifacts on the novel views due to unreliable camera pose registration. Here we show the experiment on images captured under sparse views, where the BARF exhibits ghosting on the renderings. Moreover, the difficulty of recovering pose from sparse views prevents SFM from finding accurate poses, which results in broken stripes on the synthesis of reference NURF trained under SFM poses. This further demonstrates the effectiveness of removing the requirement of precomputed camera poses from SFM. We then investigate the ablation study of eliminating global alignment. Here shows that the ablation diverges to incorrect camera poses, which in turn results in particularly poor view synthesis, while the full L2G-NURF model highly agrees with SFM pose that is colored in black and achieves high quality view synthesis. Here we provide more examples, where the image synthesis is on the left, and the expected depth is on the right. The high fidelity visual quality shown here highlights the ability of L2G-NURF to joint registration and reconstruction from scratch.",
        "output": "### L2G-NURF: Local-to-Global Registration Method for Neural Radiance Fields\n\n#### Introduction\nL2G-NURF is a local-to-global registration method used for bundle-adjusting neural radiance fields. This method integrates local and global alignment methods for simultaneous reconstruction of neural fields from RGB images and registration of camera frames.\n\n#### 2D Neural Image Alignment Experiments\nL2G-NURF is validated against the baseline methods Naive and BARF (state-of-the-art).\n\n\n#### Rigid Image Alignment Experiment\nThe experiment uses the image \"Girl with a Pearl Earring\". L2G-NURF optimizes the neural image with high fidelity, whereas Naive and BARF result in suboptimal solutions with artifacts.\n\n#### Homography Image Alignment Experiment\nThe experiment uses the ImageNet's \"Cat\" representative image. Naive and BARF still result in suboptimal solutions.\n\n#### Ablation Study (eliminating global alignment objective)\nAblation demonstrates that elimination of the global alignment objective results in distorted artifacts.\n\n#### 3D Neural Radiance Fields with Synthetic Object-Centric Scenes\nL2G-NURF achieves near-perfect registration and high-quality visual outputs compared to Naive and BARF.\n\n#### Investigating Ablation Study (eliminating global alignment)\nWithout global alignment, L2G-NURF fails to recover results, leading to artifacts.\n\n#### Learning 3D Neural Fields with NURF in Real-World Scenes\nL2G-NURF outperforms BARF, producing high-fidelity results while BARF struggles with blur and artifacts.\n\n#### Experiments with iPhone Captured Images and Hierarchical Camera Poses\nL2G-NURF demonstrates advantages in registering large displacement images, while BARF exhibits artifacts due to unreliable camera pose registration.\n\n#### Experiment with Sparse Views\nL2G-NURF performs well in comparison to BARF and SFM, which struggle with broken stripes and ghosting artifacts in the renderings.\n\n#### Investigating Ablation Study (eliminating global alignment)\nAblation results in poor view synthesis, while full L2G-NURF model agrees with SFM pose and delivers high-quality view synthesis.\n\n#### Conclusion\nL2G-NURF proves to be effective in joint registration and reconstruction of images, delivering high-fidelity visual quality when compared to other methods."
    },
    {
        "instruction": "<|SYSTEM|>You are a scholars that is taking markdown notes about some video, you would be giving some written scripts for the task.\nYou have finished writing the notes, and you want to extract the title from the notes.\nThe title should only be the name of the technology or algorithm, no extra words needed.\nPlease write the notes in markdown format. No need to write \"Title:\" for the Title. Make sure the title is within 10 words.\nHere is an example:\n## Hidden Markov Model\n<|ASSISTANT|>## ",
        "input": "<|USER|>### L2G-NURF: Local-to-Global Registration Method for Neural Radiance Fields\n\n#### Introduction\nL2G-NURF is a local-to-global registration method used for bundle-adjusting neural radiance fields. This method integrates local and global alignment methods for simultaneous reconstruction of neural fields from RGB images and registration of camera frames.\n\n#### 2D Neural Image Alignment Experiments\nL2G-NURF is validated against the baseline methods Naive and BARF (state-of-the-art).\n\n\n#### Rigid Image Alignment Experiment\nThe experiment uses the image \"Girl with a Pearl Earring\". L2G-NURF optimizes the neural image with high fidelity, whereas Naive and BARF result in suboptimal solutions with artifacts.\n\n#### Homography Image Alignment Experiment\nThe experiment uses the ImageNet's \"Cat\" representative image. Naive and BARF still result in suboptimal solutions.\n\n#### Ablation Study (eliminating global alignment objective)\nAblation demonstrates that elimination of the global alignment objective results in distorted artifacts.\n\n#### 3D Neural Radiance Fields with Synthetic Object-Centric Scenes\nL2G-NURF achieves near-perfect registration and high-quality visual outputs compared to Naive and BARF.\n\n#### Investigating Ablation Study (eliminating global alignment)\nWithout global alignment, L2G-NURF fails to recover results, leading to artifacts.\n\n#### Learning 3D Neural Fields with NURF in Real-World Scenes\nL2G-NURF outperforms BARF, producing high-fidelity results while BARF struggles with blur and artifacts.\n\n#### Experiments with iPhone Captured Images and Hierarchical Camera Poses\nL2G-NURF demonstrates advantages in registering large displacement images, while BARF exhibits artifacts due to unreliable camera pose registration.\n\n#### Experiment with Sparse Views\nL2G-NURF performs well in comparison to BARF and SFM, which struggle with broken stripes and ghosting artifacts in the renderings.\n\n#### Investigating Ablation Study (eliminating global alignment)\nAblation results in poor view synthesis, while full L2G-NURF model agrees with SFM pose and delivers high-quality view synthesis.\n\n#### Conclusion\nL2G-NURF proves to be effective in joint registration and reconstruction of images, delivering high-fidelity visual quality when compared to other methods.",
        "output": "## L2G-NURF"
    },
    {
        "instruction": "<|SYSTEM|>You are a scholars that is taking markdown notes about some video with Wiki style, you would be giving some written scripts for the task.\nAnd summarize each paragraph with a title.\nRemember to keep the technology or algorithm name, very important.\nPlease write the notes in markdown format.\nHere is an example:\n### Definiation\nA deadlock is a situation in which two computer programs sharing the same resource are effectively preventing each other from accessing the resource, resulting in both programs ceasing to function. \nThe earliest computer operating systems ran only one program at a time.\n\n### What it have to do with Relationsal Database ManagementSystem\nIn a database, a deadlock is an unwanted situation in which two or more transactions are waiting indefinitely for one another to give up locks. \nDeadlock is said to be one of the most feared complications in DBMS as it brings the whole system to a Halt. \nWhen you tries to insert or update data on database, sometime your record need to depend on the row record.  \n<|ASSISTANT|>#### ",
        "input": "<|USER|> Hello community. Today we are coding PAF, parameter efficient fine tuning. So we're inside the source code and a lot of you ask me, hey, what kind of things can we do with PAF? Now, as you can see here, we have four options. We have a token classification. We have a sequence classification. And we're going to focus here on a PAF model for causal language model or on a PAF model for sequence to sequence language model. And then, of course, there was the question, what methodologies does PAF offer currently? And currently I'm recording at end of March 2023. So you can you see we have four options. We have LORER, the low rank approximation for matrices. We have the ptuning, our prompt encoder. We have prefix tuning and we have prompt tuning. So those are the three methodologies currently implemented. And then there was a question about the target modules. Now, which target modules? Now, if we look at LORER specific, can we do this mapping too? And as you can see here, here are our LLMs that we have the T5, the multilingual T5, GPT-2, Bloom, OPT from Facebook. GPT-J, GPT-Neo, BERT-Robertor, then Elektra. Then here we have, of course, LAMA and other LLMs. Now, a lot of you ask me here about LAMA, this model by Mita, and how to do here on the LAMA model here. LORER. Now, let me show you. And there I would like to introduce you another great YouTube channel by Sam Witiven. Have a look at his video he's doing here, for example, fine tune your own Alpaca 7 billion parameter model. And you see when he looks here at the tokenizer, the model, all and I watched 10 different implementations, all refer to this specific model that they download from Hugging Face, the Decapoda research LAMA, Hugging Face model that has been optimized by somebody we don't know. And it is amazing. I have not seen a single implementation that is not by this particular model. So now. This is the reason I do not show you this, because I do not want to use something. I have no idea who is it that put this on the Internet. What exactly happened in the background? Can I trust those people? I have no information about this. So but Sam tells us that he himself, he looked here at this specific GitHub repository. So it is T.Lewen. This is Eric J. Wong from Stanford. And if you want to do on the LAMA model by Meta AI, you want to do Alpaca LoRa, then from all the implementations, I would highly recommend this to this implementation here. And either you go with the generate py file here. This is simply the inference you do. Or if you want to fine tune it in addition, you go with the fine tune file here. This has by far from all implementation I have seen for Alpaca LoRa based on LAMA, the best coding. Let's have a look. Let's go to generate. You see here. Yes, yes, yes. But you also even he he uses here this the Capoda research. So normally you would go to Meta. You would fill out the form. You would request access to the weights because the weights are proprietary, they're not open source. And you have to give your name, your telephone number, your email address, what you want to do, your publication in the area. And then Meta will decide if he will provide you with the specific parameters for all these models. So I understand why they access here is the resource somebody put on the Internet. But if I don't know it, I would not recommend to do it. But the code implementation. Very, very nice. I would take this one if it has to be LAMA. Then I would definitely recommend here T-loan Alpaca LoRa. Beautiful. So back to our topic. Now, of course, we want to code something. And as you can see, LAMA is just one of the model we have here. So I thought, hey, why not go with a T5 model or why not go with a blue model or why not go with an opt model or a GPT Neo model? All those models are available for us. And of course, we focus today in our video today on LoRa, the low rank adaptation. So beautiful. Here we go for the requirements. Yes, we just do a short install. And as always, we do it here in real time because yes, why not? You should enjoy it, too. So first, what we say, we have a hugging face data set. We have accelerate. We have LoRa lip is a specific library that we need for 8 bit implementation. We have bits and bytes and then we go for the main version of our transformer. And of course, we go there. Get four after. So we are just waiting for the implementation. Then we have a look how much memory we do have available. And we have a look at our cheap, you characteristic data. I'm operating here on a free Google notebook, no plus, no whatever. This is exactly what you can do. So while this loads beautiful. Now, let's have an example. Let's create now a pephed LoRa model. And as I told you, we have four options. And we have two options for today. Either we go for a core language model. We have a task of predicting the token after a specific length sequence of tokens that has been given to us. So we have now one or two sentences. And now the LLM continues with the next word. This is causal language modeling. Or we have here we go. Yeah, everything is done. Build is done. Pef is done. Transformer installed latest version. So what do we have? We have a total of 12 gigs. OK, three, seven, gee. So we have a T4. OK. And we run on CUDA version 12. This is nice. 12 is nice. OK. So, yeah, but we do not accelerate. I don't know if we do accelerate today. So let's have a look at our resources. Just to have everything here in view. So the first is we have a classical model. So we say, hey, auto model for sequence to sequence. Yes, yes. Language model from a pre-trained T5 base model. This is the classical model that we know that we love. Beautiful. And if you want, you can have now a look at this model at each layer at the absolute definitive structure of our transformer model here. So if you want to start here, we're very beginning. Yes, we have a lot of layers. So here we go. So we have a T5 for conditional generation embedding T5 stack, T5 block. Self-attention. Here we go. Here we have our beautiful weight matrices to remember the Q, the K and the V and our output. And then we have a layer normalization when we have a drop out. And then yes, yes, yes, yes. So beautiful. You have here a complete layer architecture of our model. Yes, it's really huge. So and then we can say beautiful. And now we do here have a LoRa configuration file. Now this LoRa configuration file, the first task is easy. What method of PEPF from our four methods do you want? We go LoRa. Second is what you want a causal or do you want a sequence to sequence? We have a T5. So we are based on sequence to sequence. Now for the rank I showed you is eight or 16, whatever you prefer. Sixteen has a higher accuracy than we have here a specific alpha parameter. And as I showed you, we target here our query and value weight matrices. You remember the very beginning here, T5. I showed you if you have T5 or derivations of T5 or here multilingual T5, you have the target modules that you want to apply LoRa to map LoRa to is query and value tensors. Bloom, you have this, stupid Aneo, something else. So beautiful, beautiful, beautiful. And this is it and we can define a dropout. So let's do this. And what is happening now? We create our LoRa configuration file and then it is easy. We say here this is a specific class. Get the PEPF model based on the classical model, the T5 model and our PEPF configuration file. And it is created our PEPF LoRa model. Unbelievable, beautiful. And if you want to know, remember in my last video I showed you about PEPF and LoRa, the explanation that the primary parameters are frozen. So you see here all parameters of the model we have. We have two hundred twenty three point seven million parameters and our trainable parameters are just zero dot eight million parameters. This means we only train zero dot four percent of all trainable parameters of the model. And this model now is configured and we can address it if it's a PEPF LoRa model. Beautiful. Yeah, you can have now, of course, now a look here at the modified layer structure. Let's maybe just have a look at the very first one. And you see here self-attention layer, T5 attention layer. And before we start, you see here, as I told you, we focus now on the query layer and on our value layer that here we have now in the query layer. We have now the dropout and then we have here a linear layer from LoRa and another linear layer from LoRa. So beautiful. So you can have a look at the architecture of the LoRa model now in detail. You know what's going on if you have watched my latest video. Beautiful. Now, if you want to take another example, very easy. So you say again, you take now an MT5 or MT0 large model from Big Science. You do exactly the same. It does not depend on a specific model. If it's in the category of T5, it runs. You have here now auto model sequence to sequence language model from pre-trained. You have your LoRa configuration file. You define the model and you can say what are the trainable parameters in this case. You download, of course, the normal Big Science MT0 large model that has been trained. It's based on a T5, multilingual T5 and the series indicates it has been optimized for zero shot learning. So we download this module and then with our LoRa config file, we generate our paved LoRa model. And while we are waiting that it is downloading the five gigabytes. Yes. Beautiful. Come on. Faster. Oops. Come on, come on, come on. Faster. You can see now that our trainable parameters here and here we have in total one point two billion parameter trainable. We have now here in total four point seven million parameter. This means we are here trainable parameters are just zero to three eight percent of all parameters. So you see, you can do this for almost any model. Now, as I showed you here, this command get path model. This is now an interesting command. So what is the source code of get path model? Now, as I showed you, you take any specific normal LMM model. You have your path configuration file. This was the reason why I explained it to you in my last video. And then it just generates the path model for you. And exactly as you define it to watch target modules, to what weight tensors in the layer architecture of our transformer. It should be applied. And we have now LoRa applied. This is done. Enable LoRa. Beautiful. It is so simple. You want to have a look. This is the code. But now, of course, we are here to do some real coding. So let's start. Let's change to causal language model. So these are all the different models you can use with the causal path language model. And if you want to know here the specific class path model for causal LMM, here you have all the information. Yes, yes, yes. Your instances, updates. Yes. Whatever you generate, your parameter, your mask. Beautiful. So let's go now here. I would recommend you have a look at the blog post from Hugging Face, PEPF. And there here, this is an official Hugging Face, colabresearchgoogle.com. And I open this. I leave you the link, of course, in the description of this video. So let's start. We just installed all our bits and bytes. Beautiful. And then we start to load our model in BNB integer 8. And here we load a model called opt. I think it's Facebook. It's weight in half precision. In float 16, about 13 gigabyte on the hub. And if we load them now in 8 bit, we would require around 7 gigabyte of memory instead. So let's have a look. I don't think we're going to achieve this already here. So what we do, easy. We have here bits and bytes imported. And then we have here our auto model of a causal language model from pre-trained. We take here a model from Hugging Face that's available. And now the beauty is just one line load in 8 bit is true. Device map is auto. And here we go. And now the beauty is going to happen. That we have now. Yes, require my bits and byte, enable model loading. Yes, beautiful. You have now two shards. And as you can see, we have 10 gigabytes and I think the other one is 4 gigabytes. So it's about 13, 14 gigabyte that we now download from this opt model, Facebook model. And we apply now this beautiful 8 bit. And while we wait for the model here to load, I would like to explain to you what does it mean really here if we talk about an 8 bit quantization. So, yes, you can combine the low rank adaptation of weight matrices with an integer 8 quantization to further optimize the memory usage and the speed inference on hardware with specialized instruction for the int8 operation on CPUs and AI accelerators like GPU and TPUs. But quantization is the process of reducing the precision of the weights and the activations of a model. So int8 quantization is a technique that reduces the precision of the weights and activations to 8 bits coming from 32 bits. Quantization is a process that converts the floating point number to a fixed point representation with a lower bit precision. Or if you have an int8 quantization in particular, this refers to the use of an 8 bit integers representation for the weights and the activations. So, however, if we reduce now the precision of your values of your weights in all layers, this can cause a decrease in the model's performance on a specific task. And now there is a beautiful solution to counteract this. And it is that you fine tune a quantized model. So if you have a model, you quantize the model and then you fine tune it. This involves a retraining of the model on the target task with the lower precision weights and activation, thereby slowly adjusting the new model parameters to better accommodate the reduced precision. Let me give you an example. Imagine here we have a digital model that we load here. We are playing a game on a computer and we load here this digital player. And you can either load it in 32 bit precision or you can load it in integer 8 precision. Now, what does this mean? This means, for example, that the thickness of the coding on the bat here is a little bit less, a little bit more. The grip strength has a variation. The elbow length, the model loaded with a little bit different value. The specific weight of the wood is different. The length of the bat is different. The optical vision of the player has a little bit modification. The muscle flexibility here, for example, in the upper torso is different. The rotational impulse is different. The elongation of the torso elongation is different. Maybe even the shoe size is different. Only tiny, tiny bits. But if you take it together in a complete system that interacts and all weights in all layers of our transformer interact, you can see this player maybe with all these little modifications needs a little bit training, five, ten minutes of training to be optimal with his reduced parameters to play the game. And this is exactly what we're going to do here when we say the quantized model that we load, the lower precision model. Fine tuning is an excellent way not just for the fine tuning, but to optimize the model for the fine tuning. So if you keep this picture in mind, I hope you never forget to fine tune a quantized model. Now, to show you the magnitude, what we have, imagine we have a 1000 times 1000 matrix in two dimension and we have float 32 elements. Each requires four bytes, 32 bits per element. So the total memory is 1000 times 1000 times four bytes, four million bytes or 3.8 megabytes. Now, if we quantize this matrix now to integer eight, it is now requires only one byte or eight bits per element of our matrix. So the total memory usage to store this matrix goes down from 381 megabytes to 0.95. So we have a memory reduction of 75 percent. Now, there are now a lot of variations. So just to remind you, an 8-bit binary number like this here is an 8-bit binary number represents, for example, the decimal number 214. And you remember, with 8-bit maximum is 255. Now, 16-bit integer, powerful. It can represent values between zero and 65,000 for unsigned integer. And then we have, of course, float 16. Now, this float 16 representation is now here, the 16-bit binary format of the our number 52736. And you remember, we have one leading bit for the plus minus. Then we have 12345 bit for the exponent and we have 10 bit for the significant or for the mantissa, it's called. So this is the 16-bit binary format of this number. And now you know that with the latest development of TPUs and GPUs, based on the hardware of the chip size, we have now B-float, brain float 16. And there is a significant difference now in this kind of structure. We have also here we exactly like float. We have one sign bit plus minus. But instead of five exponent bits, we have now eight exponent bits. And of course, somewhere the place has to go. And instead of 10 for the mantissa, we have only seven mantissa bits. So you say, OK, and why we change from float 16 to this very specific brain float 16 designation? Now, you have to know that on the chip, when you edge it on the chip, the hardware multiplier scales, the physical size on the chip of a hardware multiplier scales with the square of the mantissa width. So if you have your 10 bit mantissa, you scale with the square. If you only have seven bit mantissa and you scale with the square, you see where the benefits come in. So just to be absolutely sure, the physical size on chip of a B-float 16 multiplier is half the size in silicon of a FB 16 multiplier. And a B-float 16 multiplier is eight times smaller than a float 32 multiplier. And this is significant for chip development on TPU or GPU. In plus, the neural networks tend or seem to be more sensitive to the size of the exponent than to the size of the mantissa of our binary format. So what people Google and brain float 16, remember this is Google brain. So Google invented this specific format for optimal TPU chip design to ensure identical behavior of the B-float 16. It has now the same exponent size as the float 32. Both have now an eight bit exponent size. So Google optimized its new architecture chip architecture, the tensor processing unit, also primarily now for B-float 16. And of course, also if we use mixed precision training, our XLA compiler automatically now converts our values of our activation, of our weights in all layers of all tensor structures between float 32 and B-float 16 automatically. And you see this is here the improvement PyTorch 2 has now against the old PyTorch. It has now this compiler functionality where you have now a real speed improvement. Of course, XLA compiler is JAXO. The purest form is in JAX. And of course, in TensorFlow 2 and TensorFlow 3, we might even expect something faster. So this is just to show you what we have here now in our different formats. And we go and have now a look at 8-bit, 16-bit, float 16, B-float 16 and 32-bit representation of values. And they have an extreme importance of memory, speed and our calculation. And as you can see, we downloaded now our modeling 8-bit. Now, the next three steps are interesting. We need a little bit post-processing on our 8-bit model before we start here. The fine tuning. No, not the fine tuning, the adapter tuning, because at first, of course, we will freeze all the layers. And then we will take only the layer norm layers and cast them in a float 32 precision for the stability of the system. And we do the same with the output of the very last layer. So these three steps you see here, we freeze the model parameters. We say here that our small parameters should be casted in float 32. And the output of the very last one is also a float 32. So let's do this. And when we have this done, now our weights are ready. Now comes PEFT, remember? PEFT, we have our LoRa configuration file. I set the rank to 16, alpha is 32. Our target modules now change because we are working here in the EOPT model. Remember Facebook, where is Facebook? Facebook opt 6.7 billion parameter model and opt stands for open pre-trained transformer language model. It's from META and I've introduced in May 2022. And it is similar. Where is it? It is a decoder only model like GPT-3. So a competitor to GPT-3. So let's do this. Let's define our LoRa configuration. Let's have a look at our trainable parameter. And as you can see, we have now in total, oh Jesus. We have in total 6.6 billion trainable parameters by a model of 6.7 billion free trainable parameters. So sounds about right. And trainable now, that is non-frozen, about 8.3 million. And this means we only have 0.1% of all our parameters are now trainable. And now comes the magic that we're even with this smiley fraction of trainable parameters, we will get a result. So here we are. Now comes the adapted tuning phase. So we have our 8 bit, we have our model layers frozen. Beautiful. We have now the, if you want, instead of model, you can sync this is the PEPF LoRa model ready. And now we do exactly this. Now we have here a data set. And this data set here is some English quotes. Let's have a look at this that we absolutely know what we are talking about. Copyright English quote data sets. Here we go. So what do we have here? We have a quote. Be yourself for everyone else is already taken by Otto Oscar Wilde and it has a tag. Be yourself. Honesty. Love. So you see, this is our data set. Quotes by famous people that are tagged. Beautiful English. Yeah, English quotes. Sorry. This is our data set that we use. And then we go on and redefine our trainer class. And this is as always, we have our trainer. In the trainer, we have defined now our model. This is now our PEPF model. Then we have to try the training data set that we just downloaded here. Then we have to train an argument, the arguments. And we have here a batch size 4, acculumination size 4, warm up steps. 100 is quite a lot of. 30. Max steps 200. Learning rate. FP 16. Remember, we are working here on a Tesla Force. There's no brain float. There's FP 16. Logging steps one. And everything should be written in our output directory. That's called outputs. Now a data collator is here already defined. And then we say model configures case silence for the warnings. Beautiful. So here we go. We define here our parameters for the trainer class. And then comes the moment when we start here our adapter. We do hear our adapter tuning. Sorry, not fine tuning, but adapter tuning. Jesus. So and here we go. Yes. GPT2 tokenizer fast. Yes. In Rust. Yes, I know. Task. OK, no problem at all for me. If you can do it. GPU RAM 8.4 or 15 gigabytes should be fine. We have three. No, we have 200 steps. OK. Step one. Training loss. Oh gee, 200. So it needs about half an hour. Should I wait half an hour? Should I show you the result? Yeah, why not? So for you, it will be a second. And I will be back with you in. Oops. In 40 minutes. OK. See you in 40 minutes. So as you can see, after 36 minutes, we are back and we finished here 200. And if you look here at the right side of GPU RAM, my goodness, we were really close to crash here again. So now the nice thing is we can say, hey, model safe, our pre-trained and we define here a directory. So let's do this. And if we do a refresh, we have now an output directory and there should be two files in this output directory. Exactly. There is a JSON file and the adapter file, and this has only 32 megabytes. So this are here to trained adapter weights. And this is exactly what you need if we want to reload this a little bit later. But before I show you how to reload it, let's just do some inference task. We have now this beautiful path Laura model, this evolved opt model here. Now adapter tuned here. And now we run inference and we say, hey, two things are infinite. I don't know what it means, but let's see what the system comes up for a particular answer. Token as a decode output token special token skip is true. And two things are infinite, the universe and human stupidity. And I'm not sure about the universe, but I'm not sure about the universe either. Oh, but Einstein. Okay. And if we take something from, I don't know, Hemingway, I don't know who are those people, but let's see. To live is the rarest. Let's see if it comes up here. So I think it's working. You see it recognizes here our training data. It is able to reproduce here. Our famous quotes from famous people. We are running at 14.7 gigabyte of a 15 gigabyte T4. Yeah. To live is the rarest thing in the world. Most people exist. That's all. Yep, that's it. Beautiful. So it is working. And I just wanted to show you now if you want to tomorrow, for example, reload it. So at first you say thanks to Jonas Belcada for this code example, because if you want to look at half here and hugging face this here, you can find here this model that he uploaded. We now just followed his path. So if you want to download his model, this is easy. You say import torch, import path, import our transformer. Then for the model ID, you give now here exactly the hugging face name. And then exactly you have the path configuration file. You have your classical causal model from pre-trained. And then you just load the Lara model with path model from pre-trained. And here you have the classical model. And here you have the path model ID. And this is it. And just to show you that it is working. Let's call this now. Path model two. And here we go. Yes, I know my own information. Yes, yes, yes, I know this. And here now I say this is here my directory. Let's say there's no problem at all. Model three. Model three. Make sure you have enough GPU RAM. I do not have enough GPU RAM, as you can see, I'm limiting, so I do not have enough memory to load it again. OK, so this fails for this particular reason. But anyway, you see how it works. What we can achieve. We achieved some inference run. This is beautiful. And the next coding session, I think we will start in a new video and then I will show you another way how to do path models. I say thank you. Yes, we just delete here this output. Yes, it was so beautiful. I hope you enjoyed it. I hope you learned a little bit. I had a little bit of fun. I see you in my next video.",
        "output": "### PAF Introduction and Options\nToday we're coding PAF (Parameter Efficient Fine Tuning) with four methodologies currently implemented:\n\n1. LORER (Low Rank Approximation for Matrices)\n2. Ptuning (Prompt Encoder)\n3. Prefix Tuning\n4. Prompt Tuning\n\n### Target Modules in LORER\nDifferent models have different target modules for applying LORER. For example:\n\n- T5 and Multilingual T5: Target query and value weight matrices in the layers.\n- Bloom (OPT from Facebook): Target different submatrices.\n\n### Creating a PEPF LORER Model\nTo create a PEPF LORER model, first, create the classic model (e.g., T5). Then, create a LORER configuration file specifying the method (e.g., LORER), model type (e.g., sequence-to-sequence), and target modules. After this, use `GetPEPFModel` function to generate the PEPF LORER model.\n\nExample:\n```\nClassic model:\nAutoModelForSeq2SeqLM.from_pretrained(\"t5-base\")\n\nLORER configuration:\nPATHConfig(pethod=\"lorer\", model_type=\"seq2seq\", target_modules=path_lry_method.T5, rank=16, alpha=32)\n\nPEPF LORER model:\nGetPEPFModel(classic_model, config_lor)\n```\n\n### Freezing Layers and Additional Configuration\nBefore fine-tuning the PAF model, we need to freeze all its layers, convert its layer normalization to float32 precision, and also convert the output layer to float32 precision for stability.\n\n### Adapter Tuning\nUse `Trainer` class from Hugging Face Transformers to perform adapter tuning on the PAF model with a specific dataset. After training, you can save and reload the model. For reloading the model, import the required libraries and load the pre-trained model with its saved configuration.\n\n### Inference and Loading Pre-trained Model\nAfter fine-tuning the PAF model and reloading it, you can perform inference tasks using the model."
    },
    {
        "instruction": "<|SYSTEM|>You are a scholars that is taking markdown notes about some video, you would be giving some written scripts for the task.\nYou have finished writing the notes, and you want to extract the title from the notes.\nThe title should only be the name of the technology or algorithm, no extra words needed.\nPlease write the notes in markdown format. No need to write \"Title:\" for the Title. Make sure the title is within 10 words.\nHere is an example:\n## Hidden Markov Model\n<|ASSISTANT|>## ",
        "input": "<|USER|>### PAF Introduction and Options\nToday we're coding PAF (Parameter Efficient Fine Tuning) with four methodologies currently implemented:\n\n1. LORER (Low Rank Approximation for Matrices)\n2. Ptuning (Prompt Encoder)\n3. Prefix Tuning\n4. Prompt Tuning\n\n### Target Modules in LORER\nDifferent models have different target modules for applying LORER. For example:\n\n- T5 and Multilingual T5: Target query and value weight matrices in the layers.\n- Bloom (OPT from Facebook): Target different submatrices.\n\n### Creating a PEPF LORER Model\nTo create a PEPF LORER model, first, create the classic model (e.g., T5). Then, create a LORER configuration file specifying the method (e.g., LORER), model type (e.g., sequence-to-sequence), and target modules. After this, use `GetPEPFModel` function to generate the PEPF LORER model.\n\nExample:\n```\nClassic model:\nAutoModelForSeq2SeqLM.from_pretrained(\"t5-base\")\n\nLORER configuration:\nPATHConfig(pethod=\"lorer\", model_type=\"seq2seq\", target_modules=path_lry_method.T5, rank=16, alpha=32)\n\nPEPF LORER model:\nGetPEPFModel(classic_model, config_lor)\n```\n\n### Freezing Layers and Additional Configuration\nBefore fine-tuning the PAF model, we need to freeze all its layers, convert its layer normalization to float32 precision, and also convert the output layer to float32 precision for stability.\n\n### Adapter Tuning\nUse `Trainer` class from Hugging Face Transformers to perform adapter tuning on the PAF model with a specific dataset. After training, you can save and reload the model. For reloading the model, import the required libraries and load the pre-trained model with its saved configuration.\n\n### Inference and Loading Pre-trained Model\nAfter fine-tuning the PAF model and reloading it, you can perform inference tasks using the model.",
        "output": "## PAF (Parameter Efficient Fine Tuning)"
    }
]